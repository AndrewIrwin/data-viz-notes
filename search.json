[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Visualization notes",
    "section": "",
    "text": "Welcome\nData visualization is a critical skill for anyone interested in using data to think and persuade.\nThis course focuses on three aspects of data visualization:"
  },
  {
    "objectID": "index.html#what-is-data-visualization",
    "href": "index.html#what-is-data-visualization",
    "title": "Data Visualization notes",
    "section": "What is Data Visualization?",
    "text": "What is Data Visualization?\nData visualization is the practice of turning data into graphics. Good graphics are more easily interpreted than the raw data. A well-designed visualization is faithful to the original data and does not mislead the intended audience. Almost every data visualization is a simplification and approximation of a raw dataset, and thus involves a perspective — the goals and biases of the person producing the visualization.\nData visualization approaches vary. For some purposes, highly customized graphic design and visual style are paramount. That’s not our focus. We emphasize standardized graphical presentations — which span a wide variety of visualizations — that minimize the use of graphical elements not directly linked to the data."
  },
  {
    "objectID": "index.html#what-is-the-purpose-of-data-visualization",
    "href": "index.html#what-is-the-purpose-of-data-visualization",
    "title": "Data Visualization notes",
    "section": "What is the purpose of data visualization?",
    "text": "What is the purpose of data visualization?\nData visualization is used to help tell stories with data. The usual goal is to communicate an interpretation of a data set to a particular audience, to make an argument you have worked out from an analysis of the data. The visualization is a device to help you do that. Visualizations are best when accompanied by written explanations. Even if a picture is worth a thousand words, visualizations do not usually stand on their own. Once the message is understood and internalized, a good visualization can sometimes tell the story “by itself”."
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Data Visualization notes",
    "section": "Course Goals",
    "text": "Course Goals\nBy the end of the course you will be familiar with several aspects of data visualzation.\nVisual impact and aesthetic aspects of graphics\n\nUnderstand the relationship between the structure of your data and the perceptual features of your graphics.\nDescribe aesthetic features of good plots.\nUse length, shape, size, colour, annotations, and other features to effectively display data and enable comparative visual interpretation.\n\nVisualization as communication\n\nVisualization is a visual language for communication, which should be accompanied by written interpretations.\nVisualizations should be developed through an iterative process akin to editing text, including the process of refining plots to highlight key features of the data, labeling items of interest, annotating plots, and changing overall appearance.\nVisualizations can be presented in different formats for different audiences and different communications goals.\nCommunication effectiveness should be evaluated by peer-review and critical, constructive feedback.\n\nComputing skills\n\nLearn the basics of using R, Rstudio, several R add-on packages, and git.\nRead data in several different formats into R.\nCreate graphs with ggplot2 for continuous and categorical variables with informative legends.\nAdd error bars, linear models, smooths, labels, and other annotations to a graph.\nCreate small-multiple (facetted) plots.\nUse the principles of tidy data tables to facilitate transformation and analysis of data. Reshape data to make it tidy.\nSummarize and transform data using dplyr.\nReshape data using pivots.\nCreate maps and some alternatives for presenting spatial data.\nWrite reproducible documents with R markdown to document your analysis process and present your results.\nDistribute data, code, and results using git and github.\nAccess and interpret help resources (built-in help, vignettes, web pages, online discussion forums, blogs).\nDevelop skills for independently learning new data visualization methods and software.\n\nStatistical models\n\nUse a variety of modeling techniques such as LOESS, OLS, robust regression, polynomial regression, and quantile regression.\nLearn how to extract information from fitted models to compare different statistical models.\nUse principal component analysis (PCA) to reduce the dimensionality of complex datasets, increasing interpretability while minimizing information loss.\nUse multidimensional scaling (MDS) to visualize and compare similarities and dissimilarities between variables.\nDivide observations into homogeneous and distinct groups using K-means."
  },
  {
    "objectID": "lessons/101-introduction.html#goals",
    "href": "lessons/101-introduction.html#goals",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.1 Goals",
    "text": "1.1 Goals\nIn this lesson I will provide some examples of interesting and influential data visualizations."
  },
  {
    "objectID": "lessons/101-introduction.html#atmospheric-carbon-dioxide-concentration",
    "href": "lessons/101-introduction.html#atmospheric-carbon-dioxide-concentration",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.2 Atmospheric carbon dioxide concentration",
    "text": "1.2 Atmospheric carbon dioxide concentration\nClimate change over the coming century will be controlled by human-driven emission of carbon dioxide from fossil fuels into the atmosphere, and possibly our ability to remove it from the atmosphere. Starting in the 1950s, the amount of carbon dioxide in the atmosphere (in parts per million) was regularly measured. Subsequently, methods for analyzing gasses trapped in ice were used to extend this record back about one million years. There is a direct physical link between atmospheric concentration of carbon dioxide and the loss of heat from Earth to space, resulting in a mechanistic link between increasing carbon dioxide concentration in the atmosphere and the mean temperature of the surface of the Earth. Visualizations of this data and assocated global mean temperature data have been extremely influential, forming the cornerstone of books, a documentary movie, and countless educational and environmental change movements.\nThe two graphs below show carbon dioxide concentrations (in parts per million) in the atmosphere measured from a high-altitude observatory on the island of Hawai’i. The first graph shows two years of data. The second graph shows the monthly record since regular observations began.\n\n\n\n\nFigure 1.1: Two years of atmospheric CO\\(_2\\) concentration from Mauna Loa observatory. Source co2.earth and sioweb.ucsd.edu.\n\n\n\n\n\n\n\nFigure 1.2: Atmospheric CO\\(_2\\) concentration from Mauna Loa observatory, 1958 to present. Source co2.earth and sioweb.ucsd.edu.\n\n\n\nTwo signals are immediately clear:\n\nthere is an annual cycle in the CO\\(_2\\) concentration and\nthere is a yearly increase in CO\\(_2\\) concentration, and this increase has been gradually increasing over the past 60 years.\n\nA related graph, with a contrasting format shows the annual cycle of sea ice in the Arctic over the past 40 years.\n\n\nFigure 1.3: Sea ice volume, Arctic Ocean. Data from PIOMAS. Graphic by Zachary Labe @ZLabe.\n\nSample visualizations of atmospheric carbon dioxide which are regularly updated are available from the institute that has been collecting this data for decades.\nThe closely related data of estimated global mean temperature over time are available from NASA.\nMany other sites have information on these data, usually presenting data visually as a testament to the importance of visualizations."
  },
  {
    "objectID": "lessons/101-introduction.html#human-health-and-development",
    "href": "lessons/101-introduction.html#human-health-and-development",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.3 Human health and development",
    "text": "1.3 Human health and development\nHans Rosling was a physician and popularizer of data visualizations to develop understanding of human health and economic development over time and across countries. His public presentations illustrate his view of how dyanmic charts can help us come to see the trajectory of global development, particularly the connections between health and economic development. I strongly encourage you to watch one of his presentations. He was especially well known for his effort to dispell misunderstandings about differences across countries in health and human development. He popularized a style of scatterplot which combined the use of colour, symbol size, and animations to show changes over time.\n\n\n\n\nFigure 1.4: Fertility rate (babies per woman) as a function of median national income across countries. See the animated version showing how this relationship has changed over time in the video linked above. Source gapminder.org"
  },
  {
    "objectID": "lessons/101-introduction.html#weather",
    "href": "lessons/101-introduction.html#weather",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.4 Weather",
    "text": "1.4 Weather\nMany people are strongly interested in their local weather conditions. As a result of this strong interest and the complextity of the data, many visualizations have been developed. Forecasts, such as those produced by Environment Canada, and historical retrospectives, such as those produced by Weatherspark are examples that leverage familiarity with the data, broad-scale human interest, and data-rich but not overly complicated displays. Two examples are shown below.\n\n\n\n\nFigure 1.5: Environment Canada weather forecast for Halifax NS, December 10, 2020\n\n\n\n\n\n\n\nFigure 1.6: Climatological mean daily high and low temperatures in Halifax NS from weatherspark.com\n\n\n\nNote that each image contains a wealth of data:\n\ncurrent weather conditions and forecasts of weather conditions, high temperature for 12 days,\naverage historical daily highs and lows throughout the year, plus information on the quantiles of the temperature distribution on each day (shaded regions), some annotations showing high and low temperatures on selected days, and comparative descriptive labels (cold, warm, and neither)."
  },
  {
    "objectID": "lessons/101-introduction.html#journalism",
    "href": "lessons/101-introduction.html#journalism",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.5 Journalism",
    "text": "1.5 Journalism\nIn the past decade there has been a resurgence of interest in data visualizations, stimulated in part by journalists emphasising visualizations in their publications. This example in the New York Times shows projected earnings for college graduates in a range of fields of study and is accompanied by notes and discussion questions. The New York Times has a series of educational materials on both visualizations and their stories.\n\n\nFigure 1.7: Graph showing career earnings from a New York Times data journalism story."
  },
  {
    "objectID": "lessons/101-introduction.html#historically-important-visualizations",
    "href": "lessons/101-introduction.html#historically-important-visualizations",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.6 Historically important visualizations",
    "text": "1.6 Historically important visualizations\nMany ideas in contemporary data visulizations can be traced back to the 19th century, as represented by several impactful examples. In 1869, Charles Minard produced a map of Napoleon’s Russian campaign of 1812. Florence Nightingale was a pioneer user of data visualizations to communicate messages about sanitation and public health, famously in a polar histogram showing causes of mortality of soldiers. Also in public health, John Snow mapped a cholera outbreak in London, visually linking deaths to a water source. All of these visualizations were great advances over the bills of mortality produced a few centuries earlier."
  },
  {
    "objectID": "lessons/101-introduction.html#stories",
    "href": "lessons/101-introduction.html#stories",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.7 Stories",
    "text": "1.7 Stories\nA common observation is that humans learn from stories. What is the role of data and its visualization in story telling? A graph does not tell a story by itself, but a story can be woven from a combination of words and some data visualizations.\nWilke’s book has an excellent argument in favour of storytelling with data which he tells in a video (starting at time 6:42). His essential elements of a story are an arc including an opening, challenge, action, and resolution, which results in an emotional reaction such as excitement, curiosity or surprise. The principle is that the emotional response from the resolution of the challenge gets your audience engaged and helps them retain your message.\nIt may seem to you that a graph is far removed from a story. In fact, a single graph rarely tells a story on its own. A pair of graphs, or a dynamic graph, or even just an original graph and an updated graph can be used to tell a story. For example, return to the carbon dioxide figures at the top of this lesson. Two years of data show a seasonal cycle in atmospheric carbon dioxide with a modest year over year trend. Suppose that was all you knew about carbon dioxide. It would be hard to know why there was a problem. Now look at the record since 1958. It’s now clear that there is a long-term increase and the interannual variation is small in comparison. The 800,000 year record from ice cores shown below provides even more context. Current atmospheric carbon dioxide concentrations are far outside the range of documented variability for the past 800,000 years.\n\n\nFigure 1.8: Global atmospheric carbon dioxide (CO2) in parts per million (ppm) for the past 800,000 years. Source climate.gov, NCEI.\n\nWe will return to the theme of story telling frequently in the course, particularly in assignments."
  },
  {
    "objectID": "lessons/101-introduction.html#exercises",
    "href": "lessons/101-introduction.html#exercises",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.8 Exercises",
    "text": "1.8 Exercises\n\nCompare the presentation of data in the carbon dioxide and sea ice volume graphs. Which is easiest for you to understand? Why?\n\nPredict how CO\\(_2\\) over time would look if plotted in the style of the sea ice volume graph. Do the reverse prediction too. Return to this task later in the course once you have some more data manipulation and plotting skills.\n\n\nWhich visualization from this lesson is most appealing? Most easily understood? Least appealing? Hardest to understand? Try to draw some conclusions about what you think “works” in a visualization."
  },
  {
    "objectID": "lessons/101-introduction.html#futher-reading",
    "href": "lessons/101-introduction.html#futher-reading",
    "title": "1  Invitation to Data Visualization",
    "section": "\n1.9 Futher reading",
    "text": "1.9 Futher reading\n\nKurt Vonnegut summary of story arcs\n\n\nWilke (2019), in particular Chapter 29.\nA course about Data Visualization in R by Claus Wilke.\n\n\n\n\n\n\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. \"O’Reilly Media\". https://clauswilke.com/dataviz/."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#goals",
    "href": "lessons/102-intro-to-r.html#goals",
    "title": "2  Computer tools",
    "section": "\n2.1 Goals",
    "text": "2.1 Goals\nIn this lesson I will explain what computer tools we will use in the course, how they were selected, and why they are useful."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#introduction",
    "href": "lessons/102-intro-to-r.html#introduction",
    "title": "2  Computer tools",
    "section": "\n2.2 Introduction",
    "text": "2.2 Introduction\nWe will use R and Rstudio in this course. R widely used to analyze data and comes with many add-on packages for statistical analysis, data visualization, and specialized tools for many disciplines and applications. We will just scratch the surface of what R can do, but by the end of the course you should feel comfortable learning to use R for data visualization and you will have the skills to learn how to use R for many other tasks. Rstudio is a graphical user interface for R with many convenient features including tools to support report writing, getting help, and project management. For data visualization we will use ggplot2, for data analysis we will use the tidyverse style of programming, for report writing we will use R markdown, and for project management we will use R projects and git version control. All of these computer terms will mean much more to you later on in the course!"
  },
  {
    "objectID": "lessons/102-intro-to-r.html#r",
    "href": "lessons/102-intro-to-r.html#r",
    "title": "2  Computer tools",
    "section": "\n2.3 R",
    "text": "2.3 R\nR is open-source software developed by a core team of programmers, which is made available for free to all users along with the full ‘source code’ for the software so that any one can find out exactly how any part of the software works. R was first developed in the mid-1990s and is now a quite mature language, although development continues. For example a brand new notation for function composition was introduced in 2021. R was developed after a couple decades of experience with its predecessor S and a commercial product known as S+.\nR has many strengths that account for its popularity and longevity. It is a very flexible and expressive language, which allows for many styles of programming and the use of both powerful tools to perform sophisticated analyses easily and also the flexibility to develop your own computational tools from basic computations. There is a huge, global community of users who continually develop new tools for R and make them available and easy to install on your computer. This means that anyone can develop a new data analysis method and readily distribute it on a widely-used platform to anyone comparatively easily. This is tremendously powerful! A third key ingredient is a very helpful community of users who develop tutorials and books to help newcomers to R learn to use it. Our course will use three such books, which can be purchased, but are also available for free on the internet.\nOn the other hand, R is a challenging tool to learn. To accomplish any task you must type instructions into the computer. This can be daunting as you must first learn what instructions are available, what they do, and how to use them. If you have had experience with programming before (for example using Python or JavaScript) you will know that there are many ways to go wrong and the computer’s output is not always easy to understand, especially when things go wrong! Nevertheless, the R style of computing with data, once you have been initiated, allows for endless possibilities in terms of data analysis.\nOur approach in this course will be to demonstrate a few sample calculations and computing tasks in each lesson and encourage you to become proficient in these tasks through repetition and minor modifications. Over time, you will come to understand both how to use the tools to analyze and visualize data and how to interact with R to accomplish new tasks."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#rstudio",
    "href": "lessons/102-intro-to-r.html#rstudio",
    "title": "2  Computer tools",
    "section": "\n2.4 Rstudio",
    "text": "2.4 Rstudio\nRstudio is a graphical interface (or integrated development environment) to the R software. It provides an editor for documents, a convenient way to see plots, get help, explore data objects created with R, and manage report writing and sets of files used collectively as part of projects. As with R, you will find that Rstudio has many features that will take you a while to discover. In this course we will focus on a few features and leave you to find others on your own."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#grammar-of-graphics-ggplot2",
    "href": "lessons/102-intro-to-r.html#grammar-of-graphics-ggplot2",
    "title": "2  Computer tools",
    "section": "\n2.5 Grammar of graphics (ggplot2)",
    "text": "2.5 Grammar of graphics (ggplot2)\nSince this is a course in data visualization, creating plots from data is a core skill. To get a computer to make a graph, you need to bring data together with instructions for producing the graph. Traditional ways to make graphs have been idiosyncratic, with each kind of graph having its own specialized instructions, even for features common to many plots such as the use of size adjustments, symbols, or colors to represent numerical or categorical scales. The ggplot style arose out of a recognition that having a language (or ‘grammar’) to describe graphical displays of data was superior to other styles of graphics creation. (The gg in ggplot is an abbreviation for the grammar of graphics.) A common language is used to connect variables in your dataset to aesthetic features in your plot, such as position along an axis, symbol size, shape, or colour. This association is separate from selecting how the data are to be represented (e.g., as points, a line, a histogram). Another key element is how ranges of values are associated to sizes, shapes, and colours; these are called scales and they too have their own functions, which work regardless of the type of plot being made. Finally, annotations such as axis labels, and formatting of other elements (e.g., fonts, sizes, positions of non-data elements) may be adjusted to customize a plot. Essentially ggplot provides a tool for making graphs which is modular, and thus allows you to quickly and efficiently learn to make new kinds of plots and modify them. The proof that this works is that since ggplot2 was first developed, many users of the software have contributed their own styles of plotting which can be incorporated into the design of the original software. All this makes plotting data sound complicated, but in fact making a simple plot with few customizations is actually very easy once you understand the idea behind the grammar of graphics and practice with some data."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#tidyverse",
    "href": "lessons/102-intro-to-r.html#tidyverse",
    "title": "2  Computer tools",
    "section": "\n2.6 tidyverse",
    "text": "2.6 tidyverse\nThe starting point for each visualization we will produce and each analysis we will do is data. The developers of the tidyverse set of tools in R noticed that we all do many of the same kinds of operations to our data such as transformations with functions, summaries, and filtering. The approach also makes a few style suggestions about how to organize your data and how to analyze data, providing both great flexibility and making your code easier for others to understand. There is nothing that can be done in the tidyverse style that I couldn’t figure out how to do before these tools were developed, but the R code I write now is much easier for me and others to read. The problem solving process I use is much more streamlined than it used to be since I tend to use the same kinds of solutions for each new dataset I examine.\nThere are two central ideas in the tidyverse approach to data analysis.\n\nData should be arranged in rectangles (or two dimensional arrays) with variables in columns and observations in rows.\nA data analysis calculation should be broken down into a series of simple, modular functions that are composed together, such as filtering rows, grouping rows together, and computing summaries.\n\nEssentially this forms a grammar of data analysis, analogous to what ggplot did for plotting graphics."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#r-markdown",
    "href": "lessons/102-intro-to-r.html#r-markdown",
    "title": "2  Computer tools",
    "section": "\n2.7 R Markdown",
    "text": "2.7 R Markdown\nOnce you know how to produce a data visualization or statistical analysis, new challenges quickly arrive. You must communicate these results to someone else. You must be prepared to revise your analysis and repeat it on revised or new data. A common approach is write lots of little bits of computer code, cut and paste the results into a word processor, and try to remember how it all fits together. R markdown documents are key step in a better solution. An R markdown document contains both natural-langauge (e.g., English) text for humans to read and R code for the computer to use. The R markdown document can be compiled (we will say ‘knitted’) together with the results of R computations to provide a finished report. This enables the computer code and data analysis results to be kept together with the exposition about the data and results.11 As of summer 2022, an updated tool for report writing called Quarto is available. For our purposes this will be indistinguishable from R markdown and you can use either tool. Both are available in recent versions of Rstudio."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#git-and-github",
    "href": "lessons/102-intro-to-r.html#git-and-github",
    "title": "2  Computer tools",
    "section": "\n2.8 Git and Github",
    "text": "2.8 Git and Github\nGit is distributed version control software. This means that it is a tool to help you keep track of versions of computer code, including R markdown documents, both in your work and your collaboration with others. It has tools for managing versions of software in databases called repositories. Version history and management is useful if you revise a report you have prepared but want to keep access to the old report for archival purposes. Having access to old versions of data visualizations is essential for auditing and quality control as well. Git also enables two or more people working in different locations to collaborate on the same project. Git allows you to easily merge changes that are logically independent (in terms of the code; human comprehension is still necessary!) and helps collaborators resolve conflicts in edits. Git is widely used in the data science world and among software developers.\nWe will be learning the most basic uses of git in our course.\nGithub is a web service that allows you to easily publish your git repositories. Rstudio has easy to use tools for managing your work using git and github. These course notes are written and published using Rstudio, git, and github. You will use the same tools for your assignments and term project."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#further-reading",
    "href": "lessons/102-intro-to-r.html#further-reading",
    "title": "2  Computer tools",
    "section": "\n2.9 Further reading",
    "text": "2.9 Further reading\nHealy, Sections 2.1-2.4 contain excellent advice on the reasons for using R markdown, R/Rstudio, projects, the basics of R, and being patient as you learn computing tasks. We will return to the basics of R at various points in the course.\nR4DS, Chapters 26 and 27 contains a valuable introduction to R markdown and its use for communication.\nWilke has a thoughtful perspective on the ever changing landscape of software for data analysis. The tools we all use will change as the years progress, but we need to learn to make visualizations today, so we consider both the why and the how of data visualizations. Wilke emphasizes the why, but he has been strongly influenced by the tools we are using. I’ve been drawing computer graphics for over 30 years and used countless tools and ideas. The methods in this course are the best I’ve ever used; ggplot and the tidyverse constrain my work in just the right way to make it better."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#exercises",
    "href": "lessons/102-intro-to-r.html#exercises",
    "title": "2  Computer tools",
    "section": "\n2.10 Exercises",
    "text": "2.10 Exercises\n\nRead the sections of Healy and R for Data Science (R4DS) linked in the previous section.\nCan you think of an example of a piece of technology (a tool, an app on your phone) that was useful at some point, but is not useful to you now? Did you learn something general from the experience of using the tool? Or did the tool make it easier for you to learn something (develop a skill, etc.) at the time?"
  },
  {
    "objectID": "lessons/102-intro-to-r.html#resources",
    "href": "lessons/102-intro-to-r.html#resources",
    "title": "2  Computer tools",
    "section": "\n2.11 Resources",
    "text": "2.11 Resources\n\nIf you want to know a lot more about using Rstudio, here is an hour-long video overview of Rstudio features.\nIf you want to know more than we will cover about git and github, see the notes called Happy git with R. All the essentials of using these tools will be explained in future lessons.\nR Markdown for scientists is a set of notes on using R markdown for report writing and data analysis aimed at scientists.\nSome R Markdown tips.\nInformation about Quarto, the successor to R markdown."
  },
  {
    "objectID": "lessons/102-intro-to-r.html#references",
    "href": "lessons/102-intro-to-r.html#references",
    "title": "2  Computer tools",
    "section": "\n2.12 References",
    "text": "2.12 References\n\nCourse textbooks: Healy (2018); Wickham, Çetinkaya-Rundel, and Grolemund (2023); Wilke (2019).\nThe R software package: R Core Team (2022)\n\nThe Tidyverse: Wickham et al. (2019)\n\nThe grammar of graphics (ggplot) package: Wickham (2016)\n\n\n\n\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press. https://socviz.co/.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. \"O’Reilly Media\". https://r4ds.had.co.nz/.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. \"O’Reilly Media\". https://clauswilke.com/dataviz/."
  },
  {
    "objectID": "lessons/103-setup.html#goals",
    "href": "lessons/103-setup.html#goals",
    "title": "3  Setting up your computer",
    "section": "\n3.1 Goals",
    "text": "3.1 Goals\nIn this lesson you will start using the computer tools introduced in the previous lesson."
  },
  {
    "objectID": "lessons/103-setup.html#introduction",
    "href": "lessons/103-setup.html#introduction",
    "title": "3  Setting up your computer",
    "section": "\n3.2 Introduction",
    "text": "3.2 Introduction\nThe statistical software R and RStudio and version control software git will be used in this course. No prior experience with R, RStudio or git is assumed. We’ll take class time to learn the software.\nIn this lesson you will\n\ninstall the software on your computer,\nlearn to use the software “in the cloud”,\ncreate a github account, and\ncomplete a task to let me know this is done or ask for help and tell me your github account name.\n\nThe steps below are written for Windows and Macintosh computers, with necessary variations noted. If you use a Chromebook, you can’t install this software, so skip ahead to the rstudio.cloud section. If you use linux, adapt the instructions below to install the software and contact me if you have trouble."
  },
  {
    "objectID": "lessons/103-setup.html#r",
    "href": "lessons/103-setup.html#r",
    "title": "3  Setting up your computer",
    "section": "\n3.3 R",
    "text": "3.3 R\n\nTo download and install R go to r-project.org and click on the link to download R. For Mac, you want “R-4.2.1.pkg” (or a more recent version). For Windows, follow the link marked “base”. If you already have R installed, please check to be sure you have version 4.0 or later, and update the software if you have an older version. You can see the version of R with the command R.version."
  },
  {
    "objectID": "lessons/103-setup.html#rstudio",
    "href": "lessons/103-setup.html#rstudio",
    "title": "3  Setting up your computer",
    "section": "\n3.4 Rstudio",
    "text": "3.4 Rstudio\n\nTo download and install Rstudio, go to Rstudio.com and click on the download link to download Rstudio. You want the free Rstudio Desktop for your computer (Windows, Mac, Linux)."
  },
  {
    "objectID": "lessons/103-setup.html#git",
    "href": "lessons/103-setup.html#git",
    "title": "3  Setting up your computer",
    "section": "\n3.5 Git",
    "text": "3.5 Git\n\nTo download and install git:\n\non Windows, go to git-scm.org and click on the link to download a version for Windows\n\non Macintosh, use the Terminal app or the Terminal tab in Rstudio and type xcode-select --install to download and install git. There should be two minus signs in front of “install”; some web browsers may display this incorrectly as a dash (one symbol)."
  },
  {
    "objectID": "lessons/103-setup.html#rstudio.cloud",
    "href": "lessons/103-setup.html#rstudio.cloud",
    "title": "3  Setting up your computer",
    "section": "\n3.6 rstudio.cloud",
    "text": "3.6 rstudio.cloud\nIf you have a Chromebook you can use R, Rstudio and git through the cloud service rstudio.cloud. Everyone should learn to use the cloud service as a backup in case of problems with the software on their own computer."
  },
  {
    "objectID": "lessons/103-setup.html#dalhousie-on-campus-labs",
    "href": "lessons/103-setup.html#dalhousie-on-campus-labs",
    "title": "3  Setting up your computer",
    "section": "\n3.7 Dalhousie on-campus labs",
    "text": "3.7 Dalhousie on-campus labs\nR and Rstudio are available on Dalhousie computer labs, but the git version control software must be installed following the instructions for Windows computers above. Since all your user files are erased from lab computers when you log out, this process must be repeated on each login. For this reason, I don’t recommend using lab computers for this course, unless you are using the rstudio.cloud service."
  },
  {
    "objectID": "lessons/103-setup.html#packages",
    "href": "lessons/103-setup.html#packages",
    "title": "3  Setting up your computer",
    "section": "\n3.8 Packages",
    "text": "3.8 Packages\nYou are not done downloading and installing software yet! Most of the tools we will use with R are distributed as add-on packages. These are bundles of software that add new functions to R. There are three steps to use a pacakge:\n\nInstall the package (done only once)\nTell R you will be using the package (done each time you start R)\nLearn how to use the package (a major goal of this course)\n\nI install new packages all the time on my machine. Right now I have 203 installed. It’s also common to update to new versions. Rstudio trys to help you identify packages you need to install – we’ll see how later on. An optional task for today is to install packages suggested by Healy in his Preface. (We’ll use lots more packages than this, but this is a good start.) Cut and paste the following R code into the window marked “Console” in Rstudio.\nmy_packages &lt;- c(\"tidyverse\", \"broom\", \"coefplot\", \"cowplot\",\n                 \"gapminder\", \"GGally\", \"ggrepel\", \"ggridges\", \"gridExtra\",\n                 \"here\", \"interplot\", \"margins\", \"maps\", \"mapproj\",\n                 \"mapdata\", \"MASS\", \"quantreg\", \"rlang\", \"scales\",\n                 \"survey\", \"srvyr\", \"viridis\", \"viridisLite\", \n                 \"socviz\", \"devtools\", \"patchwork\", \"usethis\", \"gitcreds\",\n                 \"remotes\", \"paletteer\", \"ggthemes\", \"ggtext\")\n\ninstall.packages(my_packages)"
  },
  {
    "objectID": "lessons/103-setup.html#github",
    "href": "lessons/103-setup.html#github",
    "title": "3  Setting up your computer",
    "section": "\n3.9 Github",
    "text": "3.9 Github\nGithub is a web service for sharing and publishing github repositories and many related services. We will learn more about using the service in later lessons."
  },
  {
    "objectID": "lessons/103-setup.html#exercises",
    "href": "lessons/103-setup.html#exercises",
    "title": "3  Setting up your computer",
    "section": "\n3.10 Exercises",
    "text": "3.10 Exercises\nThe task today is especially important. Get all these tools working on your computer by installing them or using rstudio.cloud before the next lesson."
  },
  {
    "objectID": "lessons/104-look-at-data.html#goals",
    "href": "lessons/104-look-at-data.html#goals",
    "title": "4  Look at data",
    "section": "\n4.1 Goals",
    "text": "4.1 Goals\nYou will learn about some factors that influence how you should visualize data, including your motivation for creating the visualization and how human visual perception influences what we “see” when we look at a visualization."
  },
  {
    "objectID": "lessons/104-look-at-data.html#introduction",
    "href": "lessons/104-look-at-data.html#introduction",
    "title": "4  Look at data",
    "section": "\n4.2 Introduction",
    "text": "4.2 Introduction\nWe’re learning to make data visualizations, so naturally we want to make good ones. What makes one visualization good and another bad? Are there specific qualities to emulate or avoid? How can we meaningfully discuss visualization quality without emphasizing personal preferences and taste? Healy, Chapter 1 has many good answers to these questions, while recognizing that the subject is somewhat subjective.\nIn this lesson we will try to develop some tentative answers to the following questions:\n\nWhat can you learn more easily from a visualization than from a table of data?\nWhat makes a good visualization?\nWhat features of human visual perception should be considered when designing data visualizations?\n\nThere is much more in Healy, Chapter 1 than the highlights in this lesson. I suggest you read the chapter carefully and return to it several times throughout the course."
  },
  {
    "objectID": "lessons/104-look-at-data.html#problems-with-numerical-summaries",
    "href": "lessons/104-look-at-data.html#problems-with-numerical-summaries",
    "title": "4  Look at data",
    "section": "\n4.3 Problems with numerical summaries",
    "text": "4.3 Problems with numerical summaries\nNumerical summaries of data (e.g., mean, median, quantiles, standard deviation) are very useful but sometimes conceal more than they reveal. Discussions of financial aspects of life in the news – whether incomes, taxes, or assets often suffer from these problems because distributions of the underlying data are skewed. Its exceedingly rare to discuss the distribution of data explicitly except in specialized publications, probably because the concepts are not widely understood and difficult to communicate in writing. By contrasts, a visulization can sometimes show the distribution of data in an informative and impactful way. Similarly, numerical summaries of relationships between two or more variables (e.g., correlation, linear regression parameters) can be misleading in the presence of outliers (see Healy Figure 1.2, 1.3) or unexplored features of the relationship.\nThe main problem with numerical summaries is that complex data are distilled to a very small set of numbers. Visualizing data allows many more data to be represented and exploits the human visual system to interpret the relationships, without any statistical pre-filtering (e.g., by selecting an average or a linear model).\nTwo synthesized datasets illustrate these phenomena through extreme cases: Anscombe’s Quartet and the Datasaurus Dozen. The data on each panel have the same numerical summaries (mean, standard deviation, and correlation), yet the underlying patterns are clearly very different.\nThese problems are generally resolved by the rule: Always plot your data. Don’t rely only on numerical and statistical analysis."
  },
  {
    "objectID": "lessons/104-look-at-data.html#good-and-bad-figures",
    "href": "lessons/104-look-at-data.html#good-and-bad-figures",
    "title": "4  Look at data",
    "section": "\n4.4 Good and bad figures",
    "text": "4.4 Good and bad figures\nWhat makes a visualization good? Or bad? We’ll explore some answers, but the simplest test is this\n\nIs the main impression a viewer gets the one you intended as a designer?\nIs the interpretation faithful to the underlying data?\n\nTo answer these questions, you need to do some work after you’ve made the visualization. You need to continue to probe the data to be sure you are not misrepresenting the data. You need to use the visualization for communication and find out if your audience understood your message, or if they got some other impression as a result of the visualization choices you made.\n\n4.4.1 Chart junk\nSometimes to dress up a display of data, designers will add graphic elements which are not determined by the data, but are instead designed to guide your interpretation of the display. These elements are not part of the visualizations we will create – they are elements of infographics or advertising to persuade and illustrate rather than display data. We are focused on displaying data. (For examples of bad infographics, just search. The term chart junk was created by the influential statistician Edward Tufte.)\n\n\n\n\nFigure 4.1: Still from CBC Canadian Federal election coverage, October 21, 2019. Does the three-dimensional bar chart help you understand the election outcome? Are the bars the right height? Is it helpful to have the bar chart appear in the middle of the studio set? Can you judge the height of the bars?\n\n\n\n\n4.4.2 Distracting colours, images or shapes\nGood visualizations don’t have to be minimalist black and white, but the use of colour, shape and other elements should be chosen deliberately to enhance the message told by the data. Colour can be used to highlight contrasts or create groups. Shapes are harder to distinguish, especially if there are many parts. Healy examined many of these perceptual aspects of plots in Sections 1.4 and 1.5. These perceptual elements were explored in a recent popular magazine article.\n\n4.4.3 Bad data\nData can be wrong in many ways, but even if the data underlying a figure are correct, you must be sure that the message conveyed by the data matches the presentation of the figure. A survey of how people spend their time reported in Our World In Data is typical – does the plot really tell you what it looks like at first glance? Or is it more about age distributions, gender roles, and other sociological factors than the experience of a “typical person” in each country? If it’s too hard to tell, the visualization doesn’t work.\n\n\nFigure 4.2: Source: OECD time use database, Our World in Data and tweet by @simongerman600 on 2020-12-13."
  },
  {
    "objectID": "lessons/104-look-at-data.html#perception-guidelines-and-cautions",
    "href": "lessons/104-look-at-data.html#perception-guidelines-and-cautions",
    "title": "4  Look at data",
    "section": "\n4.5 Perception guidelines and cautions",
    "text": "4.5 Perception guidelines and cautions\nMany features of a visualization can make it harder to interpret than you, as a designer, intend. Once you know the message of a graph, it may be difficult to see problems with visualizations you create, since you know the intended message. So you must keep an eye out for unintended messages and difficult to decode graphs.\nStacked bar charts (Healy Figure 1.11) make some comparisons across groups difficult because the baseline of one colour does not line up to the corresponding region from one bar to the next.\n\n\nFigure 4.3: Healy, Fig. 1.11. A stacked bar chart, where comparisons across groups is difficult.\n\nThe aspect ratio (Healy Figure 1.12) can make a small change look large, and the reverse by exploiting our perceptions.\n\n\nFigure 4.4: Healy, Fig. 1.12. A demonstration of the effect of aspect ratio on the appearance of rate of change.\n\nThe checkerboard illusion (Healy Figure 1.14) is an example of how our brains process brightness in the context of neighbouring parts of the image. The squares labeled A and B are the same colour, represented by the same underlying data, but look completely different. Colour or greyscale brightness, as a result, is not always a reliable way to indicate a quantity to be compared across regions of a plot. (By contrast, a small number of colours can be very effective at distinguishing a small number of categories.)\n\n\nFigure 4.5: The checker shadow illusion. Wikipedia."
  },
  {
    "objectID": "lessons/104-look-at-data.html#decoding-graphs",
    "href": "lessons/104-look-at-data.html#decoding-graphs",
    "title": "4  Look at data",
    "section": "\n4.6 Decoding graphs",
    "text": "4.6 Decoding graphs\nData are generally described as quantitative or categorical. Each type of data has its own suite of representations and challenges with perception.\nFor quantitative data, the most direct representation is the position of a symbol on a line, like the ‘number line’ of elementary school math classes. Other comparisons such as the lengths of a line, slopes of a line or angles in a pie chart, colour brightness and saturation, and area or volume are more difficult to make quantiative (roughly in that order from easiest to hardest). These orderings are reasonably intuitive, once explained, but are also supported by quantitative experimental data (Healy Figures 1.22-1.24).\nThe options for categorical data are somewhat simpler: position is still useful, then colour hue, and shape (Figure 1.25). The number of categories should be kept small, particularly for colour and shape, or your message will likely be lost in the detail of the visualization."
  },
  {
    "objectID": "lessons/104-look-at-data.html#where-to-put-0",
    "href": "lessons/104-look-at-data.html#where-to-put-0",
    "title": "4  Look at data",
    "section": "\n4.7 Where to put 0?",
    "text": "4.7 Where to put 0?\nA common rule for quantitative displays using position is that you should always include 0 on your axis. The reason is simple: we often want to make comparisons of absolute differences and relative differences. Ratios of two quantities read from distances will only be sensible if the distance from 0 can be seen. Rules which are absolute and always true are rare. For the data shown below, which visualization is better? What if dots were used on the left and bars on the right?\n\n\n\n\nFigure 4.6: A bar chart and dot chart version of the same data. The extent of the horizontal axis differs on the two panels."
  },
  {
    "objectID": "lessons/104-look-at-data.html#lines-shapes-or-connect-the-dots",
    "href": "lessons/104-look-at-data.html#lines-shapes-or-connect-the-dots",
    "title": "4  Look at data",
    "section": "\n4.8 Lines, shapes, or connect the dots?",
    "text": "4.8 Lines, shapes, or connect the dots?\nA common question when plotting a scatter plot of two quantitative variables is whether to use lines, points, or both for the data (see the sketch below.) Which is appropriate? Often data points are discrete and little is known about values between the points; lines can thus be misleading because they give a visual indication of data between points. Lines act as guides joining points with the same symbol, which gives a stronger visual impression and can simplify the message of a complex plot. If the underlying data are continuous and the lines are a reasonable representation of the process being represented, then symbols may be unnecessary. In short, the most appropriate representation depends on your understanding of the data and the message to be communicated.\n\n\n\n\nFigure 4.7: Points or lines?"
  },
  {
    "objectID": "lessons/104-look-at-data.html#thinking-about-your-goals",
    "href": "lessons/104-look-at-data.html#thinking-about-your-goals",
    "title": "4  Look at data",
    "section": "\n4.9 Thinking about your goals",
    "text": "4.9 Thinking about your goals\nAlmost all decisions about data visualizations come down to this: what is the best choice to highlight the most important feature of the dataset? How will your reader interpret your visualization? Does your picture represent the data fairly? Are there perceptual problems that make the picture easy to misinterpret or difficult to understand? As we make graphs throughout the course, we will return to the ideas of this lesson an ask if the features we add are helpful or distorting. We will revise and edit our graphs many times to practice thinking about the effectiveness of many different visual elements."
  },
  {
    "objectID": "lessons/104-look-at-data.html#questions",
    "href": "lessons/104-look-at-data.html#questions",
    "title": "4  Look at data",
    "section": "\n4.10 Questions",
    "text": "4.10 Questions\n\nHow do the numerical summaries for Anscobe’s Quartet and the Datasaurus Dozen fail to capture important features of the underlying data? Create a scenario with some made-up meaning for the data. What would you fail to learn from the summary that would be important for you to know?"
  },
  {
    "objectID": "lessons/104-look-at-data.html#further-reading",
    "href": "lessons/104-look-at-data.html#further-reading",
    "title": "4  Look at data",
    "section": "\n4.11 Further reading",
    "text": "4.11 Further reading\nBoth these chapters are strongly recommended. This lesson is a brief introduction to complex topic. The papers below provide empirical data for some of the claims about perception of visualizations.\n\nHealy Chapter 1\n\nWilke Chapter 1 emphasizes the difference between ugly, bad, and wrong figures, echoing many of the observations made here. The rest of his book addresses these questions in considerable detail from the perspective of someone training students in good and bad features of graphics, focusing on many of the same perceptual issues introduced here.\nVisualization of biomedical data (O’Donoghue et al. 2018)\n\nRainbow colours on hydrological maps mislead readers (Stoelzle and Stein 2021)\n\n\n\n\n\n\n\n\n\nO’Donoghue, Seán I., Benedetta Frida Baldi, Susan J. Clark, Aaron E. Darling, James M. Hogan, Sandeep Kaur, Lena Maier-Hein, et al. 2018. “Visualization of Biomedical Data.” Annual Review of Biomedical Data Science 1 (1): 275–304. https://doi.org/10.1146/annurev-biodatasci-080917-013424.\n\n\nStoelzle, Michael, and Lina Stein. 2021. “Rainbow Color Map Distorts and Misleads Research in Hydrology  Guidance for Better Visualizations and Science Communication.” Hydrology and Earth System Sciences 25 (8): 4549–65. https://doi.org/10.5194/hess-25-4549-2021."
  },
  {
    "objectID": "lessons/105-first-plot.html#goals",
    "href": "lessons/105-first-plot.html#goals",
    "title": "5  Making your first plot",
    "section": "\n5.1 Goals",
    "text": "5.1 Goals\nIn this lesson I will show you how to create a basic plot and then demonstrate a sequence of refinements to the plot."
  },
  {
    "objectID": "lessons/105-first-plot.html#introduction",
    "href": "lessons/105-first-plot.html#introduction",
    "title": "5  Making your first plot",
    "section": "\n5.2 Introduction",
    "text": "5.2 Introduction\nIn this lesson you will start to learn how to use ggplot to visualize data. There is one simple example with several exercises, designed to make the essentials clear in your mind. All plots share the three features explained below, but many plots we draw later in the course are much more complex.\nYou can read these notes as a static document in a web browser, but it’s better to download the R markdown file here, open it in Rstudio, and edit the examples as you read."
  },
  {
    "objectID": "lessons/105-first-plot.html#required-elements-for-a-plot",
    "href": "lessons/105-first-plot.html#required-elements-for-a-plot",
    "title": "5  Making your first plot",
    "section": "\n5.3 Required elements for a plot",
    "text": "5.3 Required elements for a plot\nYou need three elements to make a plot using ggplot:\n\ndata, in the form of tidy data (a data frame or tibble in the language of R)\na mapping from variables in the data to features on the graph (e.g., position on the x- or y-axis, colour, shape, size)\nthe kind of graph to draw, such as a scatter plot, histogram, or bar chart."
  },
  {
    "objectID": "lessons/105-first-plot.html#your-first-plot",
    "href": "lessons/105-first-plot.html#your-first-plot",
    "title": "5  Making your first plot",
    "section": "\n5.4 Your first plot",
    "text": "5.4 Your first plot\nWe will use the gapminder data, which must be installed as part of the gapminder package and is adapted from the data at gapminder.org.\nThe function to create a plot is ggplot. You must specify the data (gapminder) and the mapping between various columns of the data and the features of the graph. Once these basics are established, we add (+) one thing to the graph: an instruction to place a symbol at each x, y combination using the “geometry” geom_point.\nWe’ll have a lot more to say about how data are organized in tables later, but for now you can look at the data by typing View(gapminder) in the Rstudio Console.\n\nlibrary(gapminder)\nlibrary(ggplot2)\nggplot(data = gapminder, \n       mapping = aes(y=lifeExp, x=gdpPercap)) +\n  geom_point()"
  },
  {
    "objectID": "lessons/105-first-plot.html#improving-your-plot",
    "href": "lessons/105-first-plot.html#improving-your-plot",
    "title": "5  Making your first plot",
    "section": "\n5.5 Improving your plot",
    "text": "5.5 Improving your plot\nWe’re going to revise the plot by building on these instructions. I’ll repeat the instructions above, but instead of displaying the figure, I will store the result in a variable (conventionally called p for plot to keep it simple, but you can use any name you like.) This will simplify the examples to follow, highlighting what makes each example different.\n\np &lt;- ggplot(data = gapminder, \n       mapping = aes(y=lifeExp, x=gdpPercap)) +\n       geom_point()\n\nAdd better labels to your plot using labs and x, y, title, subtitle, tag. My example has placeholder text. Get the R markdown file from here and revise the figure to show sensible text in these spaces.\n\np + labs(title=\"Title here\",\n         subtitle=\"A subtitle\",\n         x=\"x axis label\",\n         y=\"y axis label\",\n         caption=\"A caption.\",\n         tag=\"A.\")\n\n\n\n\nAlmost everyone makes the text on their figures too small. How small is too small is obviously a matter of judgement, but it depends on your age (and vision), the medium (on screen for yourself, on a website, in a presentation, printed on paper), and whether the figure is made by you for you, or if you are trying to reach a wider audience. It’s essential to know how to make text bigger! Many visual aspects of a ggplot can be controlled as part of the theme; we will introduce these gradually throughout the course and summarize some of the most important themes in a lesson near the end of the course.\n\np + theme(text = element_text(size=18))\n\n\n\n\nLet’s make a simple use of colour. There are many countries, so that’s not a good choice for a colour feature – there will be too many to distinguish. Let’s colour points by continent. All we need to do is to link the variable continent to the colour aesthetic.\n\np + geom_point(aes(color=continent))\n\n\n\n\nGross domestic product varies widely across countries. The uneven spread over the x-axis makes the visualization hard to read. It’s got a couple of other features, notably the fact that it is always bigger than 0 and that we tend to be interested in multiplicative comparisons, that mean a log-transformation makes a better plot. (This is debatable if you are not used to reading or noticing log scales. Log scales are very useful and well worth the trouble to learn about, so put that on your “to do list” if you haven’t learned how to read them yet.) ggplot makes changing the scale farily easily, at least for a few special cases. The stuff in brackets after scale_x_log10 is optional and makes the labels look a bit prettier. (Notice we’re using a function from a new package here.)\n\nlibrary(scales)\np + scale_x_log10(labels = trans_format('log10', math_format(10^.x)))\n\n\n\n\nThis formatting of the numbers on the x-axis is a bit tricky to get right and to explain. In fact its the sort of thing you might keep in a file of examples if, like me, you really don’t like to see \\(10^3\\) formatted as \\(1.0e3\\). (Try labels = scientific to see this other format.)"
  },
  {
    "objectID": "lessons/105-first-plot.html#exercises",
    "href": "lessons/105-first-plot.html#exercises",
    "title": "5  Making your first plot",
    "section": "\n5.6 Exercises",
    "text": "5.6 Exercises\n\nCombine several features of the examples above into one example: colour, log scale on the x axis, better axis labels and titles, and of course changing the text size.\nRepeat these examples using variables of your choice from the penguins data from the package palmerpenguins"
  },
  {
    "objectID": "lessons/105-first-plot.html#further-reading",
    "href": "lessons/105-first-plot.html#further-reading",
    "title": "5  Making your first plot",
    "section": "\n5.7 Further reading",
    "text": "5.7 Further reading\n\nHealy Section 2.5-2.6\nFor more on aesthetics and mappings from data to visualizations, see Wilke Chapter 2"
  },
  {
    "objectID": "lessons/106-version-control.html#goals",
    "href": "lessons/106-version-control.html#goals",
    "title": "6  What is version control software?",
    "section": "\n6.1 Goals",
    "text": "6.1 Goals\nIn this lesson I will explain what version control is and why we use it. I will also demonstrate how we will use version control with Rstudio and github in this course for tasks, assignments, and the term project."
  },
  {
    "objectID": "lessons/106-version-control.html#introduction",
    "href": "lessons/106-version-control.html#introduction",
    "title": "6  What is version control software?",
    "section": "\n6.2 Introduction",
    "text": "6.2 Introduction\nVersion control software is used to manage the process of creating software. It is commonly used to track changes, manage the revision process of correcting errors and adding new features, track the history of a project through different versions, synchronize contributions from many different people, and facilitate the distribution of software to a broad audience. Version control software is most commonly applied to the production of complex projects like software, but it can be used for text documents, data, web pages, and other applications."
  },
  {
    "objectID": "lessons/106-version-control.html#what-software-is-used-for-version-control",
    "href": "lessons/106-version-control.html#what-software-is-used-for-version-control",
    "title": "6  What is version control software?",
    "section": "\n6.3 What software is used for version control?",
    "text": "6.3 What software is used for version control?\nVersion control software has a history dating back several decades, so there are many different tools available. We will be using one of the most popular packages, known as git (homepage). Git keeps the entire history of a project on your own computer and does not require any central repository. Since version control software is often used to coordinate the work of many authors and to distribute the product, central repositories such as GitHub (homepage) have become very popular, to the point that some people see them as an integrated set of tools. Other popular version control software packages include Mercurial and Subversion. There are several alternatives to GitHub such as bitbucket, gitlab and more.\nWe will be using git and github in this course. There are many add-on tools to help you work with git. We will use the excellent tools built-in to Rstudio. Other tools that are popular, but we will not explore are a simpler git called gitless and many different graphical interfaces."
  },
  {
    "objectID": "lessons/106-version-control.html#why-is-version-control-used-in-data-visualization",
    "href": "lessons/106-version-control.html#why-is-version-control-used-in-data-visualization",
    "title": "6  What is version control software?",
    "section": "\n6.4 Why is version control used in data visualization?",
    "text": "6.4 Why is version control used in data visualization?\nData visualization is the process of combining data with computer code to create a visualization. Both of these parts can change with new information and new ideas, can require synthesis of skills from multiple authors, and benefit from transparency in design and distribution. As a result, version control software is a natural tool to help with the work of data visualization.\nSuppose you develop a data visualization and you want to distribute the result, but you anticipate updating the data and revising the analyses. This sequence of steps is very similar to software development: an initial version is produced and released to users, changes are made, new versions are produced. Users (of software and data visualizations) want to know what version they are using, if there are any updates, and what the changes were between the versions. Version control software can help with these tasks and, once the key concepts are understood, without much extra effort on the part of the team producing the data visualization. Understanding this workflow is a valuable technical skill on its own.\nIn this course we will integrate just enough use of version control to help you see how it can be helpful and to get you past the uncomfortable stage of knowing what version control is without knowing the basics of how to use it. You will be fluent beginner users of the git model of version control by the end of the course."
  },
  {
    "objectID": "lessons/106-version-control.html#how-will-we-use-version-control",
    "href": "lessons/106-version-control.html#how-will-we-use-version-control",
    "title": "6  What is version control software?",
    "section": "\n6.5 How will we use version control?",
    "text": "6.5 How will we use version control?\nCourse work and evaluation from this lesson onward in the course will require you to use git and github. As part of your final project, you will practice using them as tools for collaboration. This course will emphasize the most basic uses of git and give you an opportunity to practice core elements of using git."
  },
  {
    "objectID": "lessons/106-version-control.html#introduction-to-git",
    "href": "lessons/106-version-control.html#introduction-to-git",
    "title": "6  What is version control software?",
    "section": "\n6.6 Introduction to git",
    "text": "6.6 Introduction to git\nGit organizes data in a repository, commonly called a repo. The repo contains a copy of all the files you ask git to track and it tracks all changes you make to the files. One you create a repository you must stage or add files to the repository. Staging is a declaration that the current version of a file is the one you want to added to the repository. Each file must be staged as a separate step. (In Rstudio this just means checking a box.) Once you have staged all the files you want, you commit them to the repository. This updates the repository, moving files from the staged area. Complete copies of new files are stored, but if a file has been changed, only the differences between the old and new versions are retained. All of these changes happen on your own computer, in a sub-folder called .git used by git to keep track of the repository. If you are using a service like github you can then push your changes to the remote location so that they can easily be obtained by others."
  },
  {
    "objectID": "lessons/106-version-control.html#using-git-with-rstudio",
    "href": "lessons/106-version-control.html#using-git-with-rstudio",
    "title": "6  What is version control software?",
    "section": "\n6.7 Using git with Rstudio",
    "text": "6.7 Using git with Rstudio\nHere are the most important steps for using git with Rstudio. It looks like a lot of steps, because I’ve broken down each task into simple steps. In practice, once you are used to the process, it’s all quite simple.\n\nCheck to be sure git is working on your computer\nCreate a new Rstudio project in a new folder and enable version control\nAdd a file to your project\nStage your files to your local git repository\nCommit your staged changes to your local git repository"
  },
  {
    "objectID": "lessons/106-version-control.html#using-github-with-rstudio",
    "href": "lessons/106-version-control.html#using-github-with-rstudio",
    "title": "6  What is version control software?",
    "section": "\n6.8 Using github with Rstudio",
    "text": "6.8 Using github with Rstudio\nThere are a few different ways of working with github and Rstudio. For this course you will be using repositories I have created for you, so I will start by describing that process. At the end of this lesson I will describe the differences in workflow when you create your own repositories.\n\nCreate a github account\nFind the repository you want to use in your account\nAuthorize Rstudio to use your account (GitHub is very particular about security since it is such a popular tool.)\nCreate a new project by cloning the remote repository with Rstudio\nRevise a file in your project (including renaming, creating new files)\nStage those changes on your computer\nCommit those changes to the repository on your computer\nPush the changes to github\nCheck the github website to see that the changes have been received"
  },
  {
    "objectID": "lessons/106-version-control.html#setup",
    "href": "lessons/106-version-control.html#setup",
    "title": "6  What is version control software?",
    "section": "\n6.9 Setup",
    "text": "6.9 Setup\nYou should have already done this step as part of an earlier lesson. The instructions are repeated below in case you missed the step or something went wrong.\n\n6.9.1 On a Macintosh (OSX) computer\nTo install git, open the Terminal tab within Rstudio (or using the application Terminal found in the Applications &gt; Utilities folder) and type\n\nxcode-select --install\n\nand wait a few minutes.\nType git at the terminal to check that the installation worked.\n\n6.9.2 On a Windows computer\nDownload git from git-scm using the download link. Run the installer, accepting all the default options in the many dialog boxes that appear.\nWhen you are done, type git in the Rstudio terminal. If you see some help text displayed instead of an error message you know git is installed.\nDo not move on to the next steps until you have git working properly on your computer. If you can’t get it working on your own computer, use rstudio.cloud for now.\n\n6.9.3 On rstudio.cloud\nGit is already installed on rstudio.cloud. There are several ways to confirm this.\n\nYou can click on the “terminal” tab and type git. You will see a help message instead of an error message.\nUnder the menu item Tools &gt; Version Control &gt; Project setup, you will see git as a choice in the popup menu for “version control system” instead of “none”.\nUsing File &gt; New Project &gt; Version Control &gt; Git to create a new project using a remote repository (such as this sample) will work."
  },
  {
    "objectID": "lessons/106-version-control.html#tell-git-your-name-and-email-address",
    "href": "lessons/106-version-control.html#tell-git-your-name-and-email-address",
    "title": "6  What is version control software?",
    "section": "\n6.10 Tell git your name and email address",
    "text": "6.10 Tell git your name and email address\nWhen you use git to communicate with GitHub, git needs to know your name and email address to help other people know who made the changes you are sending.\nIn the Rstudio “terminal” window (not the “console” window), type the following, placing your name and email in the spaces indicated:\n\ngit config --global user.email \"you\\@example.com\"\ngit config --global user.name \"Your Name\""
  },
  {
    "objectID": "lessons/106-version-control.html#set-up-a-github-account",
    "href": "lessons/106-version-control.html#set-up-a-github-account",
    "title": "6  What is version control software?",
    "section": "\n6.11 Set up a github account",
    "text": "6.11 Set up a github account\n\nCreate a free account at github.com\n\nTell me your github account name by filling in the brightspace survey for Task 2.\nYou need to tell Rstudio how to prove to github that you are allowed to use your own account. Most sites on the internet use passwords for this authentication process, but GitHub has found this to be insecure and a bit inconvenient. Instead we will use a Personal Access Token (PAT). Here are the steps to follow:\n\nInstall the usethis package. For example, type install.packages(\"usethis\") in the R console.\nIn the R console, type usethis::create_github_token(). This will open up a web browser. You will be asked to login and perhaps use two factor authentication. Eventually you will get a page that looks like the figure below. Enter a note to describe the token and pick an expiry date (my suggestions are shown in the screenshot.) Make sure you have at least the boxes shown below checked (repo, user, workflow). Click the green generate token at the bottom of the screen.\nCopy the random numbers and letters that make up your token by clicking the two overlapping blue boxes (or highlight and select copy from the menu).\nIn the R console, type gitcreds::gitcreds_set() and then enter the PAT by pasting it into the console.\n\n\n\n\n\nFigure 6.1: GitHub Personal Access Token creation menu\n\n   These instructions are adapted from a guide called [Happy git with R](https://happygitwithr.com/).\n\n6.11.1 Clone a project from github\nOnce you have sent me your GitHub user ID, I will create two repositories in your account, one for the Tasks for the whole term and one for the Assignments for the whole term.\n\nGo to github.com. Make sure you are logged in. You will see a list of repositories on the left side of your screen.\nClick on the one called “data-viz-tasks-” followed by your ID\nClick on the green button labelled “Code” and copy the https link by clicking on the two overlapping squares (or cutting and pasting the URL).\nStart Rstudio.\nIf you are already working on some other files, choose: Session &gt; New session\nUse the Rstudio menu: File &gt; New Project… &gt; Version control &gt; Git\nPaste in the repository link (URL). Use the “Browse…” button to select a directory to store your project. Choose a deliberate place on your computer (e.g., on the Desktop, in a folder called STAT2430, or whatever you prefer.).\nClick “Create Project”.\n\nThe necessary files will be copied from GitHub onto your computer and in a few moments you will have a new R project open on your computer.\n\n6.11.2 Edit an existing markdown file\n\nFrom the Files tab (lower right of the screen), click on one of the task Rmd files.\nMake a minor change, such as adding your name in the indicated spot near the top of the file.\nSave the file.\n\n6.11.3 Stage and commit your changes\n\nClick the “Git” tab in the upper right of the Rstudio window.\nYou should see the Rmd file you just created with a status “M” (in blue) that means the file has been modified.\nCheck the “Staged” box beside all the file. You have now told git you want to add these to your local repository when you next commit changes.\nClick “Commit”. A pop-up window will show you the files you are committing with changes. You are asked for a “commit message” in the upper right. Type a short informative message here. This is a valuable record of what you were hoping to achieve with these changes. I’ll type “learning git and making my first commit to my first repository”.\nClick “Commit”. A message box appears showing you what happened. I don’t usually read this.\nClick “Close”.\nClose the Commit popup by using the “X” button to close a window on your computer.\n\n6.11.4 Make some more changes to your files\nYou should notice that the “status” display in the Git tab is now blank. There are no differences between the files in your R project and your local git repository. Let’s see what happens when we change this.\n\nMake a simple change to your Rmd file. For example, set the date to tomorrow. Save the file.\nNotice that the file now appears in your Git status tab. There should be a blue M beside the file name to indicate the file has been modified.\nClick the staged checkbox.\nCommit the changes to your local repository.\n\n6.11.5 Publish your changes to GitHub\nBefore your work can be graded, you need to send it from your computer to GitHub. Once it is there, I can download it to my computer. To publish these files on github you need to push the changes from your computer to the github (remote) repository.\nRepositories on github can be public (so anyone can see them) or private (so that only you and people you give explicit permission can see them.) The course repositories I have created for you are private (and the TA and I can both edit them.)\n\nClick the green up arrow in the Git tab to “push” your new changes to github.\nReload the github window in your web browser. The start of your commit message should appear next to the files you changed.\n\nCongratulations! You are a git and github novice now."
  },
  {
    "objectID": "lessons/106-version-control.html#creating-your-own-repositories",
    "href": "lessons/106-version-control.html#creating-your-own-repositories",
    "title": "6  What is version control software?",
    "section": "\n6.12 Creating your own repositories",
    "text": "6.12 Creating your own repositories\nFor all coursework you will use repositories I have created. That is the workflow described above. If you want to create your own repositories for other work and learning, here are some steps to get you started.\n\n6.12.1 Create a project\n\nStart Rstudio.\nIf you are already working on some other files, choose: Session &gt; New session\nUse the Rstudio menu: File &gt; New Project… &gt; New Directory &gt; New Project\nGive the directory for your new project and place in a deliberate place on your computer (e.g., on the Desktop, in a folder called STAT2430, or whatever you prefer.). Make sure “Create a git repository” is checked. Then click “Create Project”.\n\n6.12.2 Create a new markdown file\n\nUse Rstudio menu: File &gt; New file &gt; R Markdown …\nGive the document a title and make sure your name appears in the space for Author. Use HTML output.\nA standard template for an R markdown file will be created.\nSave the file in your new project. Call it something like “example” or “testing”.\nIf you like, click “knit” to see the output of the R markdown document. We’ll learn more about this later.\n\n6.12.3 Stage and commit your changes\n\nClick the “Git” tab in the upper right of the Rstudio window.\nYou should see three files – the Rmd file you just created plus “.gitignore” and and Rproj file that stores information about your project.\nCheck the “Staged” box beside all three files. You have now told git you want to add these to your local repository when you next commit changes.\nClick “Commit”. A pop-up window will show you the files you are committing with changes (everything, since we’re making our first commit). You are asked for a “commit message” in the upper right. Type a short informative message here. This is a valuable record of what you were hoping to achieve with these changes. I’ll type “learning git and making my first commit to my first repository”.\nClick “Commit”. A message box appears showing you what happened. I don’t usually read this.\nClick “Close”.\nClose the Commit popup by using the “X” button to close a window on your computer.\n\n6.12.4 Make some more changes to your files\nYou should notice that the “status” display in the Git tab is now blank. There are no differences between the files in your R project and your local git repository. Let’s see what happens when we change this.\n\nMake a simple change to your Rmd file. For example, set the date to tomorrow. Save the file.\nNotice that the file now appears in your Git status tab. There should be a blue M beside the file name to indicate the file has been modified.\nCheck the staged button.\nCommit the changes to your local repository.\n\n6.12.5 Connect to github\nTo publish these files on github you have to do a few steps.\n\nYou need to create a repository on github.\nYou need to connect your local repository to the github repository.\nYou need to push the changes from your computer to the github (remote) repository.\n\nRepositories on github can be public (so anyone can see them) or private (so that only you and people you give explicit permission can see them.) We’ll make this repository private.\n\nGo to github.com.\nClick the bright green button “New” on the left next to the word “Repositories”.\nGive your repository a name. Don’t use spaces; use - or _ to connect words. I suggest “my-first-repository”.\nClick the radio button beside “Private”\nDon’t check any other boxes.\nClick the green button “Create repository”\nCopy the third last line of code on the next screen: git branch -M main.\nGo back to Rstudio. Paste the git command into the “Terminal” window and hit enter.\nCopy the second last line of code on the next screen. On my example its git remote add origin https://github.com/AndrewIrwin/my-first-repository.git. Yours will have your github name and repository name. Paste it into the Terminal window.\nDo the same thing with the last line of code: git push -u origin main\n\nGo back to github and refresh your window. You should see your three files from your R project there in the web browser now.\n\n6.12.6 Make and push another change\nYou’re all done the setup now. Now we will practice the normal day-to-day work on an R project. These are the same steps you would perform with a repository cloned from github (see the previous section).\n\nMake a change to your Rmd file. Perhaps change the title or add your middle initial.\nSave the file.\nStage and commit your changes to your local repository.\nClick the green up arrow in the Git tab to “push” your new changes to github.\nReload the github window in your web browser. The start of your commit message should appear next to the files you changed."
  },
  {
    "objectID": "lessons/106-version-control.html#clean-up",
    "href": "lessons/106-version-control.html#clean-up",
    "title": "6  What is version control software?",
    "section": "\n6.13 Clean up",
    "text": "6.13 Clean up\nIf you don’t want to keep this repository, you can get rid of it in two steps.\n\nDelete the folder from your computer by dragging it to the trash (Finder on Macintosh, File explorer on Windows)\nDelete it from github. On the repository page on github.com, go to: Settings, then scroll to the bottom. From the “danger zone” choose “Delete this repository”. Github really doesn’t want you to do this by mistake, so it requires two confirmation steps. You must type the repository name to confirm and then enter your github password to confirm."
  },
  {
    "objectID": "lessons/106-version-control.html#resources",
    "href": "lessons/106-version-control.html#resources",
    "title": "6  What is version control software?",
    "section": "\n6.14 Resources",
    "text": "6.14 Resources\n\nRstudio help for git\nSoftware Carpentry tutorial using git with Rstudio\n\nHappy Git with R\nHighlights of key features of git and github for developers\n\nAdvanced Git cheat sheet\nA more advanced look at the internals of git\n\nArticle on teaching statistics with git and github, for students and instructors."
  },
  {
    "objectID": "lessons/302-staying-organized.html#what-is-a-file-what-is-a-folder",
    "href": "lessons/302-staying-organized.html#what-is-a-file-what-is-a-folder",
    "title": "Staying organized",
    "section": "What is a file? What is a folder?",
    "text": "What is a file? What is a folder?\nWhen you use an application on a desktop computer you are usually invited to give your work a name and save it to a file in a particular place (a folder) on your computer storage so that it can be accessed later. A big exception occurs when you use a web browser; in this case you rarely permanently store your work on your computer, usually your work is stored “in the cloud”. By contrast, on a phone or tablet data you create is usually associated with the app you are using and there is no way to work with the data except through the app.\nWhat do all these words (save, file, folder ) mean? I need to explain some of these ideas because we will be managing a lot of files — for both our writing and finding the data we want to use.\nTo save data means that you want to keep it safe against loss. Work done in many applications is temporary – it will be lost if you quit the application – unless you save the data. A file is a place on the computer where you store your data. Usually files are identified by their file name. Since a computer has a vast number of files, its conveient to put them in a folder or directory. Again there are many folders, so folders are placed inside other folders. Files and folders are references to old physical storage technology for paper. When you save data to a file in a folder on your computer, it’s important to remember where you put it so that you can find it again! This all sounds mundane, but picking good names for files and folders to store them in is surprisingly tricky1. It’s well worth a bit of time to develop some easy to use habits.1 Most guides you find for organizing files emphasize the use of version numbers in file names. There is a better way – see the lesson on version control!\nHere are some suggestions to help you get and stay organized."
  },
  {
    "objectID": "lessons/302-staying-organized.html#put-everything-in-a-folder",
    "href": "lessons/302-staying-organized.html#put-everything-in-a-folder",
    "title": "Staying organized",
    "section": "Put everything in a folder",
    "text": "Put everything in a folder\nWhen you first start learning a computing tool like R, most people just put their files in “Downloads” or “Documents” or even on the “Desktop” with no particular thought to names or organization. This is fine when you are first experimenting, but it quickly gets out of control, especially when you want to keep one project separate from another, add data files, or want to share files with a collaborator.\nWhenever you start a new project, such as a course, create a folder with an easy to understand name like “STAT2430” or “Data-Visualization”. Spaces in file names in general are fine, but if you plan to share files with others its usually best to use a dash (-) or underscore (_) instead of spaces, since spaces in file names cause problems with some computing tools.\nAs soon as you start work on a new project, you should give some thought to backups too. I put project folders in ‘Dropbox’ or ‘One Drive’ to reduce the chance of losing my work."
  },
  {
    "objectID": "lessons/302-staying-organized.html#sub-projects",
    "href": "lessons/302-staying-organized.html#sub-projects",
    "title": "Staying organized",
    "section": "Sub-projects",
    "text": "Sub-projects\nFor each component of a project, create a new folder (directory) called something meaningful like “Assignments”. When I open R, instead of creating a new R markdown file right away, I first start a new project (File &gt; New Project…). This new project can be in this new folder or you can create a new folder.\nYou can choose to start using version control with the project. I suggest you do! Version control on your local machine takes very little extra effort and can be really handy if (when!) you delete something you didn’t mean to, or decide to undo a whole bunch of changes that didn’t work out the way you hoped. This works on your own computer – no need to use github.\nA project encourages you keep all your files together in the same place. Rstudio will remember which files were open when you quit and re-open them for you. It makes the process of switching from one project to another much easier. You can have two different Rstudio sessions open in different windows at the same time. Each R session has its own window and workspace and operates completely independently of any other R session you have open. I suggest you don’t mix files from multiple projects in an R session; that works against your effort to stay organized."
  },
  {
    "objectID": "lessons/302-staying-organized.html#files-and-folders",
    "href": "lessons/302-staying-organized.html#files-and-folders",
    "title": "Staying organized",
    "section": "Files and Folders",
    "text": "Files and Folders\nYou can put all your files in this project in this one folder, but if you have different kinds of files – R markdown documents, data files, figures, and presentations, it can help you stay organized to create a sub-folder for each type of file. I find it particularly useful to do this for two kinds of files. Any file I get from somewhere else, typically a data file from a collaborator or the internet, goes in its own folder (“sources”) and I never modify these files. That way I can clean up copies of a file and always know what the original file looked like. This is not really necessary if you use version control, but the original data file is the most likely file I’ll want to use again and I think the duplication is worth the effort. Second, any files that are created by my R code go in their own folder (“output”): figures, data tables, or the results of time-consuming calculations, for example, because I know they can always be deleted and recreated later on. Putting them in their own folder reminds me I don’t need to use version control on them; my code created them and there is no need to keep the results."
  },
  {
    "objectID": "lessons/302-staying-organized.html#naming-conventions",
    "href": "lessons/302-staying-organized.html#naming-conventions",
    "title": "Staying organized",
    "section": "Naming conventions",
    "text": "Naming conventions\nIt’s helpful to create informative names for files. If two files are related, use similar names with a different suffix (world-map.R and world-map.png, for example). If some steps need to be performed in a particular order, a simple way is to use a number (01, 02, etc) as a prefix. I used this convention when writing these notes to help me keep the sections in order. For advanced projects, there are specialty tools for performing calculations in a particular order, but a simple numbering scheme works for many projects that outgrow simple sequencing of calculations in a single file."
  },
  {
    "objectID": "lessons/302-staying-organized.html#resources",
    "href": "lessons/302-staying-organized.html#resources",
    "title": "Staying organized",
    "section": "Resources",
    "text": "Resources\n\nHealy. Appendix. Managing projects and files\n\nAvoid using directory paths that won’t exist on someone else’s computer when you share files by using the Here pacakge (github).\nR for Data Science notes on scripts and projects\n\nDrake package and manual\n\nIf you are worried about versions of packages changing, you can keep track of the packages you used and their versions using checkpoint or packrat"
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#goals",
    "href": "lessons/107-grammar-graphics.html#goals",
    "title": "7  Grammar of Graphics",
    "section": "\n7.1 Goals",
    "text": "7.1 Goals\nIn this lesson I will explain the concept behind the “grammar of graphics”. The emphasis is on the idea and not the particular computing environment (R and ggplot2) that we will use."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#introduction",
    "href": "lessons/107-grammar-graphics.html#introduction",
    "title": "7  Grammar of Graphics",
    "section": "\n7.2 Introduction",
    "text": "7.2 Introduction\nBy the end of this lesson you should understand the design of the grammar of graphics. This will help you develop a mental model of what the R functions for plotting do, making the process easier to remember and modify to suit your own goals.\nThe next lesson will show you the R commands that accompany each concept.\nAfter both lessons, and some practice, you should be able to create new visualizations of many different types, customize them, and be ready to learn many more methods and extensions to the basics."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#the-layers-of-a-plot",
    "href": "lessons/107-grammar-graphics.html#the-layers-of-a-plot",
    "title": "7  Grammar of Graphics",
    "section": "\n7.3 The layers of a plot",
    "text": "7.3 The layers of a plot\nA plot is assembled in layers (see below), starting with data, then adding aesthetic mappings from variables to features, geometries that determine how the features are displayed, optional facets for splitting the plot into subplots, statistics for computing new variables from the data, a coordinate system (usually Cartesian), and finally themes for modifying the appearance of the plot independently from the data.\n\n\n\n\nFigure 7.1: ?(caption)"
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#data",
    "href": "lessons/107-grammar-graphics.html#data",
    "title": "7  Grammar of Graphics",
    "section": "\n7.4 Data",
    "text": "7.4 Data\nData always come from a data frame (or tibble) with columns representing variables and each row corresponding to a different observation. We’ll have a lot more to say about organizing and manipulating data later."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#aesthetic-mappings",
    "href": "lessons/107-grammar-graphics.html#aesthetic-mappings",
    "title": "7  Grammar of Graphics",
    "section": "\n7.5 Aesthetic mappings",
    "text": "7.5 Aesthetic mappings\nVariables in your data are connected to features of your visualization, such as position on the x and y axes, colours, shapes, sizes, and other features with a mapping that connects the two.\nEach mapping has an associated scale. If a variable is represented on the x axis, the scale translates a numerical value onto a position. For the aesthetic shape, the scale converts a categorical value into a square, circle or other shape. Similarly there are scales for colours, line types, symbol sizes and more.\nAesthetic mappings can be used in several ways in the same plot, for example to help place symbols and text labels. A process called inheritance allows this to happen. Inherited aesthetics can be overridden too; thinking this through can be a subtle but have dramatic effects on your plot. Be on the lookout when we get to examples.\nSometimes you will want to customize a plot by making all symbols squares or changing all the lines to green. You don’t use aesthetic mappings for this, because you are not connecting a variable to a feature of the graph. Instead you will override the default plot appearance specifying a particular colour or shape as part of the geometry."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#geometries",
    "href": "lessons/107-grammar-graphics.html#geometries",
    "title": "7  Grammar of Graphics",
    "section": "\n7.6 Geometries",
    "text": "7.6 Geometries\nA geometry is a visual representation of data: points, lines, shaded regions, boxplots, histograms, tiles, text labels, and many, many more. The representation is defined by the geometry using the data mapped to aesthetic features (position, colour) of the geometry. You need at least one geometry to create a plot, but you can combine two or more geometries (lines and points for example) to make more complex plots."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#facets",
    "href": "lessons/107-grammar-graphics.html#facets",
    "title": "7  Grammar of Graphics",
    "section": "\n7.7 Facets",
    "text": "7.7 Facets\nA simple plot has one facet – think about standard x-y plots that show all the data in one panel. You can also separate out the data in one plot into many facets (or panels, subplots), using a variable from the dataset to define which data appear on each facet. Depending on the data you might arrange facets in a grid with common x and y axes on all the facets. Or you might arrange them in a line with different scales on each facet. Facets can make complex plots! We will skip over facets on our discussion of making plots in the next lesson, but come back to them later."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#statistics",
    "href": "lessons/107-grammar-graphics.html#statistics",
    "title": "7  Grammar of Graphics",
    "section": "\n7.8 Statistics",
    "text": "7.8 Statistics\nSometimes you want to use a variable for a feature – position on the y axis – but before you do that, you want to perform some computations. Perhaps you want to plot a mean or compute a standard error of the mean. Or maybe you want to count the number of occurrences of a level of a categorical variable. There are two ways to go about this. You can compute the derived variables “by hand” – writing R code to do it. Or you can use stat features of ggplot to compute these for you. For routine plots like bar graphs, histograms, and boxplots, its generally easiest to let ggplot do these calculations for you. For less commonly used plot types, or custom plots you invent, you will want do to the calculations yourself. We’ll talk about summarizing data in a future lesson."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#coordinates",
    "href": "lessons/107-grammar-graphics.html#coordinates",
    "title": "7  Grammar of Graphics",
    "section": "\n7.9 Coordinates",
    "text": "7.9 Coordinates\nUsually we plot data on the familiar (Cartesian) x-y axes. But you can use polar coordinates. Or project a 3d model onto a plane. Or draw lots of other shapes like trees or maps. Cartesian coordinates are so familiar we usually pass over this choice without even thinking."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#themes",
    "href": "lessons/107-grammar-graphics.html#themes",
    "title": "7  Grammar of Graphics",
    "section": "\n7.10 Themes",
    "text": "7.10 Themes\nAll other features of the visualization are combined into the idea of the theme. This includes colour schemes, line thicknesses, how tick marks are drawn, the font type and size, and where legends are placed. We will look at these features – in fact we’ve already seen a simple example of changing font size – throughout the course and collect a bunch of examples at the end."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#extensions",
    "href": "lessons/107-grammar-graphics.html#extensions",
    "title": "7  Grammar of Graphics",
    "section": "\n7.11 Extensions",
    "text": "7.11 Extensions\nThere are several kinds of extensions to the basic ggplot plots. Many R packages define new geometries, colour palettes, map projections, and themes. A few packages define amazing new ways of transforming ggplots, for example to create animations or three dimensional rendering using raytracing."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#summary",
    "href": "lessons/107-grammar-graphics.html#summary",
    "title": "7  Grammar of Graphics",
    "section": "\n7.12 Summary",
    "text": "7.12 Summary\nThat’s the basic idea behind the grammar of graphics approach to plotting. It takes a bit of getting used to, and seems like a lot of work when you first get started, but you’ll see that the flexibility and structure the grammar provides allows you to think creatively to build a huge number of visualizations from these few elements. Even more important, a consistent language for describing the visualization of data has made very complex tools relatively easy to use, even when many different people develop those tools."
  },
  {
    "objectID": "lessons/107-grammar-graphics.html#further-reading",
    "href": "lessons/107-grammar-graphics.html#further-reading",
    "title": "7  Grammar of Graphics",
    "section": "\n7.13 Further reading",
    "text": "7.13 Further reading\n\nA chapter on these ggplot concepts from a data science course\n\n\nCheatsheets for R, in particular a ggplot cheatsheet summarizing a huge amount of information in two pages\nThe R graph gallery\n\nWickham’s article on the grammar of graphics (Wickham 2010)\n\n\n\n\n\n\n\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098."
  },
  {
    "objectID": "lessons/108-ggplot.html#goals",
    "href": "lessons/108-ggplot.html#goals",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.1 Goals",
    "text": "8.1 Goals\nIn this lesson I will demonstrate how to use R and ggplot2 to make visualizations using the ideas from the previous lesson. The emphasis is on the mechanics of making the visualizations. In time we will integrate the ideas about what features of visualizations work best to convey an idea that were introduced at the start of the course."
  },
  {
    "objectID": "lessons/108-ggplot.html#introduction",
    "href": "lessons/108-ggplot.html#introduction",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.2 Introduction",
    "text": "8.2 Introduction\nBy the end of this lesson you should understand how to make many different plots using ggplot. The mental model developed in the previous lesson will connect directly to the R commands in this lesson.\nIncidentally, Hadley Wickham, who originally developed ggplot2 is from New Zealand and one consequence is that he allows for “British” and “American” spellings of some words. So you can use color or colour. In a future lesson when we summarize data you’ll see we can write summarize or summarise. If I switch back and forth, don’t get confused. Both are OK."
  },
  {
    "objectID": "lessons/108-ggplot.html#data",
    "href": "lessons/108-ggplot.html#data",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.3 Data",
    "text": "8.3 Data\nWe will use the diamonds dataset for examples in this lesson. As always, you should use str or View to take a look at the data to familiarize yourself with the variables and the number of rows in the data before you begin to make a plot.\n\nstr(diamonds)\n\ntibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n $ cut    : Ord.factor w/ 5 levels \"Fair\"&lt;\"Good\"&lt;..: 5 4 2 4 2 3 3 3 1 3 ...\n $ color  : Ord.factor w/ 7 levels \"D\"&lt;\"E\"&lt;\"F\"&lt;\"G\"&lt;..: 2 2 2 6 7 7 6 5 2 5 ...\n $ clarity: Ord.factor w/ 8 levels \"I1\"&lt;\"SI2\"&lt;\"SI1\"&lt;..: 2 3 5 4 2 6 7 3 4 5 ...\n $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\n\n\nThis is a large dataset, with over 50,000 rows. There are 7 quantitative variables and three categorical variables. Read the help page on the dataset to learn more."
  },
  {
    "objectID": "lessons/108-ggplot.html#aesthetic-mappings-and-geometries",
    "href": "lessons/108-ggplot.html#aesthetic-mappings-and-geometries",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.4 Aesthetic mappings and geometries",
    "text": "8.4 Aesthetic mappings and geometries\nWe usually pick the aesthetic mappings once we’ve thought about what geometry we want to use. The goal of this lesson will be to demonstrate some of the basics: histogram, box plot, and scatter plot. For a survey of other common geometries, consult [Wilke, chapter 5]](https://clauswilke.com/dataviz/directory-of-visualizations.html). Even these three kinds give us lots of room to show of the power of the grammar of graphics."
  },
  {
    "objectID": "lessons/108-ggplot.html#histogram",
    "href": "lessons/108-ggplot.html#histogram",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.5 Histogram",
    "text": "8.5 Histogram\nLet’s draw a histogram of the price of diamonds in the dataset. We map price to the x axis and request the histogram geometry.\n\ndiamonds |&gt; ggplot(aes(x=price)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is very skewed distribution – with lots more observations at the low end of the price range.\nMaybe one of the categorical features will help us see features in the data. Let’s break the bars down by cut using colour.\n\ndiamonds |&gt; ggplot(aes(x=price, fill=cut)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor skewed distribution of positive numbers, a log transform can sometimes help reveal patterns. Let’s change the scale to see if that works.\n\ndiamonds |&gt; ggplot(aes(x=price, fill=cut)) + \n  geom_histogram() +\n  scale_x_log10()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nStacked bar graphs like this are interesting, but they can be hard to read. Is the distribution the same for all the cuts? Or are there more Premium and Very Good cuts for the more expensive diamonds? Let’s try a few different ways to split the histogram.\nWe can modify the geometry by modifing the histogram geom. It’s helpful to have fewer bars in this histogram, so I’ve set the number of bars to 10 using bins=10.\n\ndiamonds |&gt; ggplot(aes(x=price, fill=cut)) + \n  scale_x_log10() + \n  geom_histogram(bins = 10, position=\"dodge\")\n\n\n\n\nThe peak for Ideal is definitely at a lower price than the peak for Premium or Very Good."
  },
  {
    "objectID": "lessons/108-ggplot.html#box-plots",
    "href": "lessons/108-ggplot.html#box-plots",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.6 Box plots",
    "text": "8.6 Box plots\nBox plots are useful for showing distributions too. You can draw a box plot with one quantitative variable, or with a quantitative variable and a categorical variable. You can use either x or y for the quantitative variable. A plot with too many colours is hard to read, but we can interpret lots of side-by-side boxplots. So I’ll switch to clarity for the categorical variable.\n\ndiamonds |&gt; ggplot(aes(x = price, y = clarity)) + geom_boxplot()\n\n\n\n\nAs before the distributions are skewed, so let’s use the log transform again. Notice how similar the code is to the boxplots.\n\ndiamonds |&gt; ggplot(aes(x = price, y = clarity)) + \n  geom_boxplot() + scale_x_log10()\n\n\n\n\nIf you are willing to read a complex plot, you can fill the boxes using cut. (Try color= instead of fill= to compare the two ways of using colour.) This figure is probably too complicated to show someone else, but might be useful as an exploratory plot to see a lot of information in a small space. Think of it – this is a summary of over 50,000 prices across two cateogorical variables with 5 x 8 = 40 different combinations!\n\ndiamonds |&gt; ggplot(aes(x = price, y = clarity, fill = cut)) + \n  geom_boxplot() + scale_x_log10()"
  },
  {
    "objectID": "lessons/108-ggplot.html#scatter-plot",
    "href": "lessons/108-ggplot.html#scatter-plot",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.7 Scatter plot",
    "text": "8.7 Scatter plot\nFor our third geom, we will use geom_point to make a scatter plot. Just knowing that you can probably create the plot below by modifying the code above. In the code below, I changed geom_boxplot to geom_point, changed fill to color and changed clarity to carat to have a second quantative variable on the y axis.\n\ndiamonds |&gt; ggplot(aes(x = price, y = carat, color= cut)) + \n  geom_point() + scale_x_log10()\n\n\n\n\nThat’s too many points on a scatter plot! There are a few tricks you can use, like making the points smaller and making them partly transparent – but they don’t really help with this much data.\n\ndiamonds |&gt; ggplot(aes(x = price, y = carat, color= cut)) +\n  geom_point(alpha = 0.5, size = 0.2) + \n  scale_x_log10()"
  },
  {
    "objectID": "lessons/108-ggplot.html#two-dimensional-histogram",
    "href": "lessons/108-ggplot.html#two-dimensional-histogram",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.8 Two dimensional histogram",
    "text": "8.8 Two dimensional histogram\nWhat to do? Let’s create a histogram with two quantitative variables, and show the height of each bar using color.\n\ndiamonds |&gt; ggplot(aes(x = price, y = carat)) + \n  geom_bin2d() + scale_x_log10()\n\n\n\n\nAccurate quantitative assessment is hard to make (basically impossible) with colour brightness, but you can see the price and carat combinations for most of the diamonds. We had to give up using colour for clarity. We’ll return to this data when we talk about facets in a future lesson to see how we can add in one more categorical variable.\nWe can do a little better with a contour plot instead of colours. You can even add color=cut back in if you like. Try geom_density_2d_filled for an interesting variant.\n\ndiamonds |&gt; ggplot(aes(x = price, y = carat)) + \n  geom_density_2d() + scale_x_log10()\n\n\n\n\n\n8.8.1 Statistics\nWe said that in addition to connecting variables to aesthetic features, we could use statistical transformations to create new derived variables for our plots. So let’s try that!\nInstead of plotting a point for each diamond in the dataset, let’s compute averages and standard errors for all the diamonds group by clarity.\n\ndiamonds |&gt; ggplot(aes(x = price, y = clarity)) + \n  stat_summary(fun.data = \"mean_se\") + scale_x_log10()\n\n\n\n\nNow adding a colour for each cut doesn’t make the plot too complicated.\n\ndiamonds |&gt; ggplot(aes(x = price, y = clarity, color=cut)) +\n  stat_summary(fun.data = \"mean_se\") + scale_x_log10()\n\n\n\n\nMost of the stat_ functions are directly linked to geom_ functions, but a few like stat_summary or stat_unique are handy on their own."
  },
  {
    "objectID": "lessons/108-ggplot.html#scales",
    "href": "lessons/108-ggplot.html#scales",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.9 Scales",
    "text": "8.9 Scales\nWe’ve seen how scales can be used to transform the x axis, but there is a lot more we can do.\nFirst, we can set the limits of the axis anywhere we want, to highlight some values or expand the range. (Maybe we have a very specific price range in mind for our data analysis.)\n\ndiamonds |&gt; ggplot(aes(x = price, y = clarity, color=cut)) +\n  stat_summary(fun.data = \"mean_se\") +\n  scale_x_log10() +\n  xlim(2000,4000)\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\nWarning: Removed 43582 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nThis is an example of using the power of ggplot and accidentally shooting your own (data) foot off. The data outside this x range were discarded before the mean and standard error were computed! We got a warning, but it was hard to understand! So this is dangerous with summary statistics. (Another reason we will learn to summarize data on our own in a future lesson.)\nIt’s perfectly safe with raw unsummarized data. We still get a warning, but all the dots shown are untransformed, so we don’t need to wonder if the axis limits were set before or after transforming the data.\n\ndiamonds |&gt; ggplot(aes(x = price, y = carat, color=cut)) +\n  geom_point(size=0.1) + scale_x_log10() +\n  xlim(2000,4000) + ylim(0,1.7)\n\n\n\n\nColors are also controlled with a scale. We’ll have a whole lesson on colour, so here is just one example.\nThe yellow we used before didn’t won’t look good printed in a report, so let’s change the range of the colours.\n\ndiamonds |&gt; ggplot(aes(x=price, fill=cut)) + scale_x_log10() +\n  geom_histogram(bins = 10, position=\"dodge\") +\n  scale_fill_viridis_d(begin = 0.0, end = 0.8)\n\n\n\n\nThe viridis colour scale is supposed to be colour-blind friendly and to translate well when printed in gray scale on paper. It’s a range of colours selected between two extremes. Experiment with different values for begin and end between 0 and 1."
  },
  {
    "objectID": "lessons/108-ggplot.html#annotations",
    "href": "lessons/108-ggplot.html#annotations",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.10 Annotations",
    "text": "8.10 Annotations\nThe most important annotations are labels for the axes, guides for colours and shapes, and the title, subtitle, and caption. Here’s an example showing how to change each one using the labs (for labels) function.\n\ndiamonds |&gt; ggplot(aes(x=price, fill=cut)) + scale_x_log10() + \n  geom_histogram(bins = 10, position=\"dodge\") +\n  scale_fill_viridis_d(begin = 0.0, end = 0.8) +\n  labs(x = \"Price ($, log scale)\",\n       y = \"Number of diamonds\",\n       fill = \"Cut\",\n       title = \"Diamond price varies with cut quality\",\n       subtitle = \"I don't often use subtitles, but you can\",\n       caption = \"For the source of the data or other note\")\n\n\n\n\nAnother kind of annotation adds text to a figure. It’s called an annotation instead of a geom because the annotation is a custom thing you add that doesn’t come from the data. Sometimes this is a corporate branding graphic. Or a cartoon reminding the reader what the data are about. Here I’ll add a text message.\n\ndiamonds |&gt; ggplot(aes(x=price, fill=cut)) + scale_x_log10() + \n  geom_histogram(bins = 10, position=\"dodge\") +\n  annotate(geom=\"text\", x = 1300, y = 4500, \n           label = \"Compare the peaks for\\nIdeal and Good.\",\n           hjust = 0, vjust = 0.5, size = 5)\n\n\n\n\nYou can add annotations in the shape of points or arrows too.\nA better way to annotate is to create a data frame with x and y locations and a label. Here I’ll find the average price and carat for each combination of cut and clarity, use colour for cut and add a text label for clarity. We’ll learn more about summarizing data later, so feel free to skip over the calculation and focus on the plotting for now.\n\ns &lt;- diamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(price = mean(price), carat = mean(carat),\n            .groups = \"drop\")\ns |&gt; ggplot(aes(x= price, y = carat, color = cut)) + scale_x_log10() +\n  geom_point() +\n  geom_text(aes(label = clarity ))\n\n\n\n\nThere’s a few problems with that graph! The labels are coloured too. The color scale for “cut” looks strange. The text labels are on top of the points.\nThe colour of the labels comes from the inheritance of the aesthetics. It’s easy to fix. Only map clarity to a colour in the geom_point.\n\ns |&gt; ggplot(aes(x= price, y = carat)) + scale_x_log10() +\n  geom_point(aes(color=cut)) +\n  geom_text(aes(label = clarity ))\n\n\n\n\nA simple change makes a huge difference.\nWe can use geom_text_repel from the ggrepel package to fix the placement of the labels. I’ll shrink the size of the text a bit too.\n\nlibrary(ggrepel)\ns |&gt; ggplot(aes(x= price, y = carat)) + scale_x_log10() +\n  geom_point(aes(color=cut)) +\n  geom_text_repel(aes(label = clarity ), size = 3)\n\n\n\n\nThere are too many labels on the plot so it’s not a good final visualization, but it demonstrates how to add labels and make a plot that might be very useful for you as you explore a dataset."
  },
  {
    "objectID": "lessons/108-ggplot.html#theme",
    "href": "lessons/108-ggplot.html#theme",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.11 Theme",
    "text": "8.11 Theme\nThe theme allows you to set text font and size for labels and numbers on scale, line thicknesses for axes and ticks, position of the guides and many other features. You can also use predefined themes created by others. Here are a few examples that I find useful as an introduction to this topic.\nMy favourite gets rid of the gray background.\n\ns |&gt; ggplot(aes(x= price, y = carat, color = cut)) + scale_x_log10() +\n  geom_point() +\n  theme_bw()\n\n\n\n\nSecond favourite makes the text larger for all elements.\n\ns |&gt; ggplot(aes(x= price, y = carat, color = cut)) + scale_x_log10() +\n  geom_point() + theme_bw() +\n  theme(text = element_text(size=20))\n\n\n\n\nIf you have room you can put the guide inside the plot. The coordinates range from 0 to 1 on both scales from left to right and bottom to top.\n\ns |&gt; ggplot(aes(x= price, y = carat, color = cut)) + scale_x_log10() +\n  geom_point() + theme_bw() +\n  theme(text = element_text(size=20),\n        legend.position = c(0.15,0.75))"
  },
  {
    "objectID": "lessons/108-ggplot.html#further-reading",
    "href": "lessons/108-ggplot.html#further-reading",
    "title": "8  Using the grammar of graphics",
    "section": "\n8.12 Further reading",
    "text": "8.12 Further reading\n\nHealy Chapter 3 on making plots\n\nA chapter on these ggplot concepts from a data science course\n\nA ggplot cheatsheet summarizing a huge amount of information in two pages\nA guide to themes from the ggplot2 book\n\nA whole book on ggplot2"
  },
  {
    "objectID": "lessons/110-summarizing-data.html#creating-new-variables-with-mutate",
    "href": "lessons/110-summarizing-data.html#creating-new-variables-with-mutate",
    "title": "9  Summarizing data",
    "section": "\n9.1 Creating new variables with mutate",
    "text": "9.1 Creating new variables with mutate\nThe diamonds data has dimensions (x, y, and z) and a mass (carat) of each diamond. It might be interesting to compare a cuboid approximation of the diamond to its mass to get a quantitative measure of how different each diamond is from a box. To start, we’ll create a new variable called box defined by the product of x, y, and z. Then we’ll create the ratio box_ratio as the quotient of its mass (carat) and this volume.\n\ndiamonds |&gt;\n  mutate(box = x*y*z,\n         box_ratio = carat / box) |&gt;\n  head()\n\n# A tibble: 6 × 12\n  carat cut    color clarity depth table price     x     y     z   box box_ratio\n  &lt;dbl&gt; &lt;ord&gt;  &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  0.23 Ideal  E     SI2      61.5    55   326  3.95  3.98  2.43  38.2   0.00602\n2  0.21 Premi… E     SI1      59.8    61   326  3.89  3.84  2.31  34.5   0.00609\n3  0.23 Good   E     VS1      56.9    65   327  4.05  4.07  2.31  38.1   0.00604\n4  0.29 Premi… I     VS2      62.4    58   334  4.2   4.23  2.63  46.7   0.00621\n5  0.31 Good   J     SI2      63.3    58   335  4.34  4.35  2.75  51.9   0.00597\n6  0.24 Very … J     VVS2     62.8    57   336  3.94  3.96  2.48  38.7   0.00620\n\n\nThis function adds two new columns to the data table, computing values for every row in the table. It doesn’t change any of the other columns. You can also modify an existing column, for example to change units from dollars to thousands of dollars or carats to grams, by using the name of an existing column on the left hand side of the equals sign.\nA few things to note:\n\nWe use the name of a column to create or modify on the left hand side of the equals sign/\nWe use an = instead of &lt;- when creating or modifying columns. The arrow is used to assign values to new R objects in your environment (see the Environment tab in R studio); here we are modifying the columns of a data frame so a different notation is appropriate.\nThe transformation shown above does not modify the diamonds object – it creates a new object, which you can then store in a new R variable in your environment. I often perform transformations simply for the purposes of making a table or a plot without ever storing the the result."
  },
  {
    "objectID": "lessons/110-summarizing-data.html#filtering-rows",
    "href": "lessons/110-summarizing-data.html#filtering-rows",
    "title": "9  Summarizing data",
    "section": "\n9.2 Filtering rows",
    "text": "9.2 Filtering rows\nTbe diamonds data has many rows (more than 50,000). You might be interested in just a subset of the rows. We use filter to select rows to keep from a table. (We can specify rows to remove and use the logical not operator ! to reverse the logic and keep all the other rows.)\nFor example, there are only 5 diamonds of more than 4 carats:\n\ndiamonds |&gt; \n  filter(carat &gt; 4)\n\n# A tibble: 5 × 10\n  carat cut     color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  4.01 Premium I     I1       61      61 15223  10.1 10.1   6.17\n2  4.01 Premium J     I1       62.5    62 15223  10.0  9.94  6.24\n3  4.13 Fair    H     I1       64.8    61 17329  10    9.85  6.43\n4  5.01 Fair    J     I1       65.5    59 18018  10.7 10.5   6.98\n5  4.5  Fair    J     I1       65.8    58 18531  10.2 10.2   6.72\n\n\nYou can combine filtering operations by chaining them together with commas, pipes, or using boolean logic (& for and; | for or). The following three expressions all do the same thing.\n\ndiamonds |&gt; filter(color == \"J\") |&gt; \n  filter(cut == \"Premium\") |&gt; filter(carat &gt; 3.0)\n\n# A tibble: 4 × 10\n  carat cut     color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  4.01 Premium J     I1       62.5    62 15223 10.0   9.94  6.24\n2  3.51 Premium J     VS2      62.5    59 18701  9.66  9.63  6.03\n3  3.01 Premium J     SI2      60.7    59 18710  9.35  9.22  5.64\n4  3.01 Premium J     SI2      59.7    58 18710  9.41  9.32  5.59\n\ndiamonds |&gt; filter(color == \"J\", cut == \"Premium\", carat &gt; 3.0)\n\n# A tibble: 4 × 10\n  carat cut     color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  4.01 Premium J     I1       62.5    62 15223 10.0   9.94  6.24\n2  3.51 Premium J     VS2      62.5    59 18701  9.66  9.63  6.03\n3  3.01 Premium J     SI2      60.7    59 18710  9.35  9.22  5.64\n4  3.01 Premium J     SI2      59.7    58 18710  9.41  9.32  5.59\n\ndiamonds |&gt; filter(color == \"J\" & cut == \"Premium\" & carat &gt; 3.0)\n\n# A tibble: 4 × 10\n  carat cut     color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  4.01 Premium J     I1       62.5    62 15223 10.0   9.94  6.24\n2  3.51 Premium J     VS2      62.5    59 18701  9.66  9.63  6.03\n3  3.01 Premium J     SI2      60.7    59 18710  9.35  9.22  5.64\n4  3.01 Premium J     SI2      59.7    58 18710  9.41  9.32  5.59\n\n\nThe second example a convenient abbreviation of the first, but why would we want the third way? Because we can use a logical “or” instead of an and to make different kinds of selections. For example, we might want all the diamonds that meet any of three different tests.\n\ndiamonds |&gt; filter((color == \"J\" | cut == \"Premium\") & carat &gt; 4.0)\n\n# A tibble: 4 × 10\n  carat cut     color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  4.01 Premium I     I1       61      61 15223  10.1 10.1   6.17\n2  4.01 Premium J     I1       62.5    62 15223  10.0  9.94  6.24\n3  5.01 Fair    J     I1       65.5    59 18018  10.7 10.5   6.98\n4  4.5  Fair    J     I1       65.8    58 18531  10.2 10.2   6.72\n\n\nThese logical expressions can be challenging to interpret, so use care and test them carefully to be sure the calculation does what you think it should."
  },
  {
    "objectID": "lessons/110-summarizing-data.html#getting-just-some-rows-and-columns",
    "href": "lessons/110-summarizing-data.html#getting-just-some-rows-and-columns",
    "title": "9  Summarizing data",
    "section": "\n9.3 Getting just some rows and columns",
    "text": "9.3 Getting just some rows and columns\nAll of the tables above showed all 10 columns. Sometimes you only want some of the columns. Use select to extract and reorder columns:\n\ndiamonds |&gt; select(price, cut, color, clarity, carat) |&gt; head()\n\n# A tibble: 6 × 5\n  price cut       color clarity carat\n  &lt;int&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt;\n1   326 Ideal     E     SI2      0.23\n2   326 Premium   E     SI1      0.21\n3   327 Good      E     VS1      0.23\n4   334 Premium   I     VS2      0.29\n5   335 Good      J     SI2      0.31\n6   336 Very Good J     VVS2     0.24\n\n\nYou’ll notice I’ve used head to get just the first 6 rows of the data table. You can also use tail to get the last few rows. And you can modify the number of rows (see the help page for head and tail.)\nA more general method for extracting rows by number is slice.\n\ndiamonds |&gt; slice(c(1, 100, 200, 1000, 5000, 1000))\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.8  Premium   H     SI1      61.5    58  2760  5.97  5.93  3.66\n3  0.72 Premium   F     SI1      61.8    61  2777  5.82  5.71  3.56\n4  1.12 Premium   J     SI2      60.6    59  2898  6.68  6.61  4.03\n5  1.05 Very Good I     SI2      62.3    59  3742  6.42  6.46  4.01\n6  1.12 Premium   J     SI2      60.6    59  2898  6.68  6.61  4.03\n\n\nSometimes with a large dataset I want to look at a random subset of all the rows. Use these variants on slice_ for that task. (Try it yourself: you will get different rows each time.)\n\ndiamonds |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 10\n  carat cut     color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.22 Ideal   G     VVS1     61.5    56 12081  6.86  6.9   4.23\n2  1.5  Good    E     SI1      58.9    61 11179  7.43  7.5   4.4 \n3  1.2  Premium D     VS2      61.6    60  8693  6.78  6.72  4.16\n4  0.54 Ideal   G     VVS1     62.2    56  2080  5.18  5.23  3.24\n5  0.64 Ideal   H     VVS1     61.1    55  2084  5.58  5.61  3.43\n\n\nYou can use arrange to sort the data by any number of columns before slicing or displaying the data.\n\ndiamonds |&gt; arrange(cut, -color, -carat) |&gt; \n  slice_head(n = 10) |&gt; \n  select(carat, cut, color, price)\n\nWarning: There was 1 warning in `arrange()`.\nℹ In argument: `..2 = -color`.\nCaused by warning in `Ops.ordered()`:\n! '-' is not meaningful for ordered factors\n\n\n# A tibble: 10 × 4\n   carat cut   color price\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt;\n 1  5.01 Fair  J     18018\n 2  4.5  Fair  J     18531\n 3  4.13 Fair  H     17329\n 4  3.65 Fair  H     11668\n 5  3.4  Fair  D     15964\n 6  3.11 Fair  J      9823\n 7  3.02 Fair  I     10577\n 8  3.01 Fair  H     10761\n 9  3.01 Fair  I     18242\n10  3.01 Fair  I     18242\n\n\nYou’ll notice that there are only a few levels of each of the categorical variables (cut, color, clarity). Suppose you want to see all combinations that exist? Use select to pick out those rows, then distinct to show only the rows that are different from each other. (There are 276 combinations, so I’m showing only the top 5, after sorting.)\n\ndiamonds |&gt; select(cut, clarity, color) |&gt; \n  distinct() |&gt; arrange(color, clarity, cut) |&gt; \n  head(5)\n\n# A tibble: 5 × 3\n  cut       clarity color\n  &lt;ord&gt;     &lt;ord&gt;   &lt;ord&gt;\n1 Fair      I1      D    \n2 Good      I1      D    \n3 Very Good I1      D    \n4 Premium   I1      D    \n5 Ideal     I1      D"
  },
  {
    "objectID": "lessons/110-summarizing-data.html#summarizing-data-tables",
    "href": "lessons/110-summarizing-data.html#summarizing-data-tables",
    "title": "9  Summarizing data",
    "section": "\n9.4 Summarizing data tables",
    "text": "9.4 Summarizing data tables\nThese filtering and transformation functions are powerful and encourage a really useful way of probing a complext dataset. The summarize (or summarise) provides a way to apply a function to one or more variables across all the rows or colletions of rows. To use this effectively you want to think of functions that operate on a set of numbers and return just a single one, such as mean, median, max, min, sd, or the simplest of all n which counts the number of rows.\n\ndiamonds |&gt; summarise(n_rows = n(), \n                      mean_price = mean(price),\n                      median_price = median(price),\n                      min_carat = min(carat))\n\n# A tibble: 1 × 4\n  n_rows mean_price median_price min_carat\n   &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1  53940      3933.         2401       0.2\n\n\nAll the original data is gone, replaced by these four functions applied to all the data in the corresponding column. The name of the new column appears on the left side of the = and the calculation on the right. This is the same way we wrote the mutate function, but instead of creating a new variable and computing a value for each row, we get one value computed using all the rows.\nRows can be grouped to get summaries for each level of the grouped varible. Let’s group on cut but otherwise repeat this calculation:\n\ndiamonds |&gt; group_by(cut) |&gt;\n  summarise(n_rows = n(), \n            mean_price = mean(price),\n            median_price = median(price),\n            min_carat = min(carat))\n\n# A tibble: 5 × 5\n  cut       n_rows mean_price median_price min_carat\n  &lt;ord&gt;      &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Fair        1610      4359.        3282       0.22\n2 Good        4906      3929.        3050.      0.23\n3 Very Good  12082      3982.        2648       0.2 \n4 Premium    13791      4584.        3185       0.2 \n5 Ideal      21551      3458.        1810       0.2 \n\n\nNow we get 5 rows.\nYou can group on as many variables at once as you like; the result will have one row for each combination of levels from each grouped variable. (You can even group on quantitative varibles, but you usually don’t want to!)\n\ndiamonds |&gt; group_by(cut, clarity, color) |&gt;\n  summarise(n_rows = n(), \n                       mean_price = mean(price),\n                       median_price = median(price),\n                       min_carat = min(carat)) |&gt;\n  head(5)\n\n# A tibble: 5 × 7\n# Groups:   cut, clarity [1]\n  cut   clarity color n_rows mean_price median_price min_carat\n  &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt;  &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Fair  I1      D          4      7383         5538.      0.91\n2 Fair  I1      E          9      2095.        2036       0.7 \n3 Fair  I1      F         35      2544.        1570       0.34\n4 Fair  I1      G         53      3187.        1954       0.5 \n5 Fair  I1      H         52      4213.        3340.      0.7 \n\n\nGrouping doen’t do any calculations, it just makes a notation on a data frame that your grouping should be used for subsequent calculations that pay attention to grouping. After you summarise data, one grouping variable is removed (the last one) but the result is still grouped by the remaining variables. (See the notation: Groups: cut, clarity in the result above.)\nsummarise has an optional argument .groups to say what grouping you want to retain after the calculation is done. drop_last is the default, described above. Probably the next most frequently used choice is drop which ungroups all the rows. To see the diference, I’ll add another step, count(), which is equivalent to summarise(n=n()) and counts rows.\nUsing drop_last I get the number of rows for each level of cut and clarity (usually 7 for the levels of color: D, E, F, G, H, I, J) for all 40 combinations of cut and clarity (only a few shown here.)\n\ndiamonds |&gt; group_by(cut, clarity, color) |&gt;\n  summarise(n_rows = n(), \n                       mean_price = mean(price),\n                       median_price = median(price),\n                       min_carat = min(carat),\n             .groups=\"drop_last\") |&gt;\n  count() |&gt;\n  head()\n\n# A tibble: 6 × 3\n# Groups:   cut, clarity [6]\n  cut   clarity     n\n  &lt;ord&gt; &lt;ord&gt;   &lt;int&gt;\n1 Fair  I1          7\n2 Fair  SI2         7\n3 Fair  SI1         7\n4 Fair  VS2         7\n5 Fair  VS1         7\n6 Fair  VVS2        7\n\n\nChange the options to .groups = \"drop\" and I get a very different result – just the number of combinations of cut, clarity, and color (276 of the maximum 5 x 8 x 7 = 280 which are possible):\n\ndiamonds |&gt; group_by(cut, clarity, color) |&gt;\n  summarise(n_rows = n(), \n                       mean_price = mean(price),\n                       median_price = median(price),\n                       min_carat = min(carat),\n            .groups=\"drop\") |&gt;\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   276\n\n\nYou can also use ungroup() following a summarise to discard all grouping if you don’t want to use the .groups argument."
  },
  {
    "objectID": "lessons/110-summarizing-data.html#example",
    "href": "lessons/110-summarizing-data.html#example",
    "title": "9  Summarizing data",
    "section": "\n9.5 Example",
    "text": "9.5 Example\nThese methods can be combined together to produce elaborate queries. For example, let’s use mutate to compute a price per carat ratio, then find the median of this ratio over all diamonds grouped by cut, color, and clarity. Also report the sample size used for each median. Finally we will extract the top 2 ratios for each cut type (using a grouped slice) and arrange the resulting table from largest to smallest ratio.\n\ndiamonds |&gt;\n  mutate(price_per_carat = price / carat) |&gt;\n  group_by(color, clarity, cut) |&gt;\n  summarise(median_price_per_carat = median(price_per_carat), \n            n = n(),\n            .groups = \"drop\") |&gt;\n  arrange(-median_price_per_carat) |&gt;\n  group_by(cut) |&gt;\n  slice_head(n=2) |&gt;\n  arrange(-median_price_per_carat)\n\n# A tibble: 10 × 5\n# Groups:   cut [5]\n   color clarity cut       median_price_per_carat     n\n   &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt;                      &lt;dbl&gt; &lt;int&gt;\n 1 D     IF      Good                      14932.     9\n 2 D     IF      Premium                   11057.    10\n 3 D     IF      Very Good                 10202.    23\n 4 D     IF      Ideal                      7162.    28\n 5 J     VVS1    Premium                    5336.    24\n 6 J     VVS2    Very Good                  5227.    29\n 7 H     IF      Good                       5100.     4\n 8 E     VVS1    Fair                       4921.     3\n 9 G     VS2     Fair                       4838     45\n10 H     SI1     Ideal                      4469.   763\n\n\nWhen I did this calculation the first time, I forgot the first arrange. Why is it needed before the grouped slice_head? (Remove it and see how the result changes. Try removing the slice_head to see the full results.) Why is the second arrange needed?\nThe top 4 price per carat categories are all the best colour (D) and clarity (IF), but cut seems to be less important in determining this ratio. One of the top 10 categories has a lot of diamonds in it (763), while the other groups have very few diamonds relative to the size of the whole dataset (more than 50,000)."
  },
  {
    "objectID": "lessons/110-summarizing-data.html#what-does-this-have-to-do-with-data-visualization",
    "href": "lessons/110-summarizing-data.html#what-does-this-have-to-do-with-data-visualization",
    "title": "9  Summarizing data",
    "section": "\n9.6 What does this have to do with data visualization?",
    "text": "9.6 What does this have to do with data visualization?\nThese tools are very useful for exploring data; helping you understand the structure of your data. This will help you make better visualizations. In particular, the humble n function which counts rows should be used frequently when you summarize with mean or median to let you know if you have very few observations or too many for a good plot.\nSummarizing data, for example by computing means and standard errors can be done as part of the visualization process using stat_summary, but it can be easier to know exactly what you are calculating if you compute the summary yourself using the methods in this lesson and then plot the results.\nTables can be a useful way to visualize data and grouped summaries are often the easiest way to make a table to describe your data."
  },
  {
    "objectID": "lessons/110-summarizing-data.html#further-reading",
    "href": "lessons/110-summarizing-data.html#further-reading",
    "title": "9  Summarizing data",
    "section": "\n9.7 Further reading",
    "text": "9.7 Further reading\n\nR4DS (Wickham, Çetinkaya-Rundel, and Grolemund 2023). Chapter 5 on Data transformations\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. \"O’Reilly Media\". https://r4ds.had.co.nz/."
  },
  {
    "objectID": "lessons/115-facets.html#facets-indexed-by-one-variable",
    "href": "lessons/115-facets.html#facets-indexed-by-one-variable",
    "title": "10  Facetted graphs",
    "section": "\n10.1 Facets indexed by one variable",
    "text": "10.1 Facets indexed by one variable\nIf you want to display many levels of a categorical variable across many facets, the facet_wrap function is available. We’ll do this here for just one colour using filter to select it. The facet_wrap function requires you to give a “formula” for how the facets are constructed. Here we write ~ clarity meaning that the levels of the categorical variable clarity is used to facets. One facet is drawn for each value of the variable and only points with that value of clarity are shown on the corresponding facet.\n\ndiamonds |&gt; \n  filter(color %in% c(\"F\")) |&gt;\n  ggplot(aes(x=carat, y=price, color=cut)) + \n  geom_point(size = 0.4) +\n  facet_wrap( ~ clarity) \n\n\n\n\nNotice that the x and y scales are the same on each facet. A single colour scale is used for cut and this is the same for each facet as well. We can see that there is a similar non-linear relationship between price and carat on each panel. The price is higher at any particular size (carat) in the panels near the bottom compared to the panels near the top. Cut makes a compartively small difference to the price."
  },
  {
    "objectID": "lessons/115-facets.html#facets-indexed-by-two-variables",
    "href": "lessons/115-facets.html#facets-indexed-by-two-variables",
    "title": "10  Facetted graphs",
    "section": "\n10.2 Facets indexed by two variables",
    "text": "10.2 Facets indexed by two variables\nIf you want to look at the effect of two categorical variables across different facets, you can arrange them in a grid. Again we use a formula, but now we put the variables for the “y” axis of facets on the left side of ~ and the variables determining the x axis of the facets on the right.\n\np1 + facet_grid(color ~ clarity)\n\n\n\n\nThe plots in those facets are really small. Let’s pick out just a few colours and clarity values to make a plot that is easier to read.\n\ndiamonds |&gt; \n  filter(color %in% c(\"D\", \"F\", \"H\", \"J\"), \n         clarity %in% c(\"SI1\", \"VS1\", \"IF\")) |&gt;\n  ggplot(aes(x=carat, y=price, color=cut)) + \n  geom_point(size=0.5) +\n  facet_grid(color ~ clarity) + \n  scale_colour_viridis_d(begin=0, end =0.8)"
  },
  {
    "objectID": "lessons/115-facets.html#compositing",
    "href": "lessons/115-facets.html#compositing",
    "title": "10  Facetted graphs",
    "section": "\n10.3 Compositing",
    "text": "10.3 Compositing\nIf you want to combine graphs together into a grid layout, but the graphs don’t share axes or even data, then it may be easier to draw the graphs separately and then combine them together into a layout with the patchwork package. This package is both simple to use and quite powerful, enabling complex layouts, labelling subplots with codes to aid identification, and combining guides for colours and shapes across panels. The documentation is excellent.\nHere is an example showing two plots with very different views of the gapminder data. It’s usually best to create the plots one at a time, assign them to a name in your R environment using &lt;- and a name for each plot, then put them together using + from the patchwork package.\n\nlibrary(patchwork)\np1 &lt;- gapminder |&gt; \n  filter(year == max(year)) |&gt; count(continent) |&gt; \n  ggplot(aes(x=continent, y = n, fill=continent)) +\n  geom_col(show.legend = FALSE) +\n  labs(y = \"Number of countries\") \np2 &lt;- gapminder |&gt;\n  ggplot(aes(x= year, y = lifeExp, color = continent)) + \n  geom_jitter(size = 0.5)\np1 + p2 \n\n\n\n\nYou can make complex layouts, collect legends from multiple panels together, and label panels with patchwork. Read the documentation if you want to learn more."
  },
  {
    "objectID": "lessons/115-facets.html#further-reading",
    "href": "lessons/115-facets.html#further-reading",
    "title": "10  Facetted graphs",
    "section": "\n10.4 Further reading",
    "text": "10.4 Further reading\n\nThe ggplot book has a longer tutorial on using facets.\nThe R cookbook has more examples of facetted plots, including instructions for chaging the way the facets are labelled.\nThe patchwork documentation can help you with many more tasks.\nThe cowplot package package is designed to help with many plotting tasks, including combining many figures into one using plot_grid. “cow” stands for Claus O Wilke, the author of one of our textbooks."
  },
  {
    "objectID": "lessons/111-reading-data.html#text-files",
    "href": "lessons/111-reading-data.html#text-files",
    "title": "11  Reading data",
    "section": "\n11.1 Text files",
    "text": "11.1 Text files\nIt’s very common to distribute data from one computer user to another in a “text file”. This means a file you can open with a text editor and read without further interpretation. This format takes more storage space than a binary file, because for example, a number written out like 14956.42 takes 8 characters, but might take considerably less in a “binary” format. Text formats are favoured for all but very largest files because they can be checked by humans and do not need a lot of documentation.\nText files are commonly identified as either being written as a set of comma separated values (csv) or tab separated values (tsv). Other delimeters are possible as well. A notable case arises from the way numbers are written in different cultures: some use a . for the decimal marker, others use a ,. An approximation of \\(\\pi\\) might be written as 3.14159 in Canada and 3,14159 in Spain. This causes problems for csv files and gave rise to the csv2 file which uses semicolons (;).\nThe biggest problem with text files is that, as a rule, they contain no computer-readable information about how the text in each column should be interpreted: as a number, text, a date, time, a filename, or something else. This is the source of a lot of problems! The only good solutions are to use a database (which can have other challenges) or a binary format (which has problems too). Someday this will get fixed, but not before you are done this course!\nHere are some examples using the functions in the readr and readxl packages. The three functions below are all specialize variants of read_delim.\n\nexample1a &lt;- read_csv(\"static/test-data.csv\")\n\nRows: 3 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Name, Species\ndbl  (1): Mass_kg\nlgl  (1): Friendly\ndate (1): Birthdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexample1b &lt;- read_csv2(\"static/test-data2.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\nRows: 3 Columns: 5── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (2): Name, Species\ndbl  (1): Mass_kg\nlgl  (1): Friendly\ndate (1): Birthdate\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexample2 &lt;- read_tsv(\"static/test-data.txt\") # A tab character is written as \\t\n\nRows: 3 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Name, Species\ndbl  (1): Mass_kg\nlgl  (1): Friendly\ndate (1): Birthdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice that R reports the name of each column read and the data type. Here we have examples with text (characters), numbers (double), dates (date), and true/false (boolean or logical) data.\nThese are very small files all with the same data which you can find here: csv, csv2, tsv. Tab separated files come with a variety of “extensions”: sometimestsv, tab, or txt.\nI created these files using a spreadsheet program, and you may often find yourself editing data using a program like Excel. Here is how you can read that file directly.\n\nexample3 &lt;- read_excel(\"static/test-data.xlsx\")\n\nYou should look at each of these files in a text editor (such as textedit on a Macintosh, or notepad on Windows) or spreadsheet program (Excel, LibreOffice). Edit the files by adding a line of your own data and then read the data back into R. This is the best way to practice making data and reading it into R.\nThere are lots of other functions used for reading files in R, including the similarly named read.csv and read.delim which are similar but with many important differences including how data are interpreted and encoded and how columns are named.\nFiles can be read from a folder on your computer (see [staying-organized] for advice) or directly from a website simply by putting the web address (URL) in the place of the filename. (read_excel may not work with data on websites.)"
  },
  {
    "objectID": "lessons/111-reading-data.html#checking-your-data",
    "href": "lessons/111-reading-data.html#checking-your-data",
    "title": "11  Reading data",
    "section": "\n11.2 Checking your data",
    "text": "11.2 Checking your data\nWhen you go to read a data file, there are a large number of problems that can arise. Your first task is always to be sure the data you loaded into R are what you think they should be. Check as many features as you can: the number of columns, the names of the columns, the number of rows, the type of data in each column, the extence and amount of missing data, the levels of categorical variables.\nOur example is so small we can just look at the whole table, but we’ll practice using functions you should use on any data you read.\n\nskim(example1a)\n\n\n\n\n\nName\nexample1a\n\n\nNumber of rows\n3\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nlogical\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nName\n0\n1.00\n5\n11\n0\n3\n0\n\n\nSpecies\n1\n0.67\n3\n3\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nBirthdate\n0\n1\n1982-07-11\n2008-09-01\n2001-01-01\n3\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\nFriendly\n0\n1\n0.67\nTRU: 2, FAL: 1\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nMass_kg\n0\n1\n8.6\n5.34\n4.1\n5.65\n7.2\n10.85\n14.5\n▇▇▁▁▇"
  },
  {
    "objectID": "lessons/111-reading-data.html#problems-that-arise",
    "href": "lessons/111-reading-data.html#problems-that-arise",
    "title": "11  Reading data",
    "section": "\n11.3 Problems that arise",
    "text": "11.3 Problems that arise\nHere are a few problems that arise and how you can try to fix them.\n\n11.3.1 Missing header row\nSometimes a file will come to you with no header row – there is just data in the file with no indication of what to call the each variable. You can tell read_delim to add its own headers (col_names=FALSE) or you can provide your own list of names (col_names=c(\"Name\", \"Birthdate\", \"Mass_kg\", \"Species\", \"Friendly\"))).\n\n11.3.2 Extra header rows\nSometimes there are lots of extra data in the file before the header row and the data. These are typically metadata – who collected the data, references to help you interpret the data, etc. This is valuable information, but is best stored in a separate file so that you don’t complicate the process of reading the file into R. Any number of lines can be skipped using skip = and a number.\nAnother version of this problem arises when a file has a second header row below the first row with variable names. This row might be used to describe the units of each variable. What’s the best way to skip this row? There are two okay options: skip all the headers and add your own variable names by hand or create a copy of the file and delete the extra row. The second method creates an extra steps that must to be repeated when the file is updated; the first will need to be repeated if the number or order of columns changes.\n\n11.3.3 Strange names\nSometimes R will have difficulty reading the text in the header row. Unusual symbols (µ, Å, commas in a variable name, spaces in a variable name) can all cause trouble. read_delim will clean up some of these. The clean_names function in the janitor package is useful for simplifying names.\nIt’s best to stick to a simple set of conventions when naming variables. For example, all lower case, underscores instead of spaces, units at the end, and other naming conventions to make names easier to remember, easier to type, and less prone to errors or strange problems with reading and writing.\n\n11.3.4 Bad data formatting\nThe most common data formatting problem when a dataset has been created by a human typing in data to a spreadsheet is bad data formatting. These problems can be very challenging to find.\nSuppose one column (e.g., Mass_kg) contains numbers. Down in row 500 or so, someone didn’t have data for mass and wrote “-” for missing data. Or perhaps put commas in the number for their pet elephant: 1,150.4. Or entered a whimsical “three” instead of the number 3. All of these will cause R to think the column is actually text and not numbers at all. This makes all the other numbers into text and makes any other quantitative data analysis impossible. A quick fix is to use the hablar pacakge to convert the variable back to a number: convert(example1, num(Mass_kg)). The bad data are turned into missing data (NA) and a warning message will be displayed.\nIf the error occurs after row 1000 (see below) the problem will manifest differently!\n\n11.3.5 Dates and times\nDates and times are notoriously difficult.\nIdeally your dates are written in a standard format, such as ISO 8601 and that time zone information is included or irrelevant for times. If so, then read_delim should be able to recognize the format.\nThere is a good chance that the date in particular is written in some strange way, for example with a 2 digit year, or a m/d/y or d/m/y format. The package lubridate can help with converting text into dates and times from these and many other formats.\nIf you have time of day information throughout a year, you will need to know if daylight savings time was used in your jurisdiction. This is the complication that leads many people to use UTC for time.\nIf you have data spanning February-March for one or more years, you may need to decide what to do with the leap day. That’s a data analysis problem, not a reading day problem. The lubridate functions yday and decimal_date are very helpful.\n\n11.3.6 Missing data\nThe notion of data that are missing, rather than just 0 or some other special value is a very important concept in data analysis. Missing data are used when an observation of a variable was not made.\nR uses NA to represent missing data, for text, dates, factors, and numbers. But if “NA” is written in a text file it will be interpreted as a piece of text, not missing data. Generally leaving a blank in a data file is the way to indicate data are missing. If you know there are missing data in a file, confirm that they were identified correctly by R. If you are not sure, check for the presence of NAs in your data in R and then go back to the data file to see if the interpretation is correct.\nHere are a few ways to check for NAs in a file using a function from the naniar package:\n\nn_case_miss(example1a)\n\n[1] 1\n\nn_miss(example1a)\n\n[1] 1\n\ngg_miss_var(example1a)\n\n\n\n\nIn some very old files (or data from old scientists) you may find missing data encoded as “impossible” data, for example a negative value for mass. Traditionally the value -999 or similar are used to help them stand out when looking at data by eye. Keep an eye out for these sorts of problems when validating your data. You can use na_if to get rid of values like this."
  },
  {
    "objectID": "lessons/111-reading-data.html#bad-data-types",
    "href": "lessons/111-reading-data.html#bad-data-types",
    "title": "11  Reading data",
    "section": "\n11.4 Bad data types",
    "text": "11.4 Bad data types\nread_delim looks at the first 1000 rows (or whole file if it is smaller than that) to ‘guess’ the type of each variable. If all the values can be converted to numbers with out error, then R interprets the data as numeric; similar logic is used for dates and logical values. If all the first 1000 rows for a column are missing, then a problem arises – it’s very hard to guess correctly. If a typo in a row has a number look like text (for example 9OO instead of 900 – using a letter oh instead of the number zero), there will be a problem.\nYou can use the col_types argument to read_delim to specify the type of each column or set guess_max to a number larger than 1000 (such as Inf to check all rows.)"
  },
  {
    "objectID": "lessons/111-reading-data.html#categorical-variables-factors-or-text",
    "href": "lessons/111-reading-data.html#categorical-variables-factors-or-text",
    "title": "11  Reading data",
    "section": "\n11.5 Categorical variables: factors or text",
    "text": "11.5 Categorical variables: factors or text\nBefore version 4, R would usually convert variables with just text into factor (categorical) variables. Internally a factor variable is an integer with an accompanying text label for each level of the factor variable. This was natural for some applications where the different levels of a factor would be used for defining a statistical model, but completely inappropriate for others. As R was used for more varied applications, the pressure to change this grew immense. Now text is usually treated as text. It’s still good to use str or skim to check for factor variable and be aware they can cause problems.\nFactors are still really useful, both for some statistical models, and for data visualizations. We will use factor variables to define an order other than alphabetical for text labels on a graph."
  },
  {
    "objectID": "lessons/111-reading-data.html#line-endings",
    "href": "lessons/111-reading-data.html#line-endings",
    "title": "11  Reading data",
    "section": "\n11.6 Line endings",
    "text": "11.6 Line endings\nIn a text file, a line of text is ended by a special character. The names come from decades old references to typewriters and printers: carriage return and newline. Why do these old terms matter to us? Because the three major types of computer operating systems (Windows, Linux, Macintosh) use different conventions for ending lines! When you share files between computers, these differences can matter. Fortunately these problems are usually detected by software and corrected for you automatically, but that just makes them all the more confusing when they occur.\nI know how to solve these problems, but I don’t have any good, simple advice for a beginner. Suggestions welcome!"
  },
  {
    "objectID": "lessons/111-reading-data.html#text-encoding",
    "href": "lessons/111-reading-data.html#text-encoding",
    "title": "11  Reading data",
    "section": "\n11.7 Text encoding",
    "text": "11.7 Text encoding\nAnother bit of computer lore. All data on a computer is, naturally, stored as bits (0s and 1s) grouped into clusters called bytes. These quantities can be interpreted as text, but there needs to be a conversion rule. Early on a system called ASCII was developed, but there were only a small number of letters, numbers, and punctuation symbols possible. Expanding computer text to include most of the world’s languages and mathematical symbols required far more ‘code points’ than originally planned. This lead to an explosion of confusion, generally solved using unicode. Practical considerations has led to many encodings of unicode such as UTF-8. Sometimes R will not be able to interpret the text in a file reliably because the encoding is not explicitly identified in the file. Then you have to help R out.\nAgain, I lack good advice for beginners here."
  },
  {
    "objectID": "lessons/111-reading-data.html#editing-data-tables",
    "href": "lessons/111-reading-data.html#editing-data-tables",
    "title": "11  Reading data",
    "section": "\n11.8 Editing data tables",
    "text": "11.8 Editing data tables\nA spreadsheet package (Excel, Numbers, LibreOffice Calc) is often used to enter and edit data. Many of these programs will try to interpret and modify your data as you enter it, causing havoc. Think of this as autocorrect for data. Excel in particular is notorious for interpreting numbers with dashes or slashes as dates, rewriting your data. For this reason some people prefer other ways to enter data. Two options are\n\nstand alone csv editors, such as TableTool for Macintosh, or CSVed or many others for Windows, and\n\nDataEditR package for R"
  },
  {
    "objectID": "lessons/111-reading-data.html#further-reading",
    "href": "lessons/111-reading-data.html#further-reading",
    "title": "11  Reading data",
    "section": "\n11.9 Further reading",
    "text": "11.9 Further reading\n\n\nR data import/export manual\nTidy data\nA proposal for systematic variable naming conventions which helps with data validation as well. See the pointblank and convo packages.\nProblems caused by using Excel for entering and managing data\n\nArguments for fiscal austerity following the 2008-09 economic crisis may have been partly supported by spreadsheet errors."
  },
  {
    "objectID": "lessons/112-reshaping-data.html#pivot-wider",
    "href": "lessons/112-reshaping-data.html#pivot-wider",
    "title": "12  Reshaping data",
    "section": "\n12.1 Pivot wider",
    "text": "12.1 Pivot wider\nWe’ll start by selecting three years (1987, 1997, 2007) and computing the median life expectancy for each continent in each year. (The median for the continent may not be representative of all countries in the continent, but we’ll worry about that some other time.)\n\nt1 &lt;- gapminder |&gt; \n  filter(year %in% c(1987, 1997, 2007)) |&gt;\n  group_by(continent, year) |&gt;\n  summarise(median_life_expectancy = median(lifeExp), .groups = \"drop\")\nt1\n\n# A tibble: 15 × 3\n   continent  year median_life_expectancy\n   &lt;fct&gt;     &lt;int&gt;                  &lt;dbl&gt;\n 1 Africa     1987                   51.6\n 2 Africa     1997                   52.8\n 3 Africa     2007                   52.9\n 4 Americas   1987                   69.5\n 5 Americas   1997                   72.1\n 6 Americas   2007                   72.9\n 7 Asia       1987                   66.3\n 8 Asia       1997                   70.3\n 9 Asia       2007                   72.4\n10 Europe     1987                   74.8\n11 Europe     1997                   76.1\n12 Europe     2007                   78.6\n13 Oceania    1987                   75.3\n14 Oceania    1997                   78.2\n15 Oceania    2007                   80.7\n\n\nNow we can see the data and it’s not too large, the reshaping problem is clearer. I’d like to create a table with one row for each continent, one column for each year and the numbers in the grid should be the median life expectancy. This reshaping is called a pivot, specifically a pivot to make a wider table, and we simply need to specify the current column to use as the new names (year) and the current column to use as the values for the new grid (median_life_expectancy).\n\nt1 |&gt; pivot_wider(names_from = year, values_from = median_life_expectancy)\n\n# A tibble: 5 × 4\n  continent `1987` `1997` `2007`\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Africa      51.6   52.8   52.9\n2 Americas    69.5   72.1   72.9\n3 Asia        66.3   70.3   72.4\n4 Europe      74.8   76.1   78.6\n5 Oceania     75.3   78.2   80.7\n\n\nI’ll give you an advanced peek at table formatting skills from the next lesson to make this look a bit better:\n\nt2 &lt;- t1 |&gt; pivot_wider(names_from = year, values_from = median_life_expectancy)\nt2 |&gt; rename(Continent = continent) |&gt;\n  arrange(-`2007`) |&gt;\n  kable(digits = 1) |&gt;\n  column_spec(1, bold=TRUE) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nContinent\n1987\n1997\n2007\n\n\n\nOceania\n75.3\n78.2\n80.7\n\n\nEurope\n74.8\n76.1\n78.6\n\n\nAmericas\n69.5\n72.1\n72.9\n\n\nAsia\n66.3\n70.3\n72.4\n\n\nAfrica\n51.6\n52.8\n52.9"
  },
  {
    "objectID": "lessons/112-reshaping-data.html#pivot-longer",
    "href": "lessons/112-reshaping-data.html#pivot-longer",
    "title": "12  Reshaping data",
    "section": "\n12.2 Pivot longer",
    "text": "12.2 Pivot longer\nSometimes data a provided in a “wide” format, like the summary table above. This is often very convient for data entry and visual inspection. Suppose you wanted to use the years in the table shown above as a aesthetic in a plot, or a grouping variable in a summarise operation. You can’t! So you might want to pivot the table to make it longer.\npivot_longer undoes the pivot_wider operation. You need to give the set of variables to be stacked and a name for the new variable to be filled with those column headings. Here’s how to perform the inverse of the pivot_wider shown above. You can describe a sequence of variables by giving the first one, a colon (:), and the last one. Since our variable names are just numbers (which are not standard column names in data tables), we need to put backticks around them. (The plain expression 1987:2007 would be interpreted quite differently by R. Try it in the console. )\n\nt3 &lt;- t2 |&gt; pivot_longer(cols = `1987`:`2007`, names_to = \"year\", values_to = \"median_life_expectancy\")\nt3\n\n# A tibble: 15 × 3\n   continent year  median_life_expectancy\n   &lt;fct&gt;     &lt;chr&gt;                  &lt;dbl&gt;\n 1 Africa    1987                    51.6\n 2 Africa    1997                    52.8\n 3 Africa    2007                    52.9\n 4 Americas  1987                    69.5\n 5 Americas  1997                    72.1\n 6 Americas  2007                    72.9\n 7 Asia      1987                    66.3\n 8 Asia      1997                    70.3\n 9 Asia      2007                    72.4\n10 Europe    1987                    74.8\n11 Europe    1997                    76.1\n12 Europe    2007                    78.6\n13 Oceania   1987                    75.3\n14 Oceania   1997                    78.2\n15 Oceania   2007                    80.7\n\n\nThere is an important difference between the original table t1 and this recovered table t3: the year variable in t1 was an integer (numeric) but the year variable in t3 is character (text). This will matter if you want to calculate with the new year variable or put it on a quantitative scale. The hablar package makes it really easy to convert variables from one type to another:\n\nt3 |&gt; convert(int(year))\n\n# A tibble: 15 × 3\n   continent  year median_life_expectancy\n   &lt;fct&gt;     &lt;int&gt;                  &lt;dbl&gt;\n 1 Africa     1987                   51.6\n 2 Africa     1997                   52.8\n 3 Africa     2007                   52.9\n 4 Americas   1987                   69.5\n 5 Americas   1997                   72.1\n 6 Americas   2007                   72.9\n 7 Asia       1987                   66.3\n 8 Asia       1997                   70.3\n 9 Asia       2007                   72.4\n10 Europe     1987                   74.8\n11 Europe     1997                   76.1\n12 Europe     2007                   78.6\n13 Oceania    1987                   75.3\n14 Oceania    1997                   78.2\n15 Oceania    2007                   80.7\n\n\nThe dataset who in the tidyr package is counts of tuberculosis cases by country and year. It is in wide format with many columns (new…) describing diagnosis method, sex, and age category. There are also a lot of missing data when the data are shown in this wide format.\nLet’s pivot this data to make it long.\n\nwho_long &lt;- who |&gt; pivot_longer(new_sp_m014:newrel_f65, names_to = \"category\", values_to = \"counts\")\nhead(who_long)\n\n# A tibble: 6 × 6\n  country     iso2  iso3   year category     counts\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Afghanistan AF    AFG    1980 new_sp_m014      NA\n2 Afghanistan AF    AFG    1980 new_sp_m1524     NA\n3 Afghanistan AF    AFG    1980 new_sp_m2534     NA\n4 Afghanistan AF    AFG    1980 new_sp_m3544     NA\n5 Afghanistan AF    AFG    1980 new_sp_m4554     NA\n6 Afghanistan AF    AFG    1980 new_sp_m5564     NA\n\n\nthe original data had 7240 observations and 60 columns (4 that we’ve kept and 56 that we have pivoted). As a result our new table who_long has 6 columns (4+2) and 56 x 7240 = 405,440 rows. Let’s see how many of the count data are missing.\n\nwho_long |&gt; summarize(count_NA = sum(is.na(counts)))\n\n# A tibble: 1 × 1\n  count_NA\n     &lt;int&gt;\n1   329394\n\n\nA lot of them! In fact 81% of the data in the original matrix are NA. So let’s discard them using na.omit or filter(!is.na(counts)). You can also use values_drop_na = TRUE in the pivot_longer function call. That will make a much smaller table. If you look at the smaller table, you’ll see there are some counts equal to 0. So NA did not simply mean 0. (You should never use NA to mean 0; it should mean missing. But some people do.)\n\nwho_long &lt;- who_long |&gt; filter(!is.na(counts))"
  },
  {
    "objectID": "lessons/112-reshaping-data.html#separate-and-unite",
    "href": "lessons/112-reshaping-data.html#separate-and-unite",
    "title": "12  Reshaping data",
    "section": "\n12.3 Separate and Unite",
    "text": "12.3 Separate and Unite\nThe category variable combines three pieces of information together in one label. How can we decode the category into three columns: diagnosis, sex, and age group? The separate function is made for this. The easiest way to use separate is if the variable you are separating is consistently structured with the same character between each column, such as new_rel_f_2534. That’s not the case here: some values start with new_ and some are missing the underscore after the new. Also there is no underscore between sex and age group. So we will do a bit of pre-processing using the stringr package before we use separate.\nFirst, I’ll remove “new” or “new_” (described concisely by a “regular expression” using new_*). Then I’ll change _f to _f_ and _m to _m_. When you first start learning to do these kinds of manipulations, you should check through a lot of the cases to be sure all the transformations worked the way you expect. There are a lot os str_ functions in the stringr package to help with these kinds of manipulations.\n\nwho_long |&gt; mutate(category = str_remove(category, \"new_*\"),\n                    category = str_replace(category, \"_f\", \"_f_\"),\n                    category = str_replace(category, \"_m\", \"_m_\")\n                    ) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  country     iso2  iso3   year category  counts\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan AF    AFG    1997 sp_m_014       0\n2 Afghanistan AF    AFG    1997 sp_m_1524     10\n3 Afghanistan AF    AFG    1997 sp_m_2534      6\n4 Afghanistan AF    AFG    1997 sp_m_3544      3\n5 Afghanistan AF    AFG    1997 sp_m_4554      5\n6 Afghanistan AF    AFG    1997 sp_m_5564      2\n\n\nNow, we will use separate to convert category into three columns using the underscore as a separator.\n\nwho_new &lt;- who_long |&gt; mutate(category = str_remove(category, \"new_*\"),\n                    category = str_replace(category, \"_f\", \"_f_\"),\n                    category = str_replace(category, \"_m\", \"_m_\")\n                    ) |&gt; \n  separate(col = category, into = c(\"diagnosis\", \"sex\", \"age_group\"), sep=\"_\")\nwho_new |&gt; head()\n\n# A tibble: 6 × 8\n  country     iso2  iso3   year diagnosis sex   age_group counts\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan AF    AFG    1997 sp        m     014            0\n2 Afghanistan AF    AFG    1997 sp        m     1524          10\n3 Afghanistan AF    AFG    1997 sp        m     2534           6\n4 Afghanistan AF    AFG    1997 sp        m     3544           3\n5 Afghanistan AF    AFG    1997 sp        m     4554           5\n6 Afghanistan AF    AFG    1997 sp        m     5564           2\n\n\nNow we can use the dplyr methods group_by and summarize to count cases by sex, age group, diagnosis, country, and year. Or we can use these new variables in data visualizations for facets or colours.\nSometimes you will want to do the reverse operation: combining two or more columns together. For example, if I have some biological data with the variables genus and species, I might want to combine the two, since a species is usually described by both together (Homo is our genus, and sapiens is our species name). That’s a job for unite."
  },
  {
    "objectID": "lessons/112-reshaping-data.html#suggested-reading",
    "href": "lessons/112-reshaping-data.html#suggested-reading",
    "title": "12  Reshaping data",
    "section": "\n12.4 Suggested reading",
    "text": "12.4 Suggested reading\n\nR4DS Section 12.3 Pivoting\n\nR4DS Sextion 12.4 Separate and Unite\n\nVignette on kableExtra for formatting tables. See the next lesson for a lot more on this topic.\nVignette on stringr for manipulating text variables."
  },
  {
    "objectID": "lessons/113-data-tables.html#header-rows",
    "href": "lessons/113-data-tables.html#header-rows",
    "title": "13  Format tables",
    "section": "\n13.1 Header rows",
    "text": "13.1 Header rows\nOften you will want to rename or reformat the column headings. There are two ways to do this: using rename to change the actual column names before formatting the table, or using column formatting to just affect the table appearance. You can rename just one column using rename, but with the second version below (col.names) you need to provide all the column names in the correct order.\n\nt1 |&gt; rename(`Body mass (g)` = body_mass_g) |&gt; kable()\n\n\n\nspecies\nisland\ncount\nBody mass (g)\n\n\n\nAdelie\nBiscoe\n44\n3709.659\n\n\nAdelie\nDream\n56\n3688.393\n\n\nAdelie\nTorgersen\n51\n3706.373\n\n\nChinstrap\nDream\n68\n3733.088\n\n\nGentoo\nBiscoe\n123\n5076.016\n\n\n\n\n\n\nt1 |&gt; kable(col.names = c(\"Species\", \"Island\", \"Count\",  \"Body mass (g)\"))\n\n\n\nSpecies\nIsland\nCount\nBody mass (g)\n\n\n\nAdelie\nBiscoe\n44\n3709.659\n\n\nAdelie\nDream\n56\n3688.393\n\n\nAdelie\nTorgersen\n51\n3706.373\n\n\nChinstrap\nDream\n68\n3733.088\n\n\nGentoo\nBiscoe\n123\n5076.016"
  },
  {
    "objectID": "lessons/113-data-tables.html#column-alignment",
    "href": "lessons/113-data-tables.html#column-alignment",
    "title": "13  Format tables",
    "section": "\n13.2 Column alignment",
    "text": "13.2 Column alignment\nCommonly numbers are right justified and text is left justified. That’s what’s done automatically. You can specify each column as left, centre, or right justified using the letters l, c, or r for each column. Here we’ll center the justify the numbers to demonstrate.\n\nt1 |&gt; kable(align = \"llcc\")\n\n\n\nspecies\nisland\ncount\nbody_mass_g\n\n\n\nAdelie\nBiscoe\n44\n3709.659\n\n\nAdelie\nDream\n56\n3688.393\n\n\nAdelie\nTorgersen\n51\n3706.373\n\n\nChinstrap\nDream\n68\n3733.088\n\n\nGentoo\nBiscoe\n123\n5076.016"
  },
  {
    "objectID": "lessons/113-data-tables.html#formatting-numbers",
    "href": "lessons/113-data-tables.html#formatting-numbers",
    "title": "13  Format tables",
    "section": "\n13.3 Formatting numbers",
    "text": "13.3 Formatting numbers\nAny number that comes from a calculation (such as a mean) will have a lot of decimal places displayed unless you change this. You can control the number of decimal places to show using rounding. (Use a negative number of digits to round to the left of the decimal point, for example digits=-1 to round to the tens place.) Give either one number to use for all columns, or provide a vector to control the number of digits separately for each column.\nYou can add a comma (or space for SI or . for European styles) to separate the thousands or millions using format.args = list(big.mark = \",\"). See the help for format for more options.\n\nt1 |&gt; kable(digits = 1, format.args = list(big.mark = \",\"))\n\n\n\nspecies\nisland\ncount\nbody_mass_g\n\n\n\nAdelie\nBiscoe\n44\n3,709.7\n\n\nAdelie\nDream\n56\n3,688.4\n\n\nAdelie\nTorgersen\n51\n3,706.4\n\n\nChinstrap\nDream\n68\n3,733.1\n\n\nGentoo\nBiscoe\n123\n5,076.0"
  },
  {
    "objectID": "lessons/113-data-tables.html#color-highlights-and-other-styles",
    "href": "lessons/113-data-tables.html#color-highlights-and-other-styles",
    "title": "13  Format tables",
    "section": "\n13.4 Color, highlights, and other styles",
    "text": "13.4 Color, highlights, and other styles\nThere are a lot of options for changing the appearance of text in the kableExtra package. If you are interested, look at the vignette linked in the further reading.\nThe two styles I use frequently are alternating shading to help you read across rows and making the columns only wide enough to display your data.\n\nt1 |&gt; kable() |&gt; kable_styling(full_width = FALSE, bootstrap_options = \"striped\")\n\n\n\nspecies\nisland\ncount\nbody_mass_g\n\n\n\nAdelie\nBiscoe\n44\n3709.659\n\n\nAdelie\nDream\n56\n3688.393\n\n\nAdelie\nTorgersen\n51\n3706.373\n\n\nChinstrap\nDream\n68\n3733.088\n\n\nGentoo\nBiscoe\n123\n5076.016"
  },
  {
    "objectID": "lessons/113-data-tables.html#captions",
    "href": "lessons/113-data-tables.html#captions",
    "title": "13  Format tables",
    "section": "\n13.5 Captions",
    "text": "13.5 Captions\nYou can add a caption to a table with the caption argument to kable.\n\nt1 |&gt; kable(caption = \"The number and average mass of penguins by species and island.\")\n\n\n\n\nspecies\nisland\ncount\nbody_mass_g\n\n\n\nAdelie\nBiscoe\n44\n3709.659\n\n\nAdelie\nDream\n56\n3688.393\n\n\nAdelie\nTorgersen\n51\n3706.373\n\n\nChinstrap\nDream\n68\n3733.088\n\n\nGentoo\nBiscoe\n123\n5076.016\n\n\n\nThe number and average mass of penguins by species and island."
  },
  {
    "objectID": "lessons/113-data-tables.html#putting-it-all-together",
    "href": "lessons/113-data-tables.html#putting-it-all-together",
    "title": "13  Format tables",
    "section": "\n13.6 Putting it all together",
    "text": "13.6 Putting it all together\nUsually you will want to combine these features to get the look you want. Your goal should always be to make a table that clearly displays your data.\n\nt1 |&gt; kable(digits = 1, format.args = list(big.mark = \",\"),\n             col.names = c(\"Species\", \"Island\", \"n\", \"Body mass (g)\"),\n             caption=\"The number and average mass of penguins by species and island.\") |&gt;\n  kable_styling(full_width = FALSE, bootstrap_options = \"striped\")\n\n\n\n\nSpecies\nIsland\nn\nBody mass (g)\n\n\n\nAdelie\nBiscoe\n44\n3,709.7\n\n\nAdelie\nDream\n56\n3,688.4\n\n\nAdelie\nTorgersen\n51\n3,706.4\n\n\nChinstrap\nDream\n68\n3,733.1\n\n\nGentoo\nBiscoe\n123\n5,076.0\n\n\n\nThe number and average mass of penguins by species and island."
  },
  {
    "objectID": "lessons/113-data-tables.html#adding-row-and-column-totals",
    "href": "lessons/113-data-tables.html#adding-row-and-column-totals",
    "title": "13  Format tables",
    "section": "\n13.7 Adding row and column totals",
    "text": "13.7 Adding row and column totals\nWe frequently make tables of counts of categorical variables. In these tables it can be helpful to add column or row totals. Sometimes we want to report those totals as percentages of a grand total. The janitor package makes it easy to add totals and percentages to rows and columns.\nWe’ll start with a matrix of counts showing the number of countries with life expectancy more than 75 years in each year and continent.\n\nt1 &lt;- gapminder |&gt; filter(lifeExp &gt; 75) |&gt;\n  group_by(year, continent) |&gt;\n  dplyr::summarize(n = n()) |&gt;\n  pivot_wider(names_from = \"continent\", values_from = \"n\", values_fill = 0)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nt1\n\n# A tibble: 7 × 6\n# Groups:   year [7]\n   year  Asia Europe Americas Oceania Africa\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;    &lt;int&gt;   &lt;int&gt;  &lt;int&gt;\n1  1977     1      5        0       0      0\n2  1982     2      7        1       0      0\n3  1987     3     11        2       1      0\n4  1992     5     17        3       2      0\n5  1997     6     19        5       2      0\n6  2002     7     20        7       2      1\n7  2007     9     22       10       2      1\n\n\nNow we will add row totals. No sum is computed for the first column since it is assumed to be a label.\n\nt1 |&gt; adorn_totals()\n\n  year Asia Europe Americas Oceania Africa\n  1977    1      5        0       0      0\n  1982    2      7        1       0      0\n  1987    3     11        2       1      0\n  1992    5     17        3       2      0\n  1997    6     19        5       2      0\n  2002    7     20        7       2      1\n  2007    9     22       10       2      1\n Total   33    101       28       9      2\n\n\nIf you think about this, you’ll realize this doesn’t make much sense! So let’s add column totals instead. Again the first column is ignored, assuming it is a label.\n\nt1 |&gt; adorn_totals(where = \"col\") \n\n year Asia Europe Americas Oceania Africa Total\n 1977    1      5        0       0      0     6\n 1982    2      7        1       0      0    10\n 1987    3     11        2       1      0    17\n 1992    5     17        3       2      0    27\n 1997    6     19        5       2      0    32\n 2002    7     20        7       2      1    37\n 2007    9     22       10       2      1    44\n\n\nThese tables are formatted differently compared to data.frames and tibbles, but they can still be reformatted as you would expect using |&gt; kable() |&gt; kable_styling().\nIncidentally, the janitor package has a powerful table generating function tabyl which does the counting we started this section with. We still need the filter to retain only rows with life expectancy more than 75 years. The columns are reported alphabetically instead of according to the order they appear in the original dataset.\n\ngapminder |&gt; filter(lifeExp &gt; 75) |&gt;\n  tabyl(year, continent) #  |&gt; adorn_totals(where=\"col\")\n\n year Africa Americas Asia Europe Oceania\n 1977      0        0    1      5       0\n 1982      0        1    2      7       0\n 1987      0        2    3     11       1\n 1992      0        3    5     17       2\n 1997      0        5    6     19       2\n 2002      1        7    7     20       2\n 2007      1       10    9     22       2"
  },
  {
    "objectID": "lessons/113-data-tables.html#adding-grouping-for-rows-and-columns",
    "href": "lessons/113-data-tables.html#adding-grouping-for-rows-and-columns",
    "title": "13  Format tables",
    "section": "\n13.8 Adding grouping for rows and columns",
    "text": "13.8 Adding grouping for rows and columns\nSometimes it is desirable to add a grouping label over a series of columns. For example, the in the table showing totals above, we can add a header “Continent” over the appropriate columns. We do this to a kable formatted table using the function add_header_row. This function takes an argument which is a vector of pairs: labels for each column and the number of columns that label should span. There must be enough lables to span all the columns. We will add a blank label for the first and last column (spanning 1 column each), and a label “Continent” spanning the 5 continents.\n\nt1 |&gt; adorn_totals(where = \"col\") |&gt;\n  kable() |&gt;\n  add_header_above(c(\" \" = 1, \n                  \"Continent\" = 5,\n                  \" \" = 1)) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinent\n\n\n\n\nyear\nAsia\nEurope\nAmericas\nOceania\nAfrica\nTotal\n\n\n\n\n1977\n1\n5\n0\n0\n0\n6\n\n\n1982\n2\n7\n1\n0\n0\n10\n\n\n1987\n3\n11\n2\n1\n0\n17\n\n\n1992\n5\n17\n3\n2\n0\n27\n\n\n1997\n6\n19\n5\n2\n0\n32\n\n\n2002\n7\n20\n7\n2\n1\n37\n\n\n2007\n9\n22\n10\n2\n1\n44\n\n\n\n\n\nIt can also be helpful to add grouping to rows. For example, we could label the first 5 rows as being part of the 20th century and the last 2 rows as being part of the 21st century. We add row labels one at a time, giving a label for the row and the numbers of the rows the header should span. We will do this with the kableExtra function group_rows.\n\nt1 |&gt; adorn_totals(where = \"col\") |&gt;\n  kable() |&gt;\n  group_rows(\"20th century\", 1, 5) |&gt;\n  group_rows(\"21st century\", 6, 7) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nyear\nAsia\nEurope\nAmericas\nOceania\nAfrica\nTotal\n\n\n\n20th century\n\n\n1977\n1\n5\n0\n0\n0\n6\n\n\n1982\n2\n7\n1\n0\n0\n10\n\n\n1987\n3\n11\n2\n1\n0\n17\n\n\n1992\n5\n17\n3\n2\n0\n27\n\n\n1997\n6\n19\n5\n2\n0\n32\n\n\n21st century\n\n\n2002\n7\n20\n7\n2\n1\n37\n\n\n2007\n9\n22\n10\n2\n1\n44"
  },
  {
    "objectID": "lessons/113-data-tables.html#further-reading",
    "href": "lessons/113-data-tables.html#further-reading",
    "title": "13  Format tables",
    "section": "\n13.9 Further reading",
    "text": "13.9 Further reading\n\nUsing kable to format tables\nVignette on using kable and kableExtra\n\nhttps://people.ok.ubc.ca/jpither/modules/Tables_markdown.html"
  },
  {
    "objectID": "lessons/114-asking-questions.html#asking-questions-to-learn",
    "href": "lessons/114-asking-questions.html#asking-questions-to-learn",
    "title": "14  Getting help and asking questions",
    "section": "\n14.1 Asking questions to learn",
    "text": "14.1 Asking questions to learn\nHere are some examples of questions I’ve asked myself about R and data visualization:\n\nHow do I add error bars to a scatter plot?\nHow do I customize the colours on a plot?\nHow do I change axis labels?\n\nNow I’ll show you the steps I recommend to answer these questions. If you have no idea how to solve the problem, these steps will take you a while and you may need to ask someone for help. If you have solved the problem once before, it will be easier the second time, because you will remember part of the solution. If you have solved the problem several times, the same steps are useful, but you may only need a hint to remember the answer.\n\n14.1.1 Start with google\nIf I have no idea how to solve a problem, I go to google. It’s often good to add “R” or “tidyverse” or “ggplot” to your search query. Here are a few searches that I’ve found helpful for the questions above:\n\nerror bars\nplot colours\naxis labels\n\nI look for a link on the first page or two that uses a reference I’ve found helpful. I like the tidyverse and ggplot2 documentation, the “R Graph Gallery” and some online courses (one at UBC linked below.)\nUsing the incognito mode in google to get results not influenced by my search history, I recommend the following starting places (lots of the other links you’ll find are fine too):\n\nerror bars\nplot colours\naxis labels\n\n14.1.2 Find an example\nThe web pages I find most helpful have lots of examples. Sometimes I realize that there are many ways to interpret the idea in my head and the examples help me figure out what I really want. I will often search three or more google links until I find an example I like – the output looks right and the code doesn’t look too complicated.\nI also look for an example that looks like the code I write myself. There are many different ways to solve some tasks, but some of those methods will be harder for you to understand. So if you find a “solution”, but it’s too hard to understand—there might be a way that’s easier for you to understand. In this course we are using a style called “tidyverse” which emphasizes descriptive function names. Other styles include “base” and “data table”. If you see a lot of pipes (%&gt;% or |&gt;) and familiar functions (ggplot, summarize, select, group_by), it’s tidyverse style. If you see a lot of dollar signs ($) or square brackets ([, ]) it’s probably a different coding style and you can expect it to be harder to understand from your current perspective. Go ahead and learn other styles if you are excited to explore more!\n\n14.1.3 Look at the documentation\nThe link for axis labels I picked above is just the help page for labs. You can see it by typing “labs” into the help search box on Rstudio. But maybe you didn’t know to search for “labs” – that’s where google helps.\nR help pages all have the same format and the part most useful to beginners is the “Examples” section at the bottom. Cut and paste some examples from the help page for “labs” into the console to see how the function works.\n\n14.1.4 Reproduce an example\nCut and paste an example from a help page or web page into your R session. If it doesn’t work right away, check that (a) you have installed the package and typed the required “library()” function, and (b) you have defined any objects needed for the task – sometimes these are defined near the top of a web or the examples on a help page. For example, on the “labs” help page, the first example creates a plot and stores it in the object called p. If you skip that first part of the example, the rest of the examples won’t work.\n\n14.1.5 Modify the example\nTest your understanding by modifying the example a little bit. If it does more than you need, try removing parts you don’t want. This is the same skill I suggest for every lesson in this course – experiment with some code that works to minimize frustration.\nGradually turn the example into what you want, changing one feature at a time. Using your data instead of the data in the example. Using your preferences (colours, font size, text for titles, etc.).\n\n14.1.6 Make notes to explain the solution\nI strongly suggest you keep your experiments and notes in a separate R markdown file, then call it something appropriate (“drawing-errorbars”) and put it in a folder called something like “examples”. Knit the document before moving on to be sure it works and is complete."
  },
  {
    "objectID": "lessons/114-asking-questions.html#example-learning-how-to-draw-error-bars",
    "href": "lessons/114-asking-questions.html#example-learning-how-to-draw-error-bars",
    "title": "14  Getting help and asking questions",
    "section": "\n14.2 Example: learning how to draw error bars",
    "text": "14.2 Example: learning how to draw error bars\n\nSkim the R cookbook page\nThe data are fine – some quantitative data and categorical data for grouping\nThe first plot looks like what I want, at least approximately!\nUh oh! I need a variable called tgc which is created from ToothGrowth (part of R) using a function summarySE (which I don’t have.)\nReading more, I see summarySE is defined near the bottom of the page. It looks complicated! Think a bit.\nCut and paste the function definition, look at the result. I see. I just need to use group_by and summarize to compute means, sample sizes, and qt (it looks up a value in a t-table)\nI make a version of summarySE myself just to be sure I understand what it does. Mine is simpler – it just does the calculation instead of creating a new function. Making a simpler example is a great way to understand what the more complicated version does.\n\n\nmy_summarySE &lt;- ToothGrowth |&gt; group_by(supp, dose) |&gt;\n    dplyr::summarize(N = n(),\n              mean_len = mean(len),\n              sd = sd(len),\n              se = sd / sqrt(N),\n              ci = se * qt(0.975, N-1))\n\n`summarise()` has grouped output by 'supp'. You can override using the\n`.groups` argument.\n\n\nGreat – this produces the same output as summarySE(ToothGrowth). (I used a different name for the mean column because I need len to compute the standard deviation after I compute the mean. If you use the function from the help page and write summarize above instead of dplyr::summarize you’ll get an error because the example uses the plyr package that has a different definition of summarize. I’ve bumped into that error a lot, so I recognize it right away. It will probably stump you the first time you experience it.)\n\nCut and paste the example, modified to use my data (mean_len instead of len):\n\n\nggplot(my_summarySE, aes(x=dose, y=mean_len, colour=supp)) + \n    geom_errorbar(aes(ymin=mean_len-se, ymax=mean_len+se), width=.1) +\n    geom_line() +\n    geom_point()\n\n\n\n\nGet rid of the lines, because I don’t want them.\n\nggplot(my_summarySE, aes(x=dose, y=mean_len, colour=supp)) + \n    geom_errorbar(aes(ymin=mean_len-se, ymax=mean_len+se), width=.1) +\n    geom_point()\n\n\n\n\nNow, drop in my favourite data, the Palmer penguins, making only the changes that are absolutely necessary. I had the overlapping points problem mentioned on the help page, so I borrowed it’s position_dodge idea. I also noticed that the web page used se instead of the larger ci to draw the error bars, so I changed that too.\n\nlibrary(palmerpenguins)\nmy_summarySE &lt;- penguins %&gt;% na.omit() %&gt;%\n  group_by(species, island) %&gt;%\n    dplyr::summarize(N = n(),\n              mean_mass = mean(body_mass_g),\n              sd = sd(body_mass_g),\n              se = sd / sqrt(N),\n              ci = se * qt(0.975, N-1))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\npd &lt;- position_dodge(0.2) # move them .05 to the left and right\nggplot(my_summarySE, aes(x=species, y=mean_mass, colour=island)) + \n    geom_errorbar(aes(ymin=mean_mass-ci, ymax=mean_mass+ci), width=.1, position = pd) +\n    geom_point(position=pd)"
  },
  {
    "objectID": "lessons/114-asking-questions.html#asking-question-in-this-course",
    "href": "lessons/114-asking-questions.html#asking-question-in-this-course",
    "title": "14  Getting help and asking questions",
    "section": "\n14.3 Asking question in this course",
    "text": "14.3 Asking question in this course\nAsking good questions can be challenging, especially when you are working on your own. In a classroom, it’s fairly easy: Can you give another example? Why did you do that calculation? What does the answer mean? Many questions you might ask in person don’t translate well to other contexts: email, office hours, or online forums, because the immediate context of group discussion is missing. So it’s up to you to add that context when you ask a question.\nHere I describe several different ways to get answers to your questions. Knowing which method to use is itself a skill, so I provide some guidance for that as well. It’s important to ask questions – and I want you to ask questions, so much that you should consider it part of your assigned work to formulate questions.\n\n14.3.1 Starting out\nAt the start of the course you will have a bunch of computer tasks (installing R and Rstudio, installing R packages, understanding the basics of Rstudio, working with git and github). These will range from easy to difficult depending on your previous experience. I have tried to provide written and video information to guide you, but if you get stuck on any of these, or the work associated with a “task” at any point in the course, ask for help right away.\n\n14.3.2 Learning R basics\nAs you start to write R commands and make R markdown documents you will encounter problems arising from typos (forgetting, or not knowing where to place) commas, brackets, backticks, quotation marks, pipes (%&gt;% or |&gt;), and many other bits of syntax necessary for communication with the computer. The error messages will frequently be hard to decipher. The best way to learn to solve these problems is the following: take a working R markdown document or example of R code and change it slightly. Make a prediction of what will happen and then get the computer to process the code. Compare what happens to what you predicted. Do this regularly until you can reliably predict outcomes of any changes you can think of. This is “playing computer” in your head—try to understand how it will react to your input. When something happens that you don’t understand: ask someone why. Describe what you did, what you thought would happen, and what really happened.\n\n14.3.3 Problem solving\nOnce you have developed some fluency for working with R, you will start to learn to solve problems. This means that you are trying to accomplish a data visualization task, such as make a single plot, use colour in some way, change the kind of plot, add text to an axis, or thousands of other tasks. At the start you will be just doing tasks I’ve given you and later on you will be trying to achieve your own goals by solving problems. The method described above is useful here: when you have a working example, change it in some way and predict the result. Make changes that you think will lead to good outcomes, but also make playful or “silly” changes that will lead to poor quality graphs but still help you learn to use the tools. This kind of experimentation is essential for develop a robust and accurate mental model of how R (or any other) computer software works.\n\n14.3.4 Learning new methods R\nThe tools available within R are so extensive that you can never learn them all by being taught by a course or a textbook or a website. To really master R you need to learn how to learn new tools.\nR functions have help pages – accessed from the help tab in Rstudio or by prefacing the function with a ‘?’ in the R console. These help pages have a very specific style and require practice to learn how to read them. One of the best features of the help page, especially when you are starting out, is the examples at the bottom of the page. Usually these can be simply cut and pasted into the console and you can observe the results to see how the function author intended them to be used.\nMany R packages have vignettes. These are extended documents intended to explain how to use a key feature of the package. You can find the vignettes on the web page where you found the software (CRAN or github) or using the vignette function. Read the help page – and especially the examples – to see how to use the vignette function.\nWe are focussing on the tidyverse set of packages. The tidyverse website contains help, examples, and reference documentation for these packages. These web pages combine the features of books, tutorials, vignettes, and help pages. All are worth your time! Keep an eye out for cheatsheets that are especially helpful after you learn the basics of a package.\nOne of the many reasons we are using git and GitHub in this course is that it will make it easier for you to ask me a question. If you have a problem with an R markdown document or making a function work the way you think it should or you just don’t know how to interpret the results of a calculation, you can ask me. But how do you know how much information to give me? And how do you make that easy? Send an email with the question – what are you trying to do, what is the problem or error message, and why do you think might be going wrong. Or, you can put the computer code into an R markdown document, commit the changes to your project, and push it to Github. Then tell me the name of the repository and file and I’ll have the full set of code that lead to your problem. That will make it much easier for me to help you. (Note that I can only see private repositories that we both have access to, for example course work repositories. So use those repositories to share files and ask questions.)"
  },
  {
    "objectID": "lessons/114-asking-questions.html#solving-computer-problems-debugging",
    "href": "lessons/114-asking-questions.html#solving-computer-problems-debugging",
    "title": "14  Getting help and asking questions",
    "section": "\n14.4 Solving computer problems (“Debugging”)",
    "text": "14.4 Solving computer problems (“Debugging”)\nThere are two kinds of computer problems,\n\nyour code does something, but not what you want, and\nyour code gives you and error, no result, and you don’t know why.\n\nBoth problems can be frustrating, but the second kind is especially difficult when you are starting out.\nThere are sophisticated tools for debugging in R, but we won’t discuss them in this course.\nMy recommendation for both kinds of problem is the same: simplify what you are doing until something works and you understand what is going on. Then add one step at a time. If you are adapting code you found somewhere, make sure the original example works. Develop a “mental model” of what the computer is doing, and test it by modifying your code. Be creative and relaxed. Try hard not to get frustrated! Errors are a really important part of learning – they help you develop deeper and more powerful understanding of the tools you are trying to use.\n\n14.4.1 Suggestions for asking questions\nA few suggestions:\n\nExplain the context (is this about material from a video, the course notes, an assignment – which one?, or just something that you are wondering about)\nSay what you know (the topic is X, or the question asks for Y)\nSay what you are trying to do (understand an argument, perform a calculation, fix an error in code)\nSay what you think comes next or where you think you are stuck (I’m trying to figure out X, I don’t know why the plot looks like Y)\n\n14.4.2 Asking a complete question\nIf you are trying to solve a problem with R, and the code is not working as you expect, make the smallest possible example of the problem that you can. This is called a “reproducible example” – meaning that you have provided enough information that the person you are asking for help can make the problem repeat on their computer. There is an R package to help you create these examples called reprex, but you don’t need to do this in our course.\nThe reprex function tests your code to see if you’ve included all the necessary to enable someone else to understand your code. This includes which libraries you are using and any variables that must be defined. Select your code that demonstrates the problem, then copy it to the clipboard. Type reprex() in the R Console. If your example is complete, it will appear in the preview window on the lower right. This can be cut and pasted into an email.\n\n14.4.3 What does this unknown function do?\nThere is another important kind of question – what does an-unfamiliar-R-function do? This is a bit easier since you don’t need to translate a goal into a question about R, you just need to figure out what the function does.\nYou might be wondering what position_dodge does. It’s fairly obvious from the step-by-step example, but try to figure out by reading the help page. It’s a bit hard to understand, but you should be able to get that it is a function that moves (dodges) the horizontal position of a point, error bar, or other geom."
  },
  {
    "objectID": "lessons/114-asking-questions.html#how-to-learn-material-in-this-course",
    "href": "lessons/114-asking-questions.html#how-to-learn-material-in-this-course",
    "title": "14  Getting help and asking questions",
    "section": "\n14.5 How to learn material in this course",
    "text": "14.5 How to learn material in this course\nThe course goals are summarized in the welcome to the course. One of the most important is learning how to learn to use new tools for data visualization. This sort of learning is hard but very worthwhile. It’s also a great transferrable skill. If you can master the material in this course and learn how to keep learning, you’ll be able to apply what you learned to a wide range of computing skills."
  },
  {
    "objectID": "lessons/114-asking-questions.html#getting-help",
    "href": "lessons/114-asking-questions.html#getting-help",
    "title": "14  Getting help and asking questions",
    "section": "\n14.6 Getting help",
    "text": "14.6 Getting help\n\n\nR studio tips presented as animated GIFs\nLearning resources from R studio"
  },
  {
    "objectID": "lessons/114-asking-questions.html#more-advanced-help",
    "href": "lessons/114-asking-questions.html#more-advanced-help",
    "title": "14  Getting help and asking questions",
    "section": "\n14.7 More advanced help",
    "text": "14.7 More advanced help\n\n\nWhat they forgot to teach you about R\n\nincluding Debugging tools"
  },
  {
    "objectID": "lessons/118-working-with-models.html#adding-smooths-to-a-visualization",
    "href": "lessons/118-working-with-models.html#adding-smooths-to-a-visualization",
    "title": "15  Working with models",
    "section": "\n15.1 Adding smooths to a visualization",
    "text": "15.1 Adding smooths to a visualization\nIn this lesson we will focus on visualizing models that smooth data. In the next two lessons we will take a closer look at these models, in particular getting diagnostic information about model fits and making data frames with quantitative output from the models.\nFirst, we’ll take a subset of the gapminder data and make a simple scatterplot.\n\ngp &lt;- gapminder |&gt; filter(continent == \"Europe\")\np1 &lt;- gp |&gt; ggplot(aes(x = year, y = log10(gdpPercap))) +\n  geom_point(alpha = 0.5) + theme_bw()\np1\n\n\n\n\nNow we will add some lines to highlight trends in the data.\nFirst use linear regression to add a straight line.\n\np1 + geom_smooth(method = \"lm\", formula = y ~ x)\n\n\n\n\nWe can change the formula to other curves. If you want a smoothed line that follows the data, use a B-spline with degree 3 (cubic).\n\np1 + geom_smooth(method = \"lm\", \n                 formula = y ~ splines::bs(x, df=5, degree=3))\n\n\n\n\nAnother way to describe a cubic spline is with a generalized additive model (or GAM). This model is quite a bit more sophisticated than the previous spline. Depending on the data, the model fitting process will add or remove oscillations (“wiggles”) from the curve.\n\np1 + geom_smooth(method = \"gam\", \n                 formula = y ~ s(x, bs = \"cs\"))\n\n\n\n\nAnother data smoothing method is called LOESS (locally estimated scatterplot smoothing) which does not require you to describe a model (line, cubic spline, etc.) for the data. Instead the model takes small subsets of the data along the independent variable and makes many models (usually first or second degree polynomials) and joins them together.\n\np1 + geom_smooth(method = \"loess\",\n                 formula = y ~ x)\n\n\n\n\nAs a final example we can try quantile regression. This will make lines to approximate a specific quantile (10%, 50%, 90% are shown below) instead of a mean. This might be a good idea for these data in particular since for each year we have a range of countries spanning a large distribution of GDP per capita, so the analysis allows us to see how the short, median, and long life expectancies have changed over time. The slope of the 10 and 90 percentiles seems to be very similar, but the median life expectancy has increased faster, as countries have shifted from low life expectancy to high life expectancy.\n\np1 + geom_quantile(formula = y ~ x, \n                   method = \"rq\", \n                   lambda = 1, quantiles = c(0.1, 0.50, 0.90))"
  },
  {
    "objectID": "lessons/118-working-with-models.html#putting-several-models-on-a-single-plot",
    "href": "lessons/118-working-with-models.html#putting-several-models-on-a-single-plot",
    "title": "15  Working with models",
    "section": "\n15.2 Putting several models on a single plot",
    "text": "15.2 Putting several models on a single plot\nIn this example I’ll show you how you can plot more than one smooth on the same plot (easily, but adding multiple smooths together with +) and how to add a legend and control the colours used for each line. You can learn more about colours on this cheatsheet and you can get a list of all colour names with the function colors().\n\np1 + geom_smooth(method = \"lm\", \n                 formula = y ~ x, \n                 aes(color = \"Linear\", fill = \"Linear\")) +\n  geom_smooth(method = \"loess\", \n              formula = y ~ x, \n              aes(color = \"LOESS\", fill = \"LOESS\")) + \n  scale_color_manual(name = \"Smooth\",  \n                     values = c(\"salmon\", \"limegreen\" ) ) + \n  scale_fill_manual(name = \"Smooth\", \n                    values = c(\"salmon\", \"limegreen\" ) ) + \n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "lessons/118-working-with-models.html#smooths-on-facetted-plots",
    "href": "lessons/118-working-with-models.html#smooths-on-facetted-plots",
    "title": "15  Working with models",
    "section": "\n15.3 Smooths on facetted plots",
    "text": "15.3 Smooths on facetted plots\nSmoothing works really easily on facetted plots. Simply adding a facet function to a plot will make a different smooth for each facet.\nWe will make lots of facets, one for each country, and add a smooth to each panel.\n\np1 + facet_wrap(~country) +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x)\n\n\n\n\nThis is not an ideal facetted plot – there are too many facets (countries) and the text labels don’t fit well in the space available. You might draw a plot like this for yourself to explore the data and plan an analysis or other plot."
  },
  {
    "objectID": "lessons/118-working-with-models.html#summary",
    "href": "lessons/118-working-with-models.html#summary",
    "title": "15  Working with models",
    "section": "\n15.4 Summary",
    "text": "15.4 Summary\nYou’ve learned how to add linear regressions (lm), and smooths (gam, loess) to plots."
  },
  {
    "objectID": "lessons/118-working-with-models.html#further-reading",
    "href": "lessons/118-working-with-models.html#further-reading",
    "title": "15  Working with models",
    "section": "\n15.5 Further reading",
    "text": "15.5 Further reading\n\nIn the next lesson we look at linear models in a bit more detail, showing how to extract residuals, predictions, and confidence intervals.\nHealy Chapter 6 Working with models, (Healy 2018)\n\n\n\n\n\n\n\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press. https://socviz.co/."
  },
  {
    "objectID": "lessons/119-linear-models.html#making-linear-models",
    "href": "lessons/119-linear-models.html#making-linear-models",
    "title": "16  Linear models",
    "section": "\n16.1 Making linear models",
    "text": "16.1 Making linear models\nWe will always make linear models with variables from a data frame. Designate one variable the response variable, which we will attempt to predict using one or more other variables, called predictors. You are not restricted to variables in your data frame; you can transform the variables first, for example by squaring, taking logarithms, or applying some other function. Additionally, you can use categorical (or factor) variables as predictors and in combination with quantitative variables. Be aware that the more variables or transformations you add to your list of predictors, the more likely they will be correlated and your model will be very hard to interpret. These issues are discussed in statistics courses on regression.\nOnce your data frame is created, write the linear model as a “formula” object, meaning as an equation but with a ~ instead of an = to indicate that you are modelling the left hand side and allowing for a specific model for the mismatch between predictors and response.\nWe have seen that the price of a diamond increases a bit faster than linearly as the mass of the diamond increases, we will try both a linear model and a quadratic model for data in the diamonds dataframe.\n\nlinear_model1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(linear_model1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\nquadratic_model1 &lt;- lm(price ~ poly(carat, 2), \n                       data = diamonds)\nsummary(quadratic_model1)\n\n\nCall:\nlm(formula = price ~ poly(carat, 2), data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26350.0   -724.2    -35.9    445.8  12881.1 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     3.933e+03  6.631e+00   593.1   &lt;2e-16 ***\npoly(carat, 2)1 8.539e+05  1.540e+03   554.4   &lt;2e-16 ***\npoly(carat, 2)2 3.757e+04  1.540e+03    24.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1540 on 53937 degrees of freedom\nMultiple R-squared:  0.851, Adjusted R-squared:  0.851 \nF-statistic: 1.54e+05 on 2 and 53937 DF,  p-value: &lt; 2.2e-16\n\nlinear_model2 &lt;- lm(price ~ carat + clarity + color, \n                    data = diamonds |&gt; mutate(clarity = factor(clarity, ordered=FALSE), \n                                               color = factor(color, ordered=FALSE)))\nsummary(linear_model2)\n\n\nCall:\nlm(formula = price ~ carat + clarity + color, data = mutate(diamonds, \n    clarity = factor(clarity, ordered = FALSE), color = factor(color, \n        ordered = FALSE)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17310.9   -678.0   -192.2    473.0  10313.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6699.95      47.20 -141.94   &lt;2e-16 ***\ncarat        8856.23      12.10  731.86   &lt;2e-16 ***\nclaritySI2   2832.65      44.77   63.27   &lt;2e-16 ***\nclaritySI1   3795.47      44.50   85.30   &lt;2e-16 ***\nclarityVS2   4466.10      44.69   99.93   &lt;2e-16 ***\nclarityVS1   4785.79      45.40  105.42   &lt;2e-16 ***\nclarityVVS2  5234.16      46.72  112.03   &lt;2e-16 ***\nclarityVVS1  5351.85      48.03  111.42   &lt;2e-16 ***\nclarityIF    5718.23      52.01  109.95   &lt;2e-16 ***\ncolorE       -216.45      18.53  -11.68   &lt;2e-16 ***\ncolorF       -314.92      18.72  -16.82   &lt;2e-16 ***\ncolorG       -509.09      18.33  -27.78   &lt;2e-16 ***\ncolorH       -985.01      19.49  -50.54   &lt;2e-16 ***\ncolorI      -1441.77      21.90  -65.84   &lt;2e-16 ***\ncolorJ      -2340.83      27.03  -86.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1170 on 53925 degrees of freedom\nMultiple R-squared:  0.914, Adjusted R-squared:  0.9139 \nF-statistic: 4.092e+04 on 14 and 53925 DF,  p-value: &lt; 2.2e-16\n\n\nYou can write the formulas for the regression lines using the equatiomatic package. This package must be installed from GitHub using remotes::install_github(\"datalorax/equatiomatic\"). To get the equations formatted properly, you need to add results='asis' to the {r} line in your R markdown document. The equations display correctly in the knitted document, but are show as LaTeX code in the Rstudio preview. At present this does not work correctly for functions with mathematical transformations like log, exp, poly(x, 2), so I’ll only show the linear models here.\nlibrary(equatiomatic)\nextract_eq(linear_model1)\n\n\\[\n\\operatorname{price} = \\alpha + \\beta_{1}(\\operatorname{carat}) + \\epsilon\n\\]\n\nextract_eq(linear_model2, wrap=TRUE)\n\n\\[\n\\begin{aligned}\n\\operatorname{price} &= \\alpha + \\beta_{1}(\\operatorname{carat}) + \\beta_{2}(\\operatorname{clarity}_{\\operatorname{SI2}}) + \\beta_{3}(\\operatorname{clarity}_{\\operatorname{SI1}})\\ + \\\\\n&\\quad \\beta_{4}(\\operatorname{clarity}_{\\operatorname{VS2}}) + \\beta_{5}(\\operatorname{clarity}_{\\operatorname{VS1}}) + \\beta_{6}(\\operatorname{clarity}_{\\operatorname{VVS2}}) + \\beta_{7}(\\operatorname{clarity}_{\\operatorname{VVS1}})\\ + \\\\\n&\\quad \\beta_{8}(\\operatorname{clarity}_{\\operatorname{IF}}) + \\beta_{9}(\\operatorname{color}_{\\operatorname{E}}) + \\beta_{10}(\\operatorname{color}_{\\operatorname{F}}) + \\beta_{11}(\\operatorname{color}_{\\operatorname{G}})\\ + \\\\\n&\\quad \\beta_{12}(\\operatorname{color}_{\\operatorname{H}}) + \\beta_{13}(\\operatorname{color}_{\\operatorname{I}}) + \\beta_{14}(\\operatorname{color}_{\\operatorname{J}}) + \\epsilon\n\\end{aligned}\n\\]\n\nand you can fill in the results showing the numeric coefficients in the equations:\nextract_eq(linear_model1, use_coefs=TRUE, fix_signs=TRUE)\n\n\\[\n\\operatorname{\\widehat{price}} = -2256.36 + 7756.43(\\operatorname{carat})\n\\]\n\nextract_eq(linear_model2, use_coefs=TRUE, terms_per_line = 3,\n           fix_signs=TRUE, wrap=TRUE)\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{price}} &= -6699.95 + 8856.23(\\operatorname{carat}) + 2832.65(\\operatorname{clarity}_{\\operatorname{SI2}})\\ + \\\\\n&\\quad 3795.47(\\operatorname{clarity}_{\\operatorname{SI1}}) + 4466.1(\\operatorname{clarity}_{\\operatorname{VS2}}) + 4785.79(\\operatorname{clarity}_{\\operatorname{VS1}})\\ + \\\\\n&\\quad 5234.16(\\operatorname{clarity}_{\\operatorname{VVS2}}) + 5351.85(\\operatorname{clarity}_{\\operatorname{VVS1}}) + 5718.23(\\operatorname{clarity}_{\\operatorname{IF}})\\ - \\\\\n&\\quad 216.45(\\operatorname{color}_{\\operatorname{E}}) - 314.92(\\operatorname{color}_{\\operatorname{F}}) - 509.09(\\operatorname{color}_{\\operatorname{G}})\\ - \\\\\n&\\quad 985.01(\\operatorname{color}_{\\operatorname{H}}) - 1441.77(\\operatorname{color}_{\\operatorname{I}}) - 2340.83(\\operatorname{color}_{\\operatorname{J}})\n\\end{aligned}\n\\]\n\nThese results are the jumping off point for a lot more exploration."
  },
  {
    "objectID": "lessons/119-linear-models.html#smoothing-on-facets",
    "href": "lessons/119-linear-models.html#smoothing-on-facets",
    "title": "16  Linear models",
    "section": "\n16.2 Smoothing on facets",
    "text": "16.2 Smoothing on facets\nThe geom_smooth function is especially powerful for facetted plots. The smooth is automatically computed and plotted for each facet separately.\n\ndiamonds |&gt; \n  filter(color %in% c(\"D\", \"F\", \"H\", \"J\"), \n         clarity %in% c(\"SI1\", \"VS1\", \"IF\")) |&gt;\n  ggplot(aes(x=carat, y=price)) + \n  geom_point(aes(color=cut)) +\n  facet_grid(color ~ clarity) + \n  scale_colour_viridis_d(begin=0, end =0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x,2),\n              color = \"black\", size = 0.5) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nYou can fit all these lines using lm as well, using color and clarity as variables in the regression equation. The way facet_* and geom_smooth combine together makes the visualization of these lines very easy.\nNotice that I’ve moved the color=cut from the ggplot function to the geom_point function as the aesthetic for only the points. If the colour was specified in the first ggplot function, the geom_smooth would “inherit” this aesthetic mapping and would make a separate smooth for each cut (5 lines per panel). There are not enough data in some panels for some cuts to do a good job, so I revised the plot to only draw one smooth per facet. You should move the color=cut back to the ggplot call to see how the result changes."
  },
  {
    "objectID": "lessons/119-linear-models.html#robust-regression",
    "href": "lessons/119-linear-models.html#robust-regression",
    "title": "16  Linear models",
    "section": "\n16.3 Robust regression",
    "text": "16.3 Robust regression\nWe demonstrated the importance of visualizing data at the start of the course by appealing to the example of Anscombe’s quartet. This is a set of four data sets which all have the same means, standard deviations, and correlation. Here I’ll show how to plot four lines on a single plot and how to avoid at least one of the problems with outliers by using robust regression from the MASS package.\nRobust regression is designed to be less influenced by outliers. Robust regression is a big improvement for group 3, but has little effect on any of the other problems. (Try formula = y ~ poly(x,2) to fix panel 2. There is no fix for the problem in panel 4.)\n\np1 &lt;- anscombe |&gt;\n pivot_longer(everything(),\n   names_to = c(\".value\", \"set\"),\n   names_pattern = \"(.)(.)\"\n ) |&gt; ggplot(aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~ set) \n# p1 + geom_smooth(method=\"lm\") # Try this instead\np1 + geom_smooth(method=MASS::rlm) # , formula  = y ~ poly(x,2))"
  },
  {
    "objectID": "lessons/119-linear-models.html#predicting-quantiles",
    "href": "lessons/119-linear-models.html#predicting-quantiles",
    "title": "16  Linear models",
    "section": "\n16.4 Predicting quantiles",
    "text": "16.4 Predicting quantiles\nYou can also try to predict quantiles of your data. Here we make a linear model for the 0.05, 0.50 (median), and 0.95 quantiles.\n\ndiamonds |&gt; filter(cut == \"Ideal\", color == \"G\") |&gt;\n  ggplot(aes(x=carat, y = price)) + \n  geom_point() +\n  geom_quantile(method = rqss, formula = y ~ x,\n                lambda = 1, quantiles = c(0.05, 0.5, 0.95))\n\nWarning in sparse.model.matrix(object, data = data, contrasts.arg =\ncontrasts.arg, : non-list contrasts argument ignored\n\nWarning in sparse.model.matrix(object, data = data, contrasts.arg =\ncontrasts.arg, : non-list contrasts argument ignored\n\nWarning in sparse.model.matrix(object, data = data, contrasts.arg =\ncontrasts.arg, : non-list contrasts argument ignored"
  },
  {
    "objectID": "lessons/119-linear-models.html#quantitative-model-output",
    "href": "lessons/119-linear-models.html#quantitative-model-output",
    "title": "16  Linear models",
    "section": "\n16.5 Quantitative model output",
    "text": "16.5 Quantitative model output\nThe broom package has functions to obtain model coefficients and compute residuals, predictions, and confindence intervals. Healy also shows how to use broom to fit many models (such as a model for each facet) in his Section 6.6, but I will skip those steps.\nGet the coefficients (and standard errors, t-statistic, p-value and confidence intervals) from your model using tidy.\n\ntidy(linear_model1, conf.int = TRUE) |&gt; kable(digits = 1)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-2256.4\n13.1\n-172.8\n0\n-2281.9\n-2230.8\n\n\ncarat\n7756.4\n14.1\n551.4\n0\n7728.9\n7784.0\n\n\n\n\n\nThe second model has the effect of each level of clarity on price (relative to the base case of I1). Here’s how we can plot the regression coefficients as dot plots with uncertainties using the output from tidy.\n\ntidy(linear_model2, conf.int = TRUE) |&gt;\n  filter(str_starts(term, \"clarity\")) |&gt;\n  ggplot(aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + \n  geom_pointrange()\n\n\n\n\nThis can also be done easily using coefplot:\n\nlibrary(coefplot)\ncoefplot(linear_model2, sort = \"magnitude\", intercept = FALSE)\n\n\n\n\nYou can get statistics for the model using glance. We don’t discuss these results, but if you have taken a statistics course with topics on regression you should recognize at least some of these results.\n\nglance(linear_model1) |&gt; kable()\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n0.8493305\n0.8493277\n1548.562\n304050.9\n0\n1\n-472730.3\n945466.5\n945493.2\n129345695398\n53938\n53940\n\n\n\n\nThe augment function makes it easy to plot residuals and predicted values for many models (see ?augment). We can use augment on the model object to get a data frame with the data used in the model plus fitted (predicted) values, residuals, and other quantities. You can add the other data not provided in the model object by including data = diamonds in the augment function (output not shown.)\n\naugment(linear_model1, interval = \"prediction\") |&gt; head() |&gt; kable(digits = 2)\n\n\n\nprice\ncarat\n.fitted\n.lower\n.upper\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n326\n0.23\n-472.38\n-3507.64\n2562.88\n798.38\n0\n1548.57\n0\n0.52\n\n\n326\n0.21\n-627.51\n-3662.78\n2407.75\n953.51\n0\n1548.57\n0\n0.62\n\n\n327\n0.23\n-472.38\n-3507.64\n2562.88\n799.38\n0\n1548.57\n0\n0.52\n\n\n334\n0.29\n-7.00\n-3042.25\n3028.26\n341.00\n0\n1548.58\n0\n0.22\n\n\n335\n0.31\n148.13\n-2887.12\n3183.38\n186.87\n0\n1548.58\n0\n0.12\n\n\n336\n0.24\n-394.82\n-3430.08\n2640.44\n730.82\n0\n1548.57\n0\n0.47\n\n\n\n\n# augment(linear_model1, interval = \"prediction\", data = diamonds) |&gt; head()\n\nIf you generate new data, you can make and plot predictions easily.\n\nnew_data = tibble(carat = seq(0.20, 5.01, 0.01))\naugment(linear_model1, newdata = new_data, interval = \"prediction\") |&gt;\n  ggplot(aes(x = carat, y = .fitted, ymin = .lower, ymax = .upper)) + \n  geom_ribbon(fill=\"darkblue\", alpha = 0.5) + \n  geom_line(color=\"blue\")\n\n\n\n\nOf course, this is overly complicated for plotting a straight line, but the method can be adopted to many other models.\n\n16.5.1 Revisiting models from the previous lesson\nWhile tidy, glance and augment are very convenient, you will sometimes want simpler methods to get at model results. Here I will show you how to make models and predictions for each of the model types from the previous lesson.\nEach of these models can be fitted independently of making a plot. If you do this, you get a complex object called a fitted model that can be used to give you a lot of information about your model. Healy discusses how to look at the model output, but we will skip over this with two exceptions. We will look at the difference between model and data (called residuals) and making predictions with models.\nHere we will fit each of the models generated above and look at the summary output from each model. I’ll start by computing log(GDP) and the number of years since 1950 to use as variables in my models.\nA linear model fitting a straight line to the data (slope, intercept):\n\ngp &lt;- gapminder |&gt; filter(continent == \"Europe\") |&gt;\n  mutate(year1950 = year - 1950,\n         logGDP = log10(gdpPercap))\nm1 &lt;- lm(logGDP ~ year1950, data = gp)\nsummary(m1)\n\n\nCall:\nlm(formula = logGDP ~ year1950, data = gp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79487 -0.15384  0.05888  0.20269  0.45675 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.7416631  0.0271072  138.03   &lt;2e-16 ***\nyear1950    0.0107310  0.0007931   13.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2597 on 358 degrees of freedom\nMultiple R-squared:  0.3383,    Adjusted R-squared:  0.3365 \nF-statistic: 183.1 on 1 and 358 DF,  p-value: &lt; 2.2e-16\n\n\nA robust version of this line that is less influenced by outliers:\n\nlibrary(MASS)\nm2 &lt;- rlm(logGDP ~ year1950, data = gp)\nsummary(m2)\n\n\nCall: rlm(formula = logGDP ~ year1950, data = gp)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82123 -0.17437  0.03441  0.17481  0.43542 \n\nCoefficients:\n            Value    Std. Error t value \n(Intercept)   3.7575   0.0264   142.2993\nyear1950      0.0110   0.0008    14.2157\n\nResidual standard error: 0.2592 on 358 degrees of freedom\n\n\nHere I fit splines two ways: using lm and using generalized additive models:\n\nm3 &lt;- lm(logGDP ~ splines::bs(year1950, df = 5, degree=3), data = gp)\nm4 &lt;- mgcv::gam(logGDP ~ s(year1950), data = gp)\n# summary(m3) # this is a bit hard to read, so I don't show it here.\n# summary(m4) # same comment here!\n\nHere is a LOESS smooth:\n\nm5 &lt;- loess(logGDP ~ year1950, data = gp)\n# summary(m5)\n\nAnd finally here is a quantile regression. The quantile predicted by the model is given by tau; we’ve selected the 10 percentile here. lambda is a parameter that\n\nm6 &lt;- quantreg::rq(logGDP ~ year1950, data= gp, \n                     tau = 0.1)\nsummary(m6)\n\nWarning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be\nnonunique\n\n\n\nCall: quantreg::rq(formula = logGDP ~ year1950, tau = 0.1, data = gp)\n\ntau: [1] 0.1\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 3.36938      3.23544  3.47966 \nyear1950    0.00939      0.00692  0.01221 \n\n\n\n16.5.2 Computing and plotting residuals\nAn important visualization for any model is to compare observations with the predictions of the model. This difference is called the residual. (From the residual variation in the data not described by the model.) The function residuals applied to the model object gives you access to these values and makes them easy to plot.\nYou can make histograms of residuals:\n\ntibble(residuals = residuals(m1)) |&gt;\n  ggplot(aes(x = residuals)) + geom_histogram(bins = 20)\n\n\n\n\nUsing bind_cols and bind_rows to combine variables and observations into one large table, you can plot the residuals of several models:\n\nresiduals &lt;- bind_rows(\n  bind_cols(model = \"linear\", residual = residuals(m1)),\n  bind_cols(model = \"spline\", residual = residuals(m3)),\n  bind_cols(model = \"loess\", residual = residuals(m5))\n)\nresiduals |&gt; ggplot(aes(x = residual, fill = model)) +\n  geom_histogram(bins=20)\n\n\n\nresiduals |&gt; ggplot(aes(x = residual)) + \n  geom_histogram(bins=20) + facet_grid(model ~ .)\n\n\n\n\nWe can see that the distribution (histogram) of the residuals is fairly similar for all three models. There are defintely negative residuals larger in magnitude than any of the positive residuals. The modal residuals are bigger than zero. This suggests there are some points where the model “over predicts” (model prediction larger than observed data) and the most common result is an under prediction (model prediction is smaller than observed value.)\nMore commonly you will want to compare the residuals to one of the independent variable, dependent variable, or predicted values. This is easy to do by adding predictions and residuals to the original data.\n\ngp1 &lt;- gp |&gt; mutate(linear_residuals = residuals(m1),\n              linear_predictions = predict(m1))\ngp1 |&gt;  ggplot(aes(x = logGDP, \n                    y = linear_predictions)) + \n  geom_point() +\n  geom_abline(aes(intercept = 0, slope=1))\n\n\n\ngp1 |&gt;  ggplot(aes(x = linear_predictions, \n                    y = linear_residuals)) +\n  geom_point()\n\n\n\n\nNote that models generally don’t make predictions from missing data, so if year or GDP data were missing, for any country or year, there would be some problems with the code above. The easiest solution is to filter out all rows from your data table that have missing data.\nWhen making predictions, you can also generate confidence or prediction intervals to add a measure of uncertainty to your visualization. For plotting purposes you may want to generate a uniform grid along the x-axis and make predictions for these values. Here’s how you can do that. I’ve used summarize to determine the range of the years since 1950 (not shown). You should always be very cautious interpreting predictions, but especially predictions for values outside the observed values in your original data.\n\nnew_data &lt;- tibble(year1950 = seq(from = 2, to = 57, by = 1))\npredictions1 &lt;- predict(m3, new_data, interval = \"prediction\") |&gt; as_tibble()\npredictions2 &lt;- predict(m5, new_data, se=TRUE) |&gt; as_tibble()\nbind_cols(new_data, predictions1) |&gt;\n  ggplot(aes(x = year1950, y = fit)) + \n  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.4) + \n  geom_line(color = \"blue\") +\n  geom_point(data = gp, aes(x = year1950, y = logGDP))\n\n\n\nbind_cols(new_data, predictions2) |&gt;\n  ggplot(aes(x = year1950, y = fit)) + \n  geom_ribbon(aes(ymin = fit -se.fit, ymax = fit + se.fit), fill = \"blue\", alpha = 0.4) + \n  geom_line(color = \"blue\") +\n  geom_point(data = gp, aes(x = year1950, y = logGDP))\n\n\n\n\nThe confidence intervals are a lot narrower than the prediction intervals. The confidence interval gives you a measure of the uncertainty in the mean value of log GDP per capita for each year, while the prediction interval gives you a measure of uncertainty in a prediction for a new observation of log GDP per capita. The prediction of a new observation should be much more uncertain than your knowledge of the mean from all the data.\nA few technical observations. predict does not generate a data frame; it makes a matrix, so we use as_tibble to convert it to a tibble so that variable names work as expected. The predict functions for linear models, GAMs, LOESS, etc are all different functions and have some important differences. Here you can see that the spline can calculate prediction or confidence intervals and the output is the upper and lower limits of the interval. The LOESS predict function only calculates the standard error of the estimated value; this value must be scaled and added to the predicted value to generate an uncertainty estimate."
  },
  {
    "objectID": "lessons/119-linear-models.html#further-reading",
    "href": "lessons/119-linear-models.html#further-reading",
    "title": "16  Linear models",
    "section": "\n16.6 Further reading",
    "text": "16.6 Further reading\n\nHealy Chapter 6 Work with models\n\nDocumentation for equationomatic including instructions for installation."
  },
  {
    "objectID": "lessons/120-gam-loess.html#generalized-additive-models",
    "href": "lessons/120-gam-loess.html#generalized-additive-models",
    "title": "17  GAM and LOESS smoothing",
    "section": "\n17.1 Generalized Additive Models",
    "text": "17.1 Generalized Additive Models\nGeneralized additive models are a kind of linear regression, but instead of finding coefficients of predictor variables (e.g., intercepts, slopes), the model finds a “smooth” response function for each predictor. Typically this means that a piecewise cubic function (spline) is used to approximate the relationship between two variables. We can compute predicted values, confindence and prediction intervals, and show the smooth response function that arose from the model. We don’t provide a simple list of coefficients like we did with linear regression, because the spline curve is defined by many numbers which are not usually informative on their own.\nGeneralized additive models are most commonly used when there is no theoretical motivation for a functional relationship between the variables being studied. We’ll look at the Mauna Loa atmospheric CO2 concentration. These data increase year over year and have well-established interannual oscillations, but there is no clear function for either of these patterns.\nWe will start by using the last decade of data from Lesson 1.\n\n\n\nHere is a plot of atmospheric CO2 concentration over time.\n\nmy_theme = theme_linedraw() + theme(text = element_text(size = 14))\np1 &lt;- co2 |&gt; ggplot(aes(decimal_date, co2_avg)) + geom_point() + my_theme +\n  labs(x = \"Year (decimal)\", y = \"Atmospheric CO2 (ppm)\")\np1\n\n\n\n\nThe next plot shows each observation, minus the annual mean, as a function of the time of year but not the actual year. Each year’s observations are overlapped.\n\np2 &lt;- co2 |&gt; ggplot(aes(year_fraction, co2_anomaly)) + geom_point() + my_theme +\n  labs(x = \"Year fraction (decimal)\", y = \"Atmospheric CO2 anomaly (ppm)\")\np2\n\n\n\n\nHere are GAM fits to both of these pairs of data using the gam function from the mgcv package. There is a plot function in the mgcv package, but I’m using a ggplot function called draw in the gratia package.\n\ng1 &lt;- gam(co2_anomaly ~ s(year_fraction, bs=\"cs\"), data = co2)\ndraw(g1, residuals=TRUE, rug=FALSE)\n\n\n\ng2 &lt;- gam(co2_avg ~ s(decimal_date, bs=\"cs\"),  data = co2)\ndraw(g2, residuals=TRUE, rug=FALSE) \n\n\n\n\nDepending on your goal, you may find that the second plot is too smooth – the interannual oscillations are smoothed out completely. We can increase the number of “knots” (by setting k=25) in the spline to capture the oscillation, but this is not the way the smoothing is normally used:\n\ng3 &lt;- gam(co2_avg ~ s(decimal_date, bs=\"cs\", k = 25),  data = co2)\ndraw(g3, residuals=TRUE, rug=FALSE) \n\n\n\n\nThere is a summary function for GAM fits and the broom functions glance and tidy give access to these data, but the interpretation of this output is beyond the scope of the course:\n\nsummary(g1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nco2_anomaly ~ s(year_fraction, bs = \"cs\")\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 3.353e-15  2.673e-02       0        1\n\nApproximate significance of smooth terms:\n                   edf Ref.df     F p-value    \ns(year_fraction) 8.296      9 530.7  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.976   Deviance explained = 97.8%\nGCV = 0.092244  Scale est. = 0.085038  n = 119\n\nglance(g1) |&gt; kable()\n\n\n\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n9.296045\n-17.36692\n55.32592\n83.9399\n9.328996\n109.704\n119\n\n\n\ntidy(g1) |&gt; kable()\n\n\n\nterm\nedf\nref.df\nstatistic\np.value\n\n\ns(year_fraction)\n8.296045\n9\n530.7004\n0\n\n\n\n\nIt is often useful to compute residuals and extract predicted values.\n\nco2 |&gt; add_residuals(g1) |&gt; add_fitted(g1) |&gt; kable() |&gt;  scroll_box(height = \"200px\")\n\n\n\n\nyear\nmonth\ndecimal_date\nco2_avg\nyear_fraction\nco2_anomaly\n.residual\n.value\n\n\n\n2011\n1\n2011.042\n391.33\n0.0417\n-0.3216667\n0.4287558\n-0.7504224\n\n\n2011\n2\n2011.125\n391.86\n0.1250\n0.2083333\n0.3028663\n-0.0945330\n\n\n2011\n3\n2011.208\n392.60\n0.2083\n0.9483333\n0.1440665\n0.8042668\n\n\n2011\n4\n2011.292\n393.25\n0.2917\n1.5983333\n-0.6010913\n2.1994246\n\n\n2011\n5\n2011.375\n394.19\n0.3750\n2.5383333\n-0.5071835\n3.0455169\n\n\n2011\n6\n2011.458\n393.74\n0.4583\n2.0883333\n-0.1757645\n2.2640978\n\n\n2011\n7\n2011.542\n392.51\n0.5417\n0.8583333\n0.3875896\n0.4707438\n\n\n2011\n8\n2011.625\n390.13\n0.6250\n-1.5216667\n0.0541232\n-1.5757899\n\n\n2011\n9\n2011.708\n389.08\n0.7083\n-2.5716667\n0.3920423\n-2.9637089\n\n\n2011\n10\n2011.792\n388.99\n0.7917\n-2.6616667\n0.0151210\n-2.6767877\n\n\n2011\n11\n2011.875\n390.28\n0.8750\n-1.3716667\n-0.2731113\n-1.0985553\n\n\n2011\n12\n2011.958\n391.86\n0.9583\n0.2083333\n-0.2091638\n0.4174971\n\n\n2012\n1\n2012.042\n393.12\n0.0417\n-0.7350000\n0.0154224\n-0.7504224\n\n\n2012\n2\n2012.125\n393.86\n0.1250\n0.0050000\n0.0995330\n-0.0945330\n\n\n2012\n3\n2012.208\n394.40\n0.2083\n0.5450000\n-0.2592668\n0.8042668\n\n\n2012\n4\n2012.292\n396.18\n0.2917\n2.3250000\n0.1255754\n2.1994246\n\n\n2012\n5\n2012.375\n396.74\n0.3750\n2.8850000\n-0.1605169\n3.0455169\n\n\n2012\n6\n2012.458\n395.71\n0.4583\n1.8550000\n-0.4090978\n2.2640978\n\n\n2012\n7\n2012.542\n394.36\n0.5417\n0.5050000\n0.0342562\n0.4707438\n\n\n2012\n8\n2012.625\n392.39\n0.6250\n-1.4650000\n0.1107899\n-1.5757899\n\n\n2012\n9\n2012.708\n391.13\n0.7083\n-2.7250000\n0.2387089\n-2.9637089\n\n\n2012\n10\n2012.792\n391.05\n0.7917\n-2.8050000\n-0.1282123\n-2.6767877\n\n\n2012\n11\n2012.875\n392.98\n0.8750\n-0.8750000\n0.2235553\n-1.0985553\n\n\n2012\n12\n2012.958\n394.34\n0.9583\n0.4850000\n0.0675029\n0.4174971\n\n\n2013\n1\n2013.042\n395.55\n0.0417\n-0.9700000\n-0.2195776\n-0.7504224\n\n\n2013\n2\n2013.125\n396.80\n0.1250\n0.2800000\n0.3745330\n-0.0945330\n\n\n2013\n3\n2013.208\n397.43\n0.2083\n0.9100000\n0.1057332\n0.8042668\n\n\n2013\n4\n2013.292\n398.41\n0.2917\n1.8900000\n-0.3094246\n2.1994246\n\n\n2013\n5\n2013.375\n399.78\n0.3750\n3.2600000\n0.2144831\n3.0455169\n\n\n2013\n6\n2013.458\n398.60\n0.4583\n2.0800000\n-0.1840978\n2.2640978\n\n\n2013\n7\n2013.542\n397.32\n0.5417\n0.8000000\n0.3292562\n0.4707438\n\n\n2013\n8\n2013.625\n395.20\n0.6250\n-1.3200000\n0.2557899\n-1.5757899\n\n\n2013\n9\n2013.708\n393.45\n0.7083\n-3.0700000\n-0.1062911\n-2.9637089\n\n\n2013\n10\n2013.792\n393.70\n0.7917\n-2.8200000\n-0.1432123\n-2.6767877\n\n\n2013\n11\n2013.875\n395.16\n0.8750\n-1.3600000\n-0.2614447\n-1.0985553\n\n\n2013\n12\n2013.958\n396.84\n0.9583\n0.3200000\n-0.0974971\n0.4174971\n\n\n2014\n1\n2014.042\n397.85\n0.0417\n-0.7925000\n-0.0420776\n-0.7504224\n\n\n2014\n2\n2014.125\n398.01\n0.1250\n-0.6325000\n-0.5379670\n-0.0945330\n\n\n2014\n3\n2014.208\n399.71\n0.2083\n1.0675000\n0.2632332\n0.8042668\n\n\n2014\n4\n2014.292\n401.33\n0.2917\n2.6875000\n0.4880754\n2.1994246\n\n\n2014\n5\n2014.375\n401.78\n0.3750\n3.1375000\n0.0919831\n3.0455169\n\n\n2014\n6\n2014.458\n401.25\n0.4583\n2.6075000\n0.3434022\n2.2640978\n\n\n2014\n7\n2014.542\n399.11\n0.5417\n0.4675000\n-0.0032438\n0.4707438\n\n\n2014\n8\n2014.625\n397.03\n0.6250\n-1.6125000\n-0.0367101\n-1.5757899\n\n\n2014\n9\n2014.708\n395.38\n0.7083\n-3.2625000\n-0.2987911\n-2.9637089\n\n\n2014\n10\n2014.792\n396.07\n0.7917\n-2.5725000\n0.1042877\n-2.6767877\n\n\n2014\n11\n2014.875\n397.28\n0.8750\n-1.3625000\n-0.2639447\n-1.0985553\n\n\n2014\n12\n2014.958\n398.91\n0.9583\n0.2675000\n-0.1499971\n0.4174971\n\n\n2015\n1\n2015.042\n399.98\n0.0417\n-0.8458333\n-0.0954109\n-0.7504224\n\n\n2015\n2\n2015.125\n400.35\n0.1250\n-0.4758333\n-0.3813003\n-0.0945330\n\n\n2015\n3\n2015.208\n401.52\n0.2083\n0.6941667\n-0.1101001\n0.8042668\n\n\n2015\n4\n2015.292\n403.15\n0.2917\n2.3241667\n0.1247421\n2.1994246\n\n\n2015\n5\n2015.375\n403.96\n0.3750\n3.1341667\n0.0886498\n3.0455169\n\n\n2015\n6\n2015.458\n402.80\n0.4583\n1.9741667\n-0.2899311\n2.2640978\n\n\n2015\n7\n2015.542\n401.29\n0.5417\n0.4641667\n-0.0065771\n0.4707438\n\n\n2015\n8\n2015.625\n398.93\n0.6250\n-1.8958333\n-0.3200435\n-1.5757899\n\n\n2015\n9\n2015.708\n397.63\n0.7083\n-3.1958333\n-0.2321244\n-2.9637089\n\n\n2015\n10\n2015.792\n398.29\n0.7917\n-2.5358333\n0.1409543\n-2.6767877\n\n\n2015\n11\n2015.875\n400.16\n0.8750\n-0.6658333\n0.4327220\n-1.0985553\n\n\n2015\n12\n2015.958\n401.85\n0.9583\n1.0241667\n0.6066695\n0.4174971\n\n\n2016\n1\n2016.042\n402.50\n0.0417\n-1.7225000\n-0.9720776\n-0.7504224\n\n\n2016\n2\n2016.125\n404.07\n0.1250\n-0.1525000\n-0.0579670\n-0.0945330\n\n\n2016\n3\n2016.208\n404.87\n0.2083\n0.6475000\n-0.1567668\n0.8042668\n\n\n2016\n4\n2016.292\n407.42\n0.2917\n3.1975000\n0.9980754\n2.1994246\n\n\n2016\n5\n2016.375\n407.72\n0.3750\n3.4975000\n0.4519831\n3.0455169\n\n\n2016\n6\n2016.458\n406.81\n0.4583\n2.5875000\n0.3234022\n2.2640978\n\n\n2016\n7\n2016.542\n404.40\n0.5417\n0.1775000\n-0.2932438\n0.4707438\n\n\n2016\n8\n2016.625\n402.26\n0.6250\n-1.9625000\n-0.3867101\n-1.5757899\n\n\n2016\n9\n2016.708\n401.05\n0.7083\n-3.1725000\n-0.2087911\n-2.9637089\n\n\n2016\n10\n2016.792\n401.60\n0.7917\n-2.6225000\n0.0542877\n-2.6767877\n\n\n2016\n11\n2016.875\n403.53\n0.8750\n-0.6925000\n0.4060553\n-1.0985553\n\n\n2016\n12\n2016.958\n404.44\n0.9583\n0.2175000\n-0.1999971\n0.4174971\n\n\n2017\n1\n2017.042\n406.17\n0.0417\n-0.3850000\n0.3654224\n-0.7504224\n\n\n2017\n2\n2017.125\n406.47\n0.1250\n-0.0850000\n0.0095330\n-0.0945330\n\n\n2017\n3\n2017.208\n407.23\n0.2083\n0.6750000\n-0.1292668\n0.8042668\n\n\n2017\n4\n2017.292\n409.03\n0.2917\n2.4750000\n0.2755754\n2.1994246\n\n\n2017\n5\n2017.375\n409.69\n0.3750\n3.1350000\n0.0894831\n3.0455169\n\n\n2017\n6\n2017.458\n408.89\n0.4583\n2.3350000\n0.0709022\n2.2640978\n\n\n2017\n7\n2017.542\n407.13\n0.5417\n0.5750000\n0.1042562\n0.4707438\n\n\n2017\n8\n2017.625\n405.12\n0.6250\n-1.4350000\n0.1407899\n-1.5757899\n\n\n2017\n9\n2017.708\n403.37\n0.7083\n-3.1850000\n-0.2212911\n-2.9637089\n\n\n2017\n10\n2017.792\n403.63\n0.7917\n-2.9250000\n-0.2482123\n-2.6767877\n\n\n2017\n11\n2017.875\n405.12\n0.8750\n-1.4350000\n-0.3364447\n-1.0985553\n\n\n2017\n12\n2017.958\n406.81\n0.9583\n0.2550000\n-0.1624971\n0.4174971\n\n\n2018\n1\n2018.042\n407.96\n0.0417\n-0.5600000\n0.1904224\n-0.7504224\n\n\n2018\n2\n2018.125\n408.32\n0.1250\n-0.2000000\n-0.1054670\n-0.0945330\n\n\n2018\n3\n2018.208\n409.39\n0.2083\n0.8700000\n0.0657332\n0.8042668\n\n\n2018\n4\n2018.292\n410.25\n0.2917\n1.7300000\n-0.4694246\n2.1994246\n\n\n2018\n5\n2018.375\n411.24\n0.3750\n2.7200000\n-0.3255169\n3.0455169\n\n\n2018\n6\n2018.458\n410.79\n0.4583\n2.2700000\n0.0059022\n2.2640978\n\n\n2018\n7\n2018.542\n408.70\n0.5417\n0.1800000\n-0.2907438\n0.4707438\n\n\n2018\n8\n2018.625\n406.97\n0.6250\n-1.5500000\n0.0257899\n-1.5757899\n\n\n2018\n9\n2018.708\n405.52\n0.7083\n-3.0000000\n-0.0362911\n-2.9637089\n\n\n2018\n10\n2018.792\n406.00\n0.7917\n-2.5200000\n0.1567877\n-2.6767877\n\n\n2018\n11\n2018.875\n408.02\n0.8750\n-0.5000000\n0.5985553\n-1.0985553\n\n\n2018\n12\n2018.958\n409.08\n0.9583\n0.5600000\n0.1425029\n0.4174971\n\n\n2019\n1\n2019.042\n410.83\n0.0417\n-0.6041667\n0.1462558\n-0.7504224\n\n\n2019\n2\n2019.125\n411.75\n0.1250\n0.3158333\n0.4103663\n-0.0945330\n\n\n2019\n3\n2019.208\n411.97\n0.2083\n0.5358333\n-0.2684335\n0.8042668\n\n\n2019\n4\n2019.292\n413.33\n0.2917\n1.8958333\n-0.3035913\n2.1994246\n\n\n2019\n5\n2019.375\n414.64\n0.3750\n3.2058333\n0.1603165\n3.0455169\n\n\n2019\n6\n2019.458\n413.93\n0.4583\n2.4958333\n0.2317355\n2.2640978\n\n\n2019\n7\n2019.542\n411.74\n0.5417\n0.3058333\n-0.1649104\n0.4707438\n\n\n2019\n8\n2019.625\n409.95\n0.6250\n-1.4841667\n0.0916232\n-1.5757899\n\n\n2019\n9\n2019.708\n408.54\n0.7083\n-2.8941667\n0.0695423\n-2.9637089\n\n\n2019\n10\n2019.792\n408.52\n0.7917\n-2.9141667\n-0.2373790\n-2.6767877\n\n\n2019\n11\n2019.875\n410.25\n0.8750\n-1.1841667\n-0.0856113\n-1.0985553\n\n\n2019\n12\n2019.958\n411.76\n0.9583\n0.3258333\n-0.0916638\n0.4174971\n\n\n2020\n1\n2020.042\n413.39\n0.0417\n-0.6154545\n0.1349679\n-0.7504224\n\n\n2020\n2\n2020.125\n414.11\n0.1250\n0.1045455\n0.1990785\n-0.0945330\n\n\n2020\n3\n2020.208\n414.51\n0.2083\n0.5045455\n-0.2997213\n0.8042668\n\n\n2020\n4\n2020.292\n416.21\n0.2917\n2.2045455\n0.0051209\n2.1994246\n\n\n2020\n5\n2020.375\n417.07\n0.3750\n3.0645455\n0.0190286\n3.0455169\n\n\n2020\n6\n2020.458\n416.38\n0.4583\n2.3745455\n0.1104477\n2.2640978\n\n\n2020\n7\n2020.542\n414.38\n0.5417\n0.3745455\n-0.0961983\n0.4707438\n\n\n2020\n8\n2020.625\n412.55\n0.6250\n-1.4554545\n0.1203353\n-1.5757899\n\n\n2020\n9\n2020.708\n411.29\n0.7083\n-2.7154545\n0.2482544\n-2.9637089\n\n\n2020\n10\n2020.792\n411.28\n0.7917\n-2.7254545\n-0.0486669\n-2.6767877\n\n\n2020\n11\n2020.875\n412.89\n0.8750\n-1.1154545\n-0.0168992\n-1.0985553\n\n\n\n\n\n\n\nIf we generate new data (dates), we can plot predictions too. Since the model is a piecewise cubic function, extrapolations are often dramatically unreliable. The downward facing cubic in the last “bump” is simply continued, with comically bad results. Extrapolation of models, and especially smooths, is somewhere between risky and foolish!\n\nnew_data &lt;- tibble(decimal_date = seq(2017, 2022, by = 0.05))\nnew_data |&gt; add_fitted(g3) |&gt;\n  ggplot(aes(decimal_date, .value)) + \n  geom_line(color = \"blue\", size = 2) +\n  geom_point(aes(y= co2_avg), data = co2) + my_theme + xlim(2017, 2022)\n\n\n\n\nIf you want confidence intervals on the fitted values, use the confint function together with the name of the smooth you are extracting. Be aware that this function does not include the intercept (or grand mean) from the model, so the values are all centred on zero.\n\nconfint(g1, \"s(year_fraction)\", level = 0.95) |&gt; kable() |&gt;  scroll_box(height = \"200px\")\n\n\n\n\nsmooth\ntype\nby\nyear_fraction\nest\nse\ncrit\nlower\nupper\n\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0417000\n-0.7504224\n0.0911641\n1.959964\n-0.9291007\n-0.5717441\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0509586\n-0.6817731\n0.0809576\n1.959964\n-0.8404472\n-0.5230990\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0602172\n-0.6128047\n0.0733925\n1.959964\n-0.7566513\n-0.4689581\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0694758\n-0.5431982\n0.0690818\n1.959964\n-0.6785960\n-0.4078004\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0787343\n-0.4726346\n0.0681204\n1.959964\n-0.6061481\n-0.3391211\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0879929\n-0.4007948\n0.0699211\n1.959964\n-0.5378376\n-0.2637519\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.0972515\n-0.3273597\n0.0734371\n1.959964\n-0.4712938\n-0.1834257\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1065101\n-0.2520105\n0.0775478\n1.959964\n-0.4040013\n-0.1000197\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1157687\n-0.1744279\n0.0812996\n1.959964\n-0.3337721\n-0.0150836\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1250273\n-0.0942929\n0.0839688\n1.959964\n-0.2588687\n0.0702829\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1342859\n-0.0112865\n0.0850507\n1.959964\n-0.1779829\n0.1554099\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1435444\n0.0749103\n0.0842570\n1.959964\n-0.0902303\n0.2400509\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1528030\n0.1646477\n0.0816674\n1.959964\n0.0045826\n0.3247128\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1620616\n0.2583992\n0.0780514\n1.959964\n0.1054213\n0.4113771\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1713202\n0.3566685\n0.0744216\n1.959964\n0.2108048\n0.5025323\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1805788\n0.4599596\n0.0717896\n1.959964\n0.3192547\n0.6006645\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1898374\n0.5687763\n0.0709348\n1.959964\n0.4297468\n0.7078059\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.1990960\n0.6836225\n0.0721337\n1.959964\n0.5422429\n0.8250020\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2083545\n0.8050020\n0.0750446\n1.959964\n0.6579174\n0.9520866\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2176131\n0.9334187\n0.0788581\n1.959964\n0.7788597\n1.0879777\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2268717\n1.0693765\n0.0825778\n1.959964\n0.9075269\n1.2312261\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2361303\n1.2133792\n0.0852429\n1.959964\n1.0463061\n1.3804523\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2453889\n1.3659307\n0.0860566\n1.959964\n1.1972628\n1.5345985\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2546475\n1.5269277\n0.0846482\n1.959964\n1.3610204\n1.6928350\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2639061\n1.6938561\n0.0816106\n1.959964\n1.5339022\n1.8538101\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2731646\n1.8636034\n0.0779236\n1.959964\n1.7108759\n2.0163308\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2824232\n2.0330568\n0.0746644\n1.959964\n1.8867173\n2.1793963\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.2916818\n2.1991039\n0.0728081\n1.959964\n2.0564026\n2.3418053\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3009404\n2.3586321\n0.0729366\n1.959964\n2.2156789\n2.5015852\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3101990\n2.5085287\n0.0750029\n1.959964\n2.3615256\n2.6555317\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3194576\n2.6456811\n0.0783537\n1.959964\n2.4921106\n2.7992516\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3287162\n2.7669769\n0.0819889\n1.959964\n2.6062816\n2.9276722\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3379747\n2.8693034\n0.0848476\n1.959964\n2.7030052\n3.0356015\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3472333\n2.9495480\n0.0859986\n1.959964\n2.7809937\n3.1181022\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3564919\n3.0052962\n0.0849386\n1.959964\n2.8388196\n3.1717728\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3657505\n3.0369263\n0.0821713\n1.959964\n2.8758735\n3.1979791\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3750091\n3.0455143\n0.0786208\n1.959964\n2.8914204\n3.1996083\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3842677\n3.0321367\n0.0753298\n1.959964\n2.8844929\n3.1797804\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.3935263\n2.9978695\n0.0732707\n1.959964\n2.8542615\n3.1414775\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4027848\n2.9437891\n0.0730697\n1.959964\n2.8005751\n3.0870030\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4120434\n2.8709716\n0.0747628\n1.959964\n2.7244393\n3.0175039\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4213020\n2.7804934\n0.0777794\n1.959964\n2.6280486\n2.9329383\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4305606\n2.6734307\n0.0811703\n1.959964\n2.5143399\n2.8325215\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4398192\n2.5508597\n0.0838920\n1.959964\n2.3864344\n2.7152849\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4490778\n2.4138566\n0.0850124\n1.959964\n2.2472353\n2.5804779\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4583364\n2.2634823\n0.0840211\n1.959964\n2.0988041\n2.4281606\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4675949\n2.1007371\n0.0814055\n1.959964\n1.9411853\n2.2602890\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4768535\n1.9266061\n0.0780703\n1.959964\n1.7735912\n2.0796210\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4861121\n1.7420745\n0.0750318\n1.959964\n1.5950148\n1.8891341\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.4953707\n1.5481273\n0.0732232\n1.959964\n1.4046125\n1.6916421\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5046293\n1.3457498\n0.0732233\n1.959964\n1.2022348\n1.4892649\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5138879\n1.1359272\n0.0750321\n1.959964\n0.9888669\n1.2829875\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5231465\n0.9196446\n0.0780707\n1.959964\n0.7666287\n1.0726604\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5324051\n0.6978871\n0.0814060\n1.959964\n0.5383343\n0.8574399\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5416636\n0.4716399\n0.0840214\n1.959964\n0.3069609\n0.6363189\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5509222\n0.2418882\n0.0850126\n1.959964\n0.0752666\n0.4085099\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5601808\n0.0096723\n0.0838920\n1.959964\n-0.1547530\n0.1740976\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5694394\n-0.2237447\n0.0811702\n1.959964\n-0.3828354\n-0.0646541\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5786980\n-0.4570436\n0.0777794\n1.959964\n-0.6094884\n-0.3045988\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5879566\n-0.6889047\n0.0747631\n1.959964\n-0.8354377\n-0.5423718\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.5972152\n-0.9180088\n0.0730706\n1.959964\n-1.0612245\n-0.7747931\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6064737\n-1.1430364\n0.0732722\n1.959964\n-1.2866474\n-0.9994254\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6157323\n-1.3626681\n0.0753319\n1.959964\n-1.5103159\n-1.2150204\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6249909\n-1.5755845\n0.0786231\n1.959964\n-1.7296830\n-1.4214861\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6342495\n-1.7804663\n0.0821735\n1.959964\n-1.9415234\n-1.6194091\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6435081\n-1.9759939\n0.0849404\n1.959964\n-2.1424740\n-1.8095138\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6527667\n-2.1608480\n0.0859997\n1.959964\n-2.3294043\n-1.9922916\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6620253\n-2.3336718\n0.0848479\n1.959964\n-2.4999706\n-2.1673729\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6712838\n-2.4929588\n0.0819889\n1.959964\n-2.6536541\n-2.3322636\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6805424\n-2.6371655\n0.0783541\n1.959964\n-2.7907368\n-2.4835942\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6898010\n-2.7647478\n0.0750048\n1.959964\n-2.9117546\n-2.6177411\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.6990596\n-2.8741622\n0.0729410\n1.959964\n-3.0171239\n-2.7312005\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7083182\n-2.9638648\n0.0728152\n1.959964\n-3.1065800\n-2.8211495\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7175768\n-3.0323118\n0.0746736\n1.959964\n-3.1786694\n-2.8859541\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7268354\n-3.0779594\n0.0779336\n1.959964\n-3.2307065\n-2.9252124\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7360939\n-3.0992640\n0.0816197\n1.959964\n-3.2592356\n-2.9392923\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7453525\n-3.0946816\n0.0846547\n1.959964\n-3.2606017\n-2.9287615\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7546111\n-3.0626686\n0.0860598\n1.959964\n-3.2313426\n-2.8939946\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7638697\n-3.0024297\n0.0852433\n1.959964\n-3.1695036\n-2.8353559\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7731283\n-2.9161857\n0.0825785\n1.959964\n-3.0780367\n-2.7543348\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7823869\n-2.8069168\n0.0788644\n1.959964\n-2.9614882\n-2.6523454\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.7916455\n-2.6776030\n0.0750628\n1.959964\n-2.8247235\n-2.5304825\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8009040\n-2.5312246\n0.0721691\n1.959964\n-2.6726735\n-2.3897757\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8101626\n-2.3707617\n0.0709880\n1.959964\n-2.5098957\n-2.2316278\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8194212\n-2.1991946\n0.0718550\n1.959964\n-2.3400278\n-2.0583613\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8286798\n-2.0195033\n0.0744885\n1.959964\n-2.1654980\n-1.8735085\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8379384\n-1.8346680\n0.0781073\n1.959964\n-1.9877556\n-1.6815804\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8471970\n-1.6476689\n0.0817030\n1.959964\n-1.8078037\n-1.4875341\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8564556\n-1.4614862\n0.0842697\n1.959964\n-1.6266517\n-1.2963207\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8657141\n-1.2785516\n0.0850508\n1.959964\n-1.4452480\n-1.1118552\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8749727\n-1.0990792\n0.0839868\n1.959964\n-1.2636903\n-0.9344681\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8842313\n-0.9227229\n0.0813945\n1.959964\n-1.0822532\n-0.7631926\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.8934899\n-0.7491364\n0.0778159\n1.959964\n-0.9016528\n-0.5966200\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9027485\n-0.5779735\n0.0740203\n1.959964\n-0.7230506\n-0.4328964\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9120071\n-0.4088880\n0.0710024\n1.959964\n-0.5480501\n-0.2697259\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9212657\n-0.2415336\n0.0698892\n1.959964\n-0.3785140\n-0.1045532\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9305242\n-0.0755641\n0.0716662\n1.959964\n-0.2160273\n0.0648991\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9397828\n0.0893667\n0.0768070\n1.959964\n-0.0611722\n0.2399055\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9490414\n0.2536050\n0.0851237\n1.959964\n0.0867656\n0.4204444\n\n\ns(year_fraction)\nCRS (shrink)\nNA\n0.9583000\n0.4174971\n0.0959746\n1.959964\n0.2293904\n0.6056038\n\n\n\n\n\n\nconfint(g1, \"s(year_fraction)\", level = 0.95) |&gt; \n  ggplot(aes(year_fraction)) + \n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.25) + \n  geom_line(aes(y = est)) + my_theme\n\n\n\n\nYou can also use lm to fit splines; these are similar to GAMs but there are some important differences. The mgcv package has a lot of features not available with lm.\n\ns1 &lt;- lm(co2_avg ~ splines::bs(decimal_date, df = 5), data = co2)\ntidy(s1) |&gt; kable(digits = 2)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n391.73\n1.13\n347.53\n0.00\n\n\nsplines::bs(decimal_date, df = 5)1\n0.64\n2.16\n0.30\n0.77\n\n\nsplines::bs(decimal_date, df = 5)2\n6.87\n1.47\n4.69\n0.00\n\n\nsplines::bs(decimal_date, df = 5)3\n14.49\n1.98\n7.31\n0.00\n\n\nsplines::bs(decimal_date, df = 5)4\n21.01\n1.62\n13.01\n0.00\n\n\nsplines::bs(decimal_date, df = 5)5\n22.14\n1.64\n13.50\n0.00\n\n\n\n\nglance(s1) |&gt; kable(digits = 2)\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n0.91\n0.9\n2.31\n219.14\n0\n5\n-265.4\n544.8\n564.26\n602.91\n113\n119\n\n\n\n\nWe can use augment to generate data to plot and combine it with the original data.\n\na1 &lt;- augment(s1, data = co2, se_fit = TRUE, interval=\"prediction\") \na1 |&gt;  ggplot(aes(decimal_date)) + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.2) + \n  geom_line(aes(y= .fitted)) +\n  geom_point(aes(y = co2_avg)) + \n  my_theme\n\n\n\n\nFor a spline that closely traces the data, increase df to 26 or more. This increases the numbers of knots or separate cubics used to approximate the data."
  },
  {
    "objectID": "lessons/120-gam-loess.html#locally-estimated-scatterplot-smoothing-loess",
    "href": "lessons/120-gam-loess.html#locally-estimated-scatterplot-smoothing-loess",
    "title": "17  GAM and LOESS smoothing",
    "section": "\n17.2 Locally Estimated Scatterplot Smoothing (LOESS)",
    "text": "17.2 Locally Estimated Scatterplot Smoothing (LOESS)\nLOESS smooths are constructed by making a large number of quadratic (or possibly linear) regression lines as a window moves along the x-axis. The degree of the fits and the width of the window (and other details) can be adjusted. The predictions from these many local regressions are then computed and plotted as a “smooth” of the data. We usually just imagine the line is drawn through the data in a way that allows it to track fluctuations without specifying a model.\n\nl1 &lt;- loess(co2_anomaly ~ year_fraction, data = co2)\n\nAs with the GAMs, the summary function is not particularly easy to interpret and we will not explore the details, but you can see that this is a quadratic model with a “span” of 0.75, meaning that 75% of the data are used for each regression. (The weighting of each point in the regression varies as a function of x, resulting a continuous change in the predictions.)\n\nsummary(l1)\n\nCall:\nloess(formula = co2_anomaly ~ year_fraction, data = co2)\n\nNumber of Observations: 119 \nEquivalent Number of Parameters: 4.58 \nResidual Standard Error: 0.4647 \nTrace of smoother matrix: 5  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\n\nWe can use predict, residuals and augment on these model objects as we did for lm fits in the previous lesson.\n\naugment(l1, se_fit = TRUE) |&gt; kable() |&gt;  scroll_box(height = \"200px\")\n\n\n\n\nco2_anomaly\nyear_fraction\n.fitted\n.se.fit\n.resid\n\n\n\n-0.3216667\n0.0417\n-1.1874816\n0.1278142\n0.8658149\n\n\n0.2083333\n0.1250\n0.3297748\n0.0783764\n-0.1214414\n\n\n0.9483333\n0.2083\n1.4858927\n0.0756092\n-0.5375594\n\n\n1.5983333\n0.2917\n2.2386031\n0.0811237\n-0.6402698\n\n\n2.5383333\n0.3750\n2.7453971\n0.0876098\n-0.2070637\n\n\n2.0883333\n0.4583\n2.1417191\n0.0876044\n-0.0533857\n\n\n0.8583333\n0.5417\n0.4302506\n0.0876044\n0.4280828\n\n\n-1.5216667\n0.6250\n-1.5283681\n0.0876098\n0.0067014\n\n\n-2.5716667\n0.7083\n-2.4561226\n0.0811840\n-0.1155441\n\n\n-2.6616667\n0.7917\n-2.3873488\n0.0757368\n-0.2743178\n\n\n-1.3716667\n0.8750\n-1.4172800\n0.0803002\n0.0456133\n\n\n0.2083333\n0.9583\n0.4905675\n0.1329762\n-0.2822341\n\n\n-0.7350000\n0.0417\n-1.1874816\n0.1278142\n0.4524816\n\n\n0.0050000\n0.1250\n0.3297748\n0.0783764\n-0.3247748\n\n\n0.5450000\n0.2083\n1.4858927\n0.0756092\n-0.9408927\n\n\n2.3250000\n0.2917\n2.2386031\n0.0811237\n0.0863969\n\n\n2.8850000\n0.3750\n2.7453971\n0.0876098\n0.1396029\n\n\n1.8550000\n0.4583\n2.1417191\n0.0876044\n-0.2867191\n\n\n0.5050000\n0.5417\n0.4302506\n0.0876044\n0.0747494\n\n\n-1.4650000\n0.6250\n-1.5283681\n0.0876098\n0.0633681\n\n\n-2.7250000\n0.7083\n-2.4561226\n0.0811840\n-0.2688774\n\n\n-2.8050000\n0.7917\n-2.3873488\n0.0757368\n-0.4176512\n\n\n-0.8750000\n0.8750\n-1.4172800\n0.0803002\n0.5422800\n\n\n0.4850000\n0.9583\n0.4905675\n0.1329762\n-0.0055675\n\n\n-0.9700000\n0.0417\n-1.1874816\n0.1278142\n0.2174816\n\n\n0.2800000\n0.1250\n0.3297748\n0.0783764\n-0.0497748\n\n\n0.9100000\n0.2083\n1.4858927\n0.0756092\n-0.5758927\n\n\n1.8900000\n0.2917\n2.2386031\n0.0811237\n-0.3486031\n\n\n3.2600000\n0.3750\n2.7453971\n0.0876098\n0.5146029\n\n\n2.0800000\n0.4583\n2.1417191\n0.0876044\n-0.0617191\n\n\n0.8000000\n0.5417\n0.4302506\n0.0876044\n0.3697494\n\n\n-1.3200000\n0.6250\n-1.5283681\n0.0876098\n0.2083681\n\n\n-3.0700000\n0.7083\n-2.4561226\n0.0811840\n-0.6138774\n\n\n-2.8200000\n0.7917\n-2.3873488\n0.0757368\n-0.4326512\n\n\n-1.3600000\n0.8750\n-1.4172800\n0.0803002\n0.0572800\n\n\n0.3200000\n0.9583\n0.4905675\n0.1329762\n-0.1705675\n\n\n-0.7925000\n0.0417\n-1.1874816\n0.1278142\n0.3949816\n\n\n-0.6325000\n0.1250\n0.3297748\n0.0783764\n-0.9622748\n\n\n1.0675000\n0.2083\n1.4858927\n0.0756092\n-0.4183927\n\n\n2.6875000\n0.2917\n2.2386031\n0.0811237\n0.4488969\n\n\n3.1375000\n0.3750\n2.7453971\n0.0876098\n0.3921029\n\n\n2.6075000\n0.4583\n2.1417191\n0.0876044\n0.4657809\n\n\n0.4675000\n0.5417\n0.4302506\n0.0876044\n0.0372494\n\n\n-1.6125000\n0.6250\n-1.5283681\n0.0876098\n-0.0841319\n\n\n-3.2625000\n0.7083\n-2.4561226\n0.0811840\n-0.8063774\n\n\n-2.5725000\n0.7917\n-2.3873488\n0.0757368\n-0.1851512\n\n\n-1.3625000\n0.8750\n-1.4172800\n0.0803002\n0.0547800\n\n\n0.2675000\n0.9583\n0.4905675\n0.1329762\n-0.2230675\n\n\n-0.8458333\n0.0417\n-1.1874816\n0.1278142\n0.3416483\n\n\n-0.4758333\n0.1250\n0.3297748\n0.0783764\n-0.8056081\n\n\n0.6941667\n0.2083\n1.4858927\n0.0756092\n-0.7917261\n\n\n2.3241667\n0.2917\n2.2386031\n0.0811237\n0.0855636\n\n\n3.1341667\n0.3750\n2.7453971\n0.0876098\n0.3887696\n\n\n1.9741667\n0.4583\n2.1417191\n0.0876044\n-0.1675524\n\n\n0.4641667\n0.5417\n0.4302506\n0.0876044\n0.0339161\n\n\n-1.8958333\n0.6250\n-1.5283681\n0.0876098\n-0.3674653\n\n\n-3.1958333\n0.7083\n-2.4561226\n0.0811840\n-0.7397107\n\n\n-2.5358333\n0.7917\n-2.3873488\n0.0757368\n-0.1484845\n\n\n-0.6658333\n0.8750\n-1.4172800\n0.0803002\n0.7514466\n\n\n1.0241667\n0.9583\n0.4905675\n0.1329762\n0.5335992\n\n\n-1.7225000\n0.0417\n-1.1874816\n0.1278142\n-0.5350184\n\n\n-0.1525000\n0.1250\n0.3297748\n0.0783764\n-0.4822748\n\n\n0.6475000\n0.2083\n1.4858927\n0.0756092\n-0.8383927\n\n\n3.1975000\n0.2917\n2.2386031\n0.0811237\n0.9588969\n\n\n3.4975000\n0.3750\n2.7453971\n0.0876098\n0.7521029\n\n\n2.5875000\n0.4583\n2.1417191\n0.0876044\n0.4457809\n\n\n0.1775000\n0.5417\n0.4302506\n0.0876044\n-0.2527506\n\n\n-1.9625000\n0.6250\n-1.5283681\n0.0876098\n-0.4341319\n\n\n-3.1725000\n0.7083\n-2.4561226\n0.0811840\n-0.7163774\n\n\n-2.6225000\n0.7917\n-2.3873488\n0.0757368\n-0.2351512\n\n\n-0.6925000\n0.8750\n-1.4172800\n0.0803002\n0.7247800\n\n\n0.2175000\n0.9583\n0.4905675\n0.1329762\n-0.2730675\n\n\n-0.3850000\n0.0417\n-1.1874816\n0.1278142\n0.8024816\n\n\n-0.0850000\n0.1250\n0.3297748\n0.0783764\n-0.4147748\n\n\n0.6750000\n0.2083\n1.4858927\n0.0756092\n-0.8108927\n\n\n2.4750000\n0.2917\n2.2386031\n0.0811237\n0.2363969\n\n\n3.1350000\n0.3750\n2.7453971\n0.0876098\n0.3896029\n\n\n2.3350000\n0.4583\n2.1417191\n0.0876044\n0.1932809\n\n\n0.5750000\n0.5417\n0.4302506\n0.0876044\n0.1447494\n\n\n-1.4350000\n0.6250\n-1.5283681\n0.0876098\n0.0933681\n\n\n-3.1850000\n0.7083\n-2.4561226\n0.0811840\n-0.7288774\n\n\n-2.9250000\n0.7917\n-2.3873488\n0.0757368\n-0.5376512\n\n\n-1.4350000\n0.8750\n-1.4172800\n0.0803002\n-0.0177200\n\n\n0.2550000\n0.9583\n0.4905675\n0.1329762\n-0.2355675\n\n\n-0.5600000\n0.0417\n-1.1874816\n0.1278142\n0.6274816\n\n\n-0.2000000\n0.1250\n0.3297748\n0.0783764\n-0.5297748\n\n\n0.8700000\n0.2083\n1.4858927\n0.0756092\n-0.6158927\n\n\n1.7300000\n0.2917\n2.2386031\n0.0811237\n-0.5086031\n\n\n2.7200000\n0.3750\n2.7453971\n0.0876098\n-0.0253971\n\n\n2.2700000\n0.4583\n2.1417191\n0.0876044\n0.1282809\n\n\n0.1800000\n0.5417\n0.4302506\n0.0876044\n-0.2502506\n\n\n-1.5500000\n0.6250\n-1.5283681\n0.0876098\n-0.0216319\n\n\n-3.0000000\n0.7083\n-2.4561226\n0.0811840\n-0.5438774\n\n\n-2.5200000\n0.7917\n-2.3873488\n0.0757368\n-0.1326512\n\n\n-0.5000000\n0.8750\n-1.4172800\n0.0803002\n0.9172800\n\n\n0.5600000\n0.9583\n0.4905675\n0.1329762\n0.0694325\n\n\n-0.6041667\n0.0417\n-1.1874816\n0.1278142\n0.5833149\n\n\n0.3158333\n0.1250\n0.3297748\n0.0783764\n-0.0139414\n\n\n0.5358333\n0.2083\n1.4858927\n0.0756092\n-0.9500594\n\n\n1.8958333\n0.2917\n2.2386031\n0.0811237\n-0.3427698\n\n\n3.2058333\n0.3750\n2.7453971\n0.0876098\n0.4604363\n\n\n2.4958333\n0.4583\n2.1417191\n0.0876044\n0.3541143\n\n\n0.3058333\n0.5417\n0.4302506\n0.0876044\n-0.1244172\n\n\n-1.4841667\n0.6250\n-1.5283681\n0.0876098\n0.0442014\n\n\n-2.8941667\n0.7083\n-2.4561226\n0.0811840\n-0.4380441\n\n\n-2.9141667\n0.7917\n-2.3873488\n0.0757368\n-0.5268178\n\n\n-1.1841667\n0.8750\n-1.4172800\n0.0803002\n0.2331133\n\n\n0.3258333\n0.9583\n0.4905675\n0.1329762\n-0.1647341\n\n\n-0.6154545\n0.0417\n-1.1874816\n0.1278142\n0.5720271\n\n\n0.1045455\n0.1250\n0.3297748\n0.0783764\n-0.2252293\n\n\n0.5045455\n0.2083\n1.4858927\n0.0756092\n-0.9813473\n\n\n2.2045455\n0.2917\n2.2386031\n0.0811237\n-0.0340576\n\n\n3.0645455\n0.3750\n2.7453971\n0.0876098\n0.3191484\n\n\n2.3745455\n0.4583\n2.1417191\n0.0876044\n0.2328264\n\n\n0.3745455\n0.5417\n0.4302506\n0.0876044\n-0.0557051\n\n\n-1.4554545\n0.6250\n-1.5283681\n0.0876098\n0.0729135\n\n\n-2.7154545\n0.7083\n-2.4561226\n0.0811840\n-0.2593319\n\n\n-2.7254545\n0.7917\n-2.3873488\n0.0757368\n-0.3381057\n\n\n-1.1154545\n0.8750\n-1.4172800\n0.0803002\n0.3018254\n\n\n\n\n\n\naugment(l1, se_fit = TRUE) |&gt;\n  ggplot(aes(x = year_fraction, y = .fitted)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = .fitted - 2*.se.fit, ymax = .fitted + 2*.se.fit), alpha = 0.20) + \n  my_theme\n\n\n\n\nThis line is only drawn using 12 points, so we might want to generate a smoother prediction by creating a new set of dates. We also add the original data to the plot.\n\nnew_data = tibble(year_fraction = seq(min(co2$year_fraction), \n                                      max(co2$year_fraction),\n                                      length = 100))\naugment(l1, newdata = new_data, se_fit = TRUE) |&gt;\n  ggplot(aes(x = year_fraction, y = .fitted)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = .fitted - 2*.se.fit,\n                  ymax = .fitted + 2*.se.fit), \n              alpha = 0.20) + \n  geom_point(aes(y = co2_anomaly), data = co2) +\n  my_theme\n\n\n\n\nThe LOESS function will not predict outside of the range of data provided, so I had to select the range of new_data above carefully to get the line to start and end near the first and last points.\nNow we make two plots that contrast the results with different sized windows (span) and degree of the local regression models.\n\nl2 &lt;- loess(co2_avg ~ decimal_date, data = co2, \n            degree = 1, span = 0.05)\nl3 &lt;- loess(co2_avg ~ decimal_date, data = co2, \n            degree = 1, span = 0.25)\nnew_data = tibble(decimal_date = seq(min(co2$decimal_date), \n                                      max(co2$decimal_date),\n                                     length = 300))\na2 &lt;- augment(l2, newdata = new_data, se_fit = TRUE) \na3 &lt;- augment(l3, newdata = new_data, se_fit = TRUE) \nggplot(a2, aes(x = decimal_date, y = .fitted)) + \n  geom_line(col=\"green\") + \n  geom_ribbon(aes(ymin = .fitted - 2*.se.fit, \n                  ymax = .fitted + 2*.se.fit), \n              alpha = 0.20, fill=\"green\") + \n  geom_point(aes(y = co2_avg), data = co2) +\n  geom_line(data = a3, \n            col=\"blue\") + \n  geom_ribbon(data = a3, \n              aes(ymin = .fitted - 2*.se.fit, \n                  ymax = .fitted + 2*.se.fit), \n              alpha = 0.20, fill=\"blue\") + \n  my_theme\n\n\n\n\nLOESS fits are slow and take a lot of memory compared to other methods (both time and storage requirements increase like the square of the number of points), so they are usually only used for small data sets."
  },
  {
    "objectID": "lessons/120-gam-loess.html#further-reading",
    "href": "lessons/120-gam-loess.html#further-reading",
    "title": "17  GAM and LOESS smoothing",
    "section": "\n17.3 Further reading",
    "text": "17.3 Further reading\n\nLOESS (Wikipedia)\nGeneralized additive models (Wikipedia)"
  },
  {
    "objectID": "lessons/124-collaboration.html#the-basics",
    "href": "lessons/124-collaboration.html#the-basics",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.1 The basics",
    "text": "18.1 The basics\nFor the collaborative project, I will create a repository for each team and give each team member write access to the repository. You should start, as always by creating (cloning) the project onto your computer from GitHub.\nThere are now at least three copies of the repository: the remote on GitHub, a copy on your computer, and a copy on your teammate’s computer. These are all independent of each other. When you edit a file, you make changes to files on your computer in the “working directory”. Eventually you stage and commit your changes to your local repository. You then need to sychronize your changes with GitHub and, eventually, your teammate needs to get your changes too."
  },
  {
    "objectID": "lessons/124-collaboration.html#push-and-pull",
    "href": "lessons/124-collaboration.html#push-and-pull",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.2 Push and pull",
    "text": "18.2 Push and pull\nGetting changes from GitHub is called “pulling” (down arrow in the Rstudio git panel). Sending your committed changes to GitHub is called “pushing” (up arrow in the git panel.)\nI suggest when you sit down to work that you start by pulling changes from GitHub. Then do your work. Stage and commit your changes. Finally, before you stop work for the day, push your changes to GitHub."
  },
  {
    "objectID": "lessons/124-collaboration.html#changes-that-dont-conflict",
    "href": "lessons/124-collaboration.html#changes-that-dont-conflict",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.3 Changes that don’t conflict",
    "text": "18.3 Changes that don’t conflict\nIf person A edits file 1.rmd and person B edits file 2.rmd, then they both commit their edits and push them to GitHub, there will be no conflict with changes made by the other person. They both need to pull the changes made by their teammate to have a complete version.]\nIf your teammate has pushed changes to GitHub, then you try to push changes without first getting their version, you will get an error message from git, displayed in a pop-up window:\n\n\n\n\n\n\n\n\nThe way to resolve this problem is to close the message box, then pull the remote version from GitHub. There are two likely outcomes here:\n\nYou are told that there are no conflicts\nYou are told there are unresolvable conflicts that you need to address\n\nIn the situation I’ve described above, the changes are not conflicts, so git will merge the changes. You still need to push your changes to GitHub by clicking the up arrow."
  },
  {
    "objectID": "lessons/124-collaboration.html#resolving-conflicts-in-the-repository",
    "href": "lessons/124-collaboration.html#resolving-conflicts-in-the-repository",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.4 Resolving conflicts (in the repository)",
    "text": "18.4 Resolving conflicts (in the repository)\nIf person A and person B both edit the same file “at the same time” (meaning, before the other person has had a chance to push the edits to GitHub so you can pull the edits to your computer before you begin to work) then the second person to push changes to that file may get a message that there is a conflict.\nGit records changes line by line to a file (see the line numbers in the left margin of the Rstudio editing window.) If all the edits made by person A are on different lines from the edits made by person B, then git will automatically merge the two versions.\nIf both people edit the same line, then git will not know how to resolve the conflicts. When you pull the remote version to your computer, the file will be edited to show the conflicting changes, like this:\n\n\n\n\n\n\n\n\nYou need to edit the file to fix the problems. Git doesn’t supervise you here, so there are no outside rules. Just read both versions, decide what you want, and delete anything you don’t want. Then stage, commit, and push your changes.\nNote, the “conflict” resolution doesn’t help you and your partner if you actually disagree about something! That is not a problem that can be solved by software. Git simply helps you manage editing the same file and knowing if your proposed changed disagree."
  },
  {
    "objectID": "lessons/124-collaboration.html#reviewing-diffs",
    "href": "lessons/124-collaboration.html#reviewing-diffs",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.5 Reviewing diffs",
    "text": "18.5 Reviewing diffs\nWhen you sit down to work on your project, you should always start by pulling the remote version to your computer. This will make sure you have your partner’s latest work. If you see the message “Already up to date.” you know your partner didn’t make changes since you worked on the files. If you see some other message, git will tell you what files were changed. You can review these changes using the “diff” tool in Rstudio’s git pane.\nOpen up the “diff” pane and click on History (button in the upper left). By clicking on a commit message in the top panel, you can see the changes made in files. The bottom panel will show the changes, highlighting new lines in green and deleted lines in red. Differences (“diffs”) are recorded one line at a time, so if the change was an edit, you will usually see pairs of red and green lines with the old version in red and the new version in green. If the text is new, with no old version, you will see just green. If lines were deleted and not replaced with anything, you will see just red lines."
  },
  {
    "objectID": "lessons/124-collaboration.html#working-directly-through-the-github-website",
    "href": "lessons/124-collaboration.html#working-directly-through-the-github-website",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.6 Working directly through the GitHub website",
    "text": "18.6 Working directly through the GitHub website\nI find the workflow described easy to use and I strongly suggest it because it lets you use all the features of Rstudio and the collaboration tools of GitHub. There are other ways to interact through GitHub and I’ll describe some of these briefly.\n\n18.6.1 You can edit files directly through the GitHub website\nFind the file you want to edit. Click on the link so you see the contents of the file. (This works with .Rmd files, but not some more complicated files.) Click the pencil (“edit”) icon in the upper right area of the window where your file is shown. Make your edits. Scroll to the bottom of the window and click “commit” to the main branch.\nThese edits are particularly handy if you are trying to edit on a tablet or someone else’s computer.\n\n18.6.2 You can upload or download files directly to your repository through the website.\nIn the main window for the project, beside the bright green ‘Code’ button, click “Add file”. Here you can upload a new file to the repository."
  },
  {
    "objectID": "lessons/124-collaboration.html#further-reading",
    "href": "lessons/124-collaboration.html#further-reading",
    "title": "18  Collaborating with GitHub",
    "section": "\n18.7 Further reading",
    "text": "18.7 Further reading\nThe exercises in the first link are very helpful, although they are written using the git command line tutorial instead of the buttons in Rstudio.\n\n\nCollaboration tutorial from University of Toronto, including our case of two people working together without a team lead managing changes\nSoftware Carpentry lessons on git and github\n\nUsing github for collaboration on a software package (the MIT general circulation model)\n\ncompareThis is an add-in for Rstudio to help you visualize and resolve merge conflicts.\nUsing git with Rstudio"
  },
  {
    "objectID": "lessons/301-data-sources.html#finding-data",
    "href": "lessons/301-data-sources.html#finding-data",
    "title": "19  Finding and accessing data",
    "section": "\n19.1 Finding data",
    "text": "19.1 Finding data\n\n19.1.1 Data in R packages\nMany datasets are available as part of R packages. These are the easiest to use, but they are often small and designed for demonstration purposes. For example, the gapminder package only contains a small portion of the data available on the gapminder website.\nThese are the go-to datasets that we have used for demonstrating many simple visualizations:\n\nmtcars and many other well-known data in datasets package\npenguins in palmerpenguins package\ngapminder in gapminder package (but see website too Gapminder)\ndiamonds, mpg, economics, midwest, msleep in ggplot2 package\nnycflights13 in dbplyr package\ngss_sm, gss_cat, gss_sib, gss_lon and gss_lon in socviz package\n\nThe function datasets in the package datasets.load will display a list of all datasets in R packages you have installed on your computer. There is not much information about each dataset, but you once you know their names, you can read the help page for each dataset.\n\n19.1.2 Tidy Tuesday\nTidy Tuesday is a project to encourage people to develop skills at data visualization and analysis using the tidyverse packages in R. Each week a new data set is posted and participants experiment to create new visualzations. Some people share their work on GitHub and social media.\n\nThe Tidy Tuesday website describes the project and has a catalog of available datasets from previous weeks.\nThe R package to access the data is called tidytuesdayR. Examples of using the package are here\n\n\n19.1.3 R packages for accessing data\nThere are many packages designed primarily to provide access to large collections of data. Here are a few examples.\n\n\nOECD for data from the OECD\n\n\ncansim for data from Statistics Canada\n\n\ncancensus for data from the Canadian census and National household survey. You need to create an account and get an API key to use this package. The package documentation shows you how.\n\n19.1.4 Websites with data collections\nNaturally there are websites that curate lists of data available from other sites. Here are a few I’ve found useful.\n\n\nOur World In Data is a curated set of over 3000 charts with documentation and open source data.\nThe awesome public datasets project collects high quality public data, organized thematically.\nThis is one person’s collection of data of various sources\n\nr-dir has a list of freely available datasets\nWorld Bank economic development indicators\n\n\nCORGIS the collection of really great, interesting, situated datasets\n\n19.1.5 Canadian COVID data\nMany countries and other organizations have developed collections of COVID-related data. Here are some sources for Canada.\n\n\nFederal, BC, AB, SK, MB, ON, QC, NB, PE, NS,"
  },
  {
    "objectID": "lessons/301-data-sources.html#examples-of-accessing-data",
    "href": "lessons/301-data-sources.html#examples-of-accessing-data",
    "title": "19  Finding and accessing data",
    "section": "\n19.2 Examples of accessing data",
    "text": "19.2 Examples of accessing data\n\n19.2.1 Gapminder\nGapminder distributes many datasets, some of their own, and some collected by other organizations that they are redistributing. They have a webpage to help you search, browse and access the data. Using this webpage I have found data on population of countries by years from 1800-2100 (with many missing data, some interpolated data, and of course many years of projections). Here is some R code to read and work with this data. There are data on 195 countries over 302 years. I have selected five countries to make a simple plot.\n\npop &lt;- read_csv(\"static/population_total.csv\")\n\nRows: 195 Columns: 302\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): country\ndbl (301): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npop %&gt;% filter(country %in% c(\"Canada\", \"China\", \"Chile\", \"Germany\", \"United States\")) %&gt;%\n  pivot_longer(`1800`:`2100`, names_to = \"years\", values_to = \"population\") %&gt;%\n  mutate(years = as.numeric(years)) %&gt;%\n  ggplot(aes(x = years, \n             y = population, \n             color = fct_reorder(country, population, max, .desc=TRUE))) +\n  geom_line(size=2) +\n  scale_y_log10(labels = trans_format(\"log10\", math_format(10^.x))) +\n  labs(color = \"Country\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n19.2.2 Consumer price index in Canada\nOne measure of inflation is its effect on prices. Here is a table from Statistics Canada that reports over 1 million rows of data on this measure. Statistics Canada tables are an extreme version of “long” data, which usually require a lot of filtering to get just the rows you want. You will want to make use of dpylr functions summarize and count to study the structure of the data.\n\n# Population and projection in a huge table with 2 million rows, 300 MB of data\n# cansim::get_cansim(\"17100057\")  \ncpi &lt;- cansim::get_cansim(\"18-10-0004-01\")\n\nAccessing CANSIM NDM product 18-10-0004 from Statistics Canada\n\n\nParsing data\n\ncpi %&gt;% filter(`Products and product groups` == \"All-items\") %&gt;%\n  count(GEO)\n\n# A tibble: 30 × 2\n   GEO                                                    n\n   &lt;chr&gt;                                              &lt;int&gt;\n 1 Alberta                                              543\n 2 British Columbia                                     543\n 3 Calgary, Alberta                                     635\n 4 Canada                                              1319\n 5 Charlottetown and Summerside, Prince Edward Island   599\n 6 Edmonton, Alberta                                    635\n 7 Halifax, Nova Scotia                                 635\n 8 Iqaluit, Nunavut                                     252\n 9 Manitoba                                             543\n10 Montréal, Quebec                                     635\n# ℹ 20 more rows\n\n\nLet’s look at these data for Canada as a whole.\n\ncpi %&gt;% filter(`Products and product groups` == \"All-items\",\n               GEO == \"Canada\") %&gt;%\n  select(REF_DATE, VALUE) %&gt;%\n  mutate(date = lubridate::ym(REF_DATE)) %&gt;%\n  ggplot(aes(x = date, y = VALUE)) +\n  geom_line() + \n  labs(x = \"Date\", y = \"All items CPI\",\n       title = \"Consumer price index in Canada\",\n       caption = \"Scaled so that 2002 = 100\") + \n  scale_y_log10()\n\n\n\n\n\n19.2.3 Causes of death worldwide\nOur world in data has many compliations of data across many countries. Here is a table listing causes of death over time and countries. There is a link to a csv file on that page that can be easily read with R. As is often the case, this is a large table (nearly 7000 observations of 37 variables). You will need to explore the data a bit to understand it. Here is one simple plot that can be made.\n\ndeath &lt;- read_csv(\"static/annual-number-of-deaths-by-cause.csv\", guess_max = 10000)\n\nRows: 6686 Columns: 37\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Entity, Code, Number of executions (Amnesty International)\ndbl (34): Year, Deaths - Road injuries - Sex: Both - Age: All Ages (Number),...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndeath %&gt;% filter(Entity %in% c(\"Canada\", \"China\", \"Germany\", \"United States\"),\n                 Year == 2015) %&gt;%\n  select(Entity, `Deaths - Road injuries - Sex: Both - Age: All Ages (Number)`:`Terrorism (deaths)`) %&gt;%\n  pivot_longer(`Deaths - Road injuries - Sex: Both - Age: All Ages (Number)`:`Terrorism (deaths)`,\n               names_to = \"Cause\", values_to = \"Deaths\") %&gt;%\n  mutate(Cause = str_remove(Cause, \"Deaths - \") %&gt;% \n           str_remove(\" - Sex: Both - Age: All Ages \\\\(Number\\\\)\")) %&gt;%\n  ggplot(aes(x = Deaths, y = fct_reorder(Cause, Deaths), \n             color = Entity)) +\n  geom_point() +\n  scale_x_log10()\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\nOf course, it would probably be more interesting to plot these as per capita deaths; or at least, that would enable a different sort of comparison."
  },
  {
    "objectID": "lessons/301-data-sources.html#other-r-packages-related-to-data-collection",
    "href": "lessons/301-data-sources.html#other-r-packages-related-to-data-collection",
    "title": "19  Finding and accessing data",
    "section": "\n19.3 Other R packages related to data collection",
    "text": "19.3 Other R packages related to data collection\nSometimes the only way you will find data is as a table on a website or in a document. The datapasta package is useful for some common reformatting tasks that are required after copy-and-paste.\nSometimes data you want are only available on a graph – just points or lines and no numeric data at all. There are a variety of “data thief” tools for extracting quantitative data from these images, for example, the web app WebPlotDigitizer."
  },
  {
    "objectID": "lessons/301-data-sources.html#describing-data",
    "href": "lessons/301-data-sources.html#describing-data",
    "title": "19  Finding and accessing data",
    "section": "\n19.4 Describing data",
    "text": "19.4 Describing data\nOnce you have done work to find data, you will also want to do some research to learn the “5 Ws” of data. In addition to what the variables are and what each observation represents, you will want to know who collected it, when, why, and how. It’s a good idea to write a “readme” to summarize what you learn. For your term project you will be asked to provide some information on the datasets you analyze, but you should also be aware that there are online guides that provide advice on documenting data."
  },
  {
    "objectID": "lessons/301-data-sources.html#distribution-of-data",
    "href": "lessons/301-data-sources.html#distribution-of-data",
    "title": "19  Finding and accessing data",
    "section": "\n19.5 Distribution of data",
    "text": "19.5 Distribution of data\nIf you want to distribute an analysis of data or redistribute the original data, please be sure to respect the terms of use of the data. Many people encourage the use of FAIR data usage principles."
  },
  {
    "objectID": "lessons/301-data-sources.html#further-reading",
    "href": "lessons/301-data-sources.html#further-reading",
    "title": "19  Finding and accessing data",
    "section": "\n19.6 Further reading",
    "text": "19.6 Further reading\n\n\nImporting data from R4DS\nRoger Peng’s notes on importing data\n\nAn older but comprehensive guide to importing data"
  },
  {
    "objectID": "lessons/911-ethics.html#accessibility",
    "href": "lessons/911-ethics.html#accessibility",
    "title": "Accessibility, Bias, and Ethics",
    "section": "Accessibility",
    "text": "Accessibility\nData visualization is, as the name implies, the act of producing a product to be seen. This is a useful goal because our brains are very good at processing some kinds of visual information. Training to read visualizations can greatly increase the ability to extract information from a visualization, so it is important to know your audience – students, the general public, people with well developed quantiative skills, or domain experts for the data you are presenting. All of these factors are central to knowing if a visualization is suitable and effective. Our target audience is university students.\nNot everyone has the same visual abilities. Some people have vision that differs from the most common experience in some way – perceiving colours differently, reduced acuity, and other differences all the way to complete blindness. We should always keep these differences in mind when producing visualizations. To take the hardest challenge head on, what is the value in producing a visualization for someone who cannot see it? Data visualization is a process that uses the creator’s visual and technical skills to present features of a dataset. Any data visualization should be accompanied by a written description of the message to be conveyed. Ideally the visual and written aspects will complement each other. A visualization does not “say” anything by itself; a written interpretation is an essential part of the process."
  },
  {
    "objectID": "lessons/911-ethics.html#data-collection-and-analysis",
    "href": "lessons/911-ethics.html#data-collection-and-analysis",
    "title": "Accessibility, Bias, and Ethics",
    "section": "Data collection and analysis",
    "text": "Data collection and analysis\nData collection and analysis are critical tools for understanding and interacting with the world. Data are used by academic researchers, goverments, corportations, non-profit organizations, and citizens in complex and contrasting ways. All of these processes create opportunities for bias and discrimination. The links below give a few examples and stories elaborating these challenges.\n\n\nData encodes systematic racism from the MIT technology review, December 2020.\n\nCase studies in data ethics from O’Reilly publishers.\n\nData and the COVID pandemic, opinion published in Patterns, July 2020.\nA business and marketing take on the ethics of data science\n\nA student’s perspective on ethics in data science from 2018.\nA professional statement on ethical data science from the Royal statistical society and the Institute and Faculty of Actuaries\n\n\nThe following resources are in the form of checklists or questions to think about when collecting, analyzing, and presenting data.\n\nData science ethics checklist\n10 data science ethics questions\n\nIf you find discussions of these topics you find particularly thought provoking or informative, please share them with me."
  },
  {
    "objectID": "lessons/911-ethics.html#further-reading",
    "href": "lessons/911-ethics.html#further-reading",
    "title": "Accessibility, Bias, and Ethics",
    "section": "Further reading",
    "text": "Further reading\n\nData Science in a Box notes on ethics"
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#what-is-reproducibility-and-why-should-i-care",
    "href": "lessons/133-reproducible-reports.html#what-is-reproducibility-and-why-should-i-care",
    "title": "20  Reproducible reports",
    "section": "\n20.1 What is reproducibility and why should I care?",
    "text": "20.1 What is reproducibility and why should I care?\nMany people use R as an interactive computer tool. This means they sit at the keyboard, type a series of commands, and store the results they want. It’s like a very fancy calculator. This is a fantastic way to explore data, learn R, and test out ideas quickly. This work is not reproducible. You would need a recording of everything that was done to reproduce the analysis. If you need to change a small step in your work, you will need to repeat all the steps.\nIn this course, we have been using R in a slightly more disciplined way, at least for assigned work. For every project, you create a new R markdown file, and then you write your computer instructions in code “chunks”, interspersed with English (or any other natural human language) explanations. With Rstudio, you can still use R interactively in this mode, clicking the “play” button on each chunk and seeing the output. When your analysis is done, you can “knit” the whole document. This does two important things:\n\nall the code is executed in order from top to bottom, and\nthe results are kept in a new file.\n\nYou may have had the experience during the course of preparing your R markdown file, trying to knit it, and discovering an error. This error is evidence that your R markdown document is not complete, so if you rely on it, you will not be able to repeat your steps later on. So the first way R markdown aids you in reproducing your work is by giving you an easy way to test if your instructions are complete. If you can knit your file today, you can turn off your computer, and come back in a week, and be confident you will be able to reproduce your work then.\nWe have stressed the value of communication in this course. For some purposes you will just want to communicate by sending someone your knitted report. When you work in a team, communication is not just about the knitted report, but it includes the instructions needed to recreate the report. R markdown is great for this. But most analyses will require more than just one R markdown file. They will require data. This is why we have also learned to use R projects, version control, and to share files using GitHub. Now you can write a document to show someone else how to reproduce your work, from gathering data to making a report, and you can be confident that they will be able to make all the steps work and even add to your work in a time-efficient way."
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#what-are-barriers-to-reproducibility-and-how-i-can-overcome-them",
    "href": "lessons/133-reproducible-reports.html#what-are-barriers-to-reproducibility-and-how-i-can-overcome-them",
    "title": "20  Reproducible reports",
    "section": "\n20.2 What are barriers to reproducibility and how I can overcome them?",
    "text": "20.2 What are barriers to reproducibility and how I can overcome them?\n\n20.2.1 File paths\nIf you have a complex project, you will have multiple directories, and possibly many data files. When reading a data file, you will need to refer to the file by name. Make sure you never use the way files are organized on your computer as part of the script. In particular never write anything like:\nmy_data &lt;- read_csv(\"/Users/airwin/Documents/Stats-project/data/my_data.csv\")\nbecause there is no way that will work on someone else’s computer! If you reorganize your Documents folder, it won’t even work on your computer! What should you do instead?\nWe have been using R packages and organizing all files for a project in a folder (and sub-folders) created just for that project. This is a good first step. The here package provides a useful function that allows you refer to a file relative to the directory where your .Rproj file is stored. This is important to make your R code work on someone else’s computer. Here’s how you use it:\nmy_data &lt;- read_csv(here(\"static\", \"annual-number-of-deaths-by-cause.csv\"))\nIf this code works on your computer, with your R project, and you give the whole folder to a collaborator, you don’t need to worry how they organize their files.\nSometimes you will think a dataset is too large to put in your project folder. Or the data may be used by multiple projects, and you don’t want to have multiple copies. What should you do? The best options are: put the common data files in a GitHub repository (public or private), deposit the data in an online repository such as osf.io, or create an R data package of your own to manage the data on your computer and the computers of your collaborators. Which method you prefer will depend on many factors including the size of the data, how often it changes, whether it is public, or if you are allowed to redistribute it.\n\n20.2.2 Caching time-consuming computations\nR markdown is a great way to manage a analysis notebook, but every time you want to update your analysis, you need to recalculate everything in your document. It is possible to store (cache) the results of a code chunk by giving it a name and setting cache=TRUE. This will store the results of that computation so that when the markdown file is recomputed, the stored results are used. This can save time, but also lead to unpredictable results depending on how the results of one code chunk and external data influence the cached code chunk.\nHere is an example using the code chunk header {r test_caching, cache=TRUE}:\n\ns1 &lt;- summarize(mpg, count = n()) \ns1\n\n# A tibble: 1 × 1\n  count\n  &lt;int&gt;\n1   234\n\n\nIf you are going to use this feature, you should read more about caching in Rmarkdown. This page describes a do-it-yourself caching method which I recommend as well.\n\n20.2.3 Finding and installing R packages\nR packages can be obtained from several sources. The most common sources are CRAN, Bioconductor, and GitHub. The most widely used packages are on CRAN and they are checked regularly to be sure they still work. Anyone can distribute a package on GitHub and make it available, but these packages are missing a level of quality control. We have not used Bioconductor in this course; it is a CRAN-like repository forcussed on bioinformatics computations.\nInstalling a new package from CRAN is easy. If you put the appropriate library function call in your R markdown document, but you are missing the package, Rstudio will offer to download it for you. If the package is on GitHub, Rstudio won’t be able to help. The usual process is to search for the package on Google, then install it. You will help yourself and your colleagues if you write the installation command next to your library function call, but preface it with a comment character # to stop the code from being executed. (There is an example at the top of the source for this file: click on “view source” in the lower right of this window.)"
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#make-your-report-look-good",
    "href": "lessons/133-reproducible-reports.html#make-your-report-look-good",
    "title": "20  Reproducible reports",
    "section": "\n20.3 Make your report look good",
    "text": "20.3 Make your report look good\nAn easy to read report is a better report. (Formatting will not turn a bad report into a good one, but good formatting can help make good results easier to digest.)\nSpecific recommendations:\n\nUse headings (lines starting with one or more #.) Always put a blank line before and after a heading row.\nUse bulletted or numbered lists (start a seqeunce of lines with a * or 1.) when appropriate\nFormat your code nicely. Use a style guide. Or use an automatic code formatter such as styler (see below). Leave a blank line before and after your code chunk blocks.\nFormat your output nicely. This includes tables and figures. (See below.)\nknit your report, read the formatted version, revise the markdown source, and repeat.\n\nFor examples of these formatting tips, see the source for this file: click on “view source” in the lower right of this window.\nHeadings make your markdown document easier to navigate too. Look for the “show outline” button in the upper right of Rstudio’s editing pane."
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#code-formatting",
    "href": "lessons/133-reproducible-reports.html#code-formatting",
    "title": "20  Reproducible reports",
    "section": "\n20.4 Code formatting",
    "text": "20.4 Code formatting\nThere is a tidyverse style guide. Google has their own, revised from the tidyverse guide. It’s worthwhile reading these guides. Code that is formatted in a standard way is easier for you to read and easier for someone else to read.\nHere is some carelessly formatted code.\ngapminder |&gt; filter(continent ==\"Europe\") |&gt; group_by(country) |&gt; summarize(mean_life_exp = mean(lifeExp), mean_GDP = mean(gdpPercap)) |&gt; ggplot(\naes(color = mean_life_exp, x = log10(mean_GDP), y=fct_reorder(country, mean_life_exp))) + geom_point()\nHere is the same code (and its output) formatted using styler:\n\ngapminder |&gt;\n  filter(continent == \"Europe\") |&gt;\n  group_by(country) |&gt;\n  summarize(mean_life_exp = mean(lifeExp), mean_GDP = mean(gdpPercap)) |&gt;\n  ggplot(aes(color = mean_life_exp, x = log10(mean_GDP), y = fct_reorder(country, mean_life_exp))) +\n  geom_point()\n\n\n\n\nI suggest you write neat and easy to read code in your markdown documents. Don’t use automatic tidying as an excuse to make messy code."
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#customization-of-result-reporting-in-r-markdown",
    "href": "lessons/133-reproducible-reports.html#customization-of-result-reporting-in-r-markdown",
    "title": "20  Reproducible reports",
    "section": "\n20.5 Customization of result reporting in R markdown",
    "text": "20.5 Customization of result reporting in R markdown\nThe first line of a code chunk can be as simple as {r} but you can also include many options between the braces. The markdown book describes the options available. Here I will demonstrate a few chunk options related to figures and formatting output.\nKnitting a document usually stops when an error is encountered. This is a safety measure to alert you to a document which is not reproducible becuase of errors. On rare occasions you may want to use the option error=TRUE to allow error messages to appear in knitted output and not stop the knitting process.\n\n1 + \"A\"\n\nError in 1 + \"A\": non-numeric argument to binary operator\n\n\nYou can output graphics in multiple file formats by adding dev = c(\"png\", \"pdf\") (and other formats) to the chunk options. These files will be deleted unless you reqest they be kept, which is easily done by caching the results of at least one code chunk.\n\nmpg |&gt; ggplot(aes(displ, hwy)) + geom_point()\n\n\n\n\nYou can use chunk options to control whether the knitted document includes:\n\ncode (echo=FALSE to hide)\nresults (results='hide')\nmessages (message=FALSE to hide)\nwarnings (warning=FALSE to hide)\nplots (fig.show='hide')\neverything (include=FALSE)\n\nYou can also stop the code from being evaluated by setting eval=FALSE.\nIf you have multiple lines of code with output in your chunk, the knitted document will contain several blocks of code and output with space between the blocks. These blocks can be combined into one by setting collapse=TRUE.\nR code that is formatted in standardized way is easier for others to read. You can get your code automatically reformatted using tidy=TRUE and the formatR package. The styler package is another approach to automatic reformatting of R code. Use tidy='styler' in the code chunk options.\nYou can change the size of a figure using fig.width = 6 and fig.height=4 where 6 and 4 are lengths in inches. You can also use out.width=\"85%\" to set the width of the figure as a proportion of the document width. You can center a figure horizontally using fig.align='center'.\nYou can use R variables and code in the chunk options."
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#documenting-which-packages-you-use",
    "href": "lessons/133-reproducible-reports.html#documenting-which-packages-you-use",
    "title": "20  Reproducible reports",
    "section": "\n20.6 Documenting which packages you use",
    "text": "20.6 Documenting which packages you use\nIf you use R for long enough and with enough other people, you will discover that R packages get revised and don’t always work the same way as they used to. This can be a major impediment to reproducibility. The simplest solution to this problem is to document the R packages you use in your analysis by adding a line of code to the end of your report that lists the packages and their version numbers in use. By looking at your knitted output, a user having trouble (possibly you in the future!) can look to see which packages have changed.\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Halifax\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] styler_1.10.2   gapminder_1.0.0 here_1.0.1      report_0.5.8   \n [5] lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.2    \n [9] purrr_1.0.2     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n[13] ggplot2_3.4.4   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.3        generics_0.1.3    stringi_1.7.12    hms_1.1.3        \n [5] digest_0.6.33     magrittr_2.0.3    evaluate_0.21     grid_4.3.1       \n [9] timechange_0.2.0  fastmap_1.1.1     R.oo_1.25.0       R.cache_0.16.0   \n[13] rprojroot_2.0.3   jsonlite_1.8.7    R.utils_2.12.3    fansi_1.0.4      \n[17] scales_1.2.1      cli_3.6.1         rlang_1.1.1       R.methodsS3_1.8.2\n[21] munsell_0.5.0     withr_2.5.0       tools_4.3.1       tzdb_0.4.0       \n[25] colorspace_2.1-0  vctrs_0.6.3       R6_2.5.1          lifecycle_1.0.3  \n[29] htmlwidgets_1.6.2 insight_0.19.7    pkgconfig_2.0.3   pillar_1.9.0     \n[33] gtable_0.3.3      glue_1.6.2        xfun_0.40         tidyselect_1.2.0 \n[37] rstudioapi_0.15.0 knitr_1.43        farver_2.1.1      htmltools_0.5.6  \n[41] labeling_0.4.2    rmarkdown_2.23    compiler_4.3.1   \n\n\nIf you want to produce bibliographic citations for your packages you can use the report package:\nreport::cite_packages()  \n\nBryan J (2023). gapminder: Data from Gapminder. R package version 1.0.0, https://CRAN.R-project.org/package=gapminder.\nGrolemund G, Wickham H (2011). “Dates and Times Made Easy with lubridate.” Journal of Statistical Software, 40(3), 1-25. https://www.jstatsoft.org/v40/i03/.\nMakowski D, Lüdecke D, Patil I, Thériault R, Ben-Shachar M, Wiernik B (2023). “Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.” CRAN. https://easystats.github.io/report/.\nMüller K (2020). here: A Simpler Way to Find Your Files. R package version 1.0.1, https://CRAN.R-project.org/package=here.\nMüller K, Walthert L (2023). styler: Non-Invasive Pretty Printing of R Code. R package version 1.10.2, https://CRAN.R-project.org/package=styler.\nMüller K, Wickham H (2023). tibble: Simple Data Frames. R package version 3.2.1, https://CRAN.R-project.org/package=tibble.\nR Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.\nWickham H (2022). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.0, https://CRAN.R-project.org/package=stringr.\nWickham H (2023). forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0, https://CRAN.R-project.org/package=forcats.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686.\nWickham H, François R, Henry L, Müller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr.\nWickham H, Henry L (2023). purrr: Functional Programming Tools. R package version 1.0.2, https://CRAN.R-project.org/package=purrr.\nWickham H, Hester J, Bryan J (2023). readr: Read Rectangular Text Data. R package version 2.1.4, https://CRAN.R-project.org/package=readr.\nWickham H, Vaughan D, Girlich M (2023). tidyr: Tidy Messy Data. R package version 1.3.0, https://CRAN.R-project.org/package=tidyr.\n\n# See also report::report(sessionInfo())\nTo absolutely guarantee you can use R code in the future, some applications will benefit from you keeping copies of all the required packages on your own computer system. The packrat and checkpoint packages can help you manage packages. I have never felt the need to have this level of reproducibility, but I find it reassuring to know these tools exist."
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#using-other-languages",
    "href": "lessons/133-reproducible-reports.html#using-other-languages",
    "title": "20  Reproducible reports",
    "section": "\n20.7 Using other languages",
    "text": "20.7 Using other languages\nQuarto is a new publishing tool being developed based on the ideas of R markdown. Quarto is designed to work much like R markdown, but with extra features for publishing books, websites, and presentation slides. Quarto allows you to combine several different programming languages, including python and julia in the same document. For more information see the Quarto website. Quarto is already built-in to R and your Rmd documents work fine with Quarto, so if you wanto to learn more about this tool it should be easy to start.\nRelated to the topic of inter-operation of computing languages, but departing from the topic of R markdown, you can also write C++ code in an R session and execute the compiled code directly from R.\nSQL is a language for describing queries to a widely used style of database. R contains tools for interacting with SQL databases, but it can aslo generate SQL code from dplyr functions.\n\nlibrary(dbplyr)\nlibrary(RSQLite)\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\ncopy_to(con, palmerpenguins::penguins, \"penguins\")\npenguins &lt;- tbl(con, \"penguins\")\npenguins_aggr &lt;-\n  penguins |&gt;\n  group_by(species) |&gt;\n  summarize(\n    N = n(),\n    across(ends_with(\"mm\"), sum, .names = \"TOT_{.col}\"),\n    across(ends_with(\"mm\"), mean, .names = \"AVG_{.col}\"),\n  )\npenguins_aggr\n\n# Source:   SQL [3 x 8]\n# Database: sqlite 3.44.2 [:memory:]\n  species       N TOT_bill_length_mm TOT_bill_depth_mm TOT_flipper_length_mm\n  &lt;chr&gt;     &lt;int&gt;              &lt;dbl&gt;             &lt;dbl&gt;                 &lt;int&gt;\n1 Adelie      152              5858.             2770.                 28683\n2 Chinstrap    68              3321.             1253.                 13316\n3 Gentoo      124              5843.             1843.                 26714\n# ℹ 3 more variables: AVG_bill_length_mm &lt;dbl&gt;, AVG_bill_depth_mm &lt;dbl&gt;,\n#   AVG_flipper_length_mm &lt;dbl&gt;\n\ncapture.output(show_query(penguins_aggr))\n\n [1] \"&lt;SQL&gt;\"                                                 \n [2] \"SELECT\"                                                \n [3] \"  `species`,\"                                          \n [4] \"  COUNT(*) AS `N`,\"                                    \n [5] \"  SUM(`bill_length_mm`) AS `TOT_bill_length_mm`,\"      \n [6] \"  SUM(`bill_depth_mm`) AS `TOT_bill_depth_mm`,\"        \n [7] \"  SUM(`flipper_length_mm`) AS `TOT_flipper_length_mm`,\"\n [8] \"  AVG(`bill_length_mm`) AS `AVG_bill_length_mm`,\"      \n [9] \"  AVG(`bill_depth_mm`) AS `AVG_bill_depth_mm`,\"        \n[10] \"  AVG(`flipper_length_mm`) AS `AVG_flipper_length_mm`\" \n[11] \"FROM `penguins`\"                                       \n[12] \"GROUP BY `species`\"                                    \n\nrm(con)\n\nWhat does using other tools have to do with making your work reproducible? R markdown is a flexible tool that lets you use more than just R, so if your workflow contains steps external to R, they can sometimes still be included in your report."
  },
  {
    "objectID": "lessons/133-reproducible-reports.html#further-reading",
    "href": "lessons/133-reproducible-reports.html#further-reading",
    "title": "20  Reproducible reports",
    "section": "\n20.8 Further reading",
    "text": "20.8 Further reading\n\n\nProject workflow from R4DS\nGenerating SQL with dbplyr\nPeng. Reproducible research in computational science. (Peng 2011)\n\nLeVeque et al. Reproducible research for scientific computing\n\nSandve et al. Ten simple rules for reproducible computational research. (Sandve et al. 2013)\n\n\n\n\n\n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27. https://doi.org/10.1126/science.1213847.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig. 2013. “Ten Simple Rules for Reproducible Computational Research.” Edited by Philip E. Bourne. PLoS Computational Biology 9 (10): e1003285. https://doi.org/10.1371/journal.pcbi.1003285."
  },
  {
    "objectID": "lessons/121-PCA.html#example",
    "href": "lessons/121-PCA.html#example",
    "title": "21  Principal component analysis",
    "section": "\n21.1 Example",
    "text": "21.1 Example\nA simple example in two dimensions is really helpful. No one does PCA on two variables, because you can just plot the data in a normal scatterplot, but as a demonstration it shows how the principal components are chosen.\nFirst, let’s look at a regular scatterplot of two variables that have a reasonably strong linear relationship. I’ve used coord_fixed to force the same scale on the vertical and horizontal axes, to make this plot easier to compare with the next plot we will draw.\n\ncars %&gt;% ggplot(aes(x=speed, y=dist)) + geom_point() + coord_fixed()\n\n\n\n\nNow let’s perform the PCA. There are three results: the amount of variation accounted for by each principal component, the directions of the principal components along each of the original axes, and the coordinates of the observations along the principal component axes.\n\npca1 &lt;- cars %&gt;% prcomp()\n\nThe tidy functions let you obtain\n\nthe percent of the total variance projected along (“explained by”) each principal component (determined by the eigenvalues)\nthe directions of the original axes along the new principal component axes (rotation)\nthe original data transformed to the new principal component axes (scores)\n\n\npca1 %&gt;% tidy(matrix = \"eigenvalues\") %&gt;% kable()\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n1\n26.12524\n0.98628\n0.98628\n\n\n2\n3.08084\n0.01372\n1.00000\n\n\n\n\npca1 %&gt;% tidy(matrix = \"rotation\") %&gt;% kable()\n\n\n\ncolumn\nPC\nvalue\n\n\n\nspeed\n1\n-0.1656479\n\n\nspeed\n2\n-0.9861850\n\n\ndist\n1\n-0.9861850\n\n\ndist\n2\n0.1656479\n\n\n\n\npca1 %&gt;% tidy(matrix = \"scores\") %&gt;% \n  pivot_wider(names_from = \"PC\", names_prefix = \"PC_\", values_from = \"value\") %&gt;% \n  kable() %&gt;% scroll_box(height = 50)\n\n\n\n\nrow\nPC_1\nPC_2\n\n\n\n1\n42.3022457\n4.4542578\n\n\n2\n34.4127660\n5.7794410\n\n\n3\n39.8329321\n1.8269987\n\n\n4\n22.0816028\n4.8086608\n\n\n5\n27.8330646\n2.8285885\n\n\n6\n33.5845265\n0.8485162\n\n\n7\n25.5293989\n1.1875144\n\n\n8\n17.6399193\n2.5126975\n\n\n9\n9.7504396\n3.8378807\n\n\n10\n26.3499360\n0.0356815\n\n\n11\n15.5019014\n1.8578084\n\n\n12\n29.1428430\n-1.4474471\n\n\n13\n23.2257332\n-0.4535598\n\n\n14\n19.2809934\n0.2090318\n\n\n15\n15.3362535\n0.8716234\n\n\n16\n17.1429756\n-0.4458574\n\n\n17\n9.2534959\n0.8793258\n\n\n18\n9.2534959\n0.8793258\n\n\n19\n-2.5807236\n2.8671005\n\n\n20\n16.9773277\n-1.4320423\n\n\n21\n7.1154781\n0.2244366\n\n\n22\n-16.5529610\n4.1999861\n\n\n23\n-36.2766602\n7.5129440\n\n\n24\n22.7287895\n-3.4121147\n\n\n25\n16.8116798\n-2.4182273\n\n\n26\n-10.8014991\n2.2199138\n\n\n27\n10.7289221\n-2.4105249\n\n\n28\n2.8394424\n-1.0853417\n\n\n29\n10.5632742\n-3.3967098\n\n\n30\n2.6737945\n-2.0715267\n\n\n31\n-7.1880550\n-0.4150477\n\n\n32\n0.5357767\n-2.7264158\n\n\n33\n-13.2708127\n-0.4073453\n\n\n34\n-32.9945119\n2.9056126\n\n\n35\n-40.8839916\n4.2307958\n\n\n36\n6.2872386\n-4.7064882\n\n\n37\n-3.5746110\n-3.0500092\n\n\n38\n-25.2706801\n0.5942445\n\n\n39\n10.0663305\n-6.3552647\n\n\n40\n-5.7126288\n-3.7048984\n\n\n41\n-9.6573687\n-3.0423068\n\n\n42\n-13.6021085\n-2.3797152\n\n\n43\n-21.4915882\n-1.0545321\n\n\n44\n-23.7952539\n-2.6956062\n\n\n45\n-12.1266823\n-5.6695659\n\n\n46\n-28.0712895\n-4.0053845\n\n\n47\n-49.7673586\n-0.3611308\n\n\n48\n-50.7535436\n-0.1954829\n\n\n49\n-77.3805375\n4.2770102\n\n\n50\n-43.0297118\n-2.5068510\n\n\n\n\n\n\n\nWe can perform these calculations “by hand” following the linear algebra instructions:\n\ncarsM &lt;- scale(as.matrix(cars), center = TRUE, scale = FALSE)  # If scale = TRUE, then use correlation matrix below\nB1 &lt;- cov(carsM)  \nB2 &lt;- (t(carsM) %*% carsM ) / (nrow(carsM) - 1)  # divide (M^T * M) by N-1 to get covariance matrix\nsqrt(eigen(B1)$values)\n\n[1] 26.12524  3.08084\n\neigen(B1)$vectors\n\n          [,1]       [,2]\n[1,] 0.1656479 -0.9861850\n[2,] 0.9861850  0.1656479\n\n\nThe the “scores” output is equal to the original data multiplied by the rotation matrix.\n\nrotation &lt;- pca1 %&gt;% tidy(matrix = \"rotation\") %&gt;% pull(value) %&gt;% matrix(2, 2)  # also available as  pca1$rotation\ncenter &lt;- cars %&gt;% summarize(speed = mean(speed), dist = mean(dist)) # also available as pca1$center\nscores1 &lt;- pca1 %&gt;% tidy(matrix = \"scores\") %&gt;% pull(value) %&gt;% matrix(ncol = 2, byrow = TRUE)\n# scores2 &lt;- t((t(cars) - pca1$center)) %*% pca1$rotation\nscores2 &lt;- scale(cars, center = TRUE, scale = FALSE) %*% rotation\n\nNow I’ll plot the data projected onto the principal components. Notice that it is wide and thin (especially compared to the previous plot) because the data have been rotated to arrange as much of the variation as possible in the horizontal direction.\n\npca1 %&gt;% tidy(matrix = \"scores\") %&gt;% \n  pivot_wider(names_from=\"PC\", names_prefix = \"PC_\", values_from = \"value\") %&gt;%\n  ggplot(aes(x=PC_1, y=PC_2)) + geom_point() + coord_fixed()\n\n\n\n\nAn easy way to display the results of the PCA is to make a biplot using the ggfortify package. The biplot shows the observations as black dots and the original axes as red vectors. The option scale=0 keeps the same scaling as in the original plot. In normal usage you would not have coord_fixed() in the original plot and you would not use scale=0 in this plot.\n\nautoplot(pca1, data= cars, loadings=TRUE, loadings.label=TRUE, scale=0) + coord_equal()\n\n\n\n\nNormal use of autoplot would be to allow changing the scale of the two principal components (scale = 1) and to allow the axes to be scaled independently of each other (no coord_fixed()):\n\nautoplot(pca1, data= cars, loadings=TRUE, loadings.label=TRUE, scale=1, variance_percentage = TRUE)\n\n\n\n\nThe autoplot function is convenient and you can customize many features using the options in ggbiplot. I like to know exactly how a plot is drawn to check my understanding, so I’ll show you how to reproduce this plot using augment and ggplot.\nWe can make this plot (called a biplot) from the raw data by using a few scaling factors (lam, scaling) commonly used in these plots:\n\nlam &lt;- pca1$sdev[1:2] * sqrt(nrow(pca1$x))\nscaling &lt;- min(apply(abs(scores2), 2, max) / apply(abs(rotation), 2, max) / lam) * 0.8\nve &lt;- pca1$sdev^2 / sum(pca1$sdev^2)\nscores2 %&gt;% as_tibble() %&gt;% ggplot(aes(V1/lam[1], V2/lam[2])) + geom_point()  + \n  geom_segment(aes(x = 0, y = 0, xend = V1*scaling, yend = V2*scaling), \n               arrow = arrow(length = unit(0.25,\"cm\")),\n               color = \"red\",\n               data = as_tibble(rotation)) +\n  labs(x = paste0(\"PC1: \", round(ve[1]*100, 2), \"%\"),\n       y = paste0(\"PC2: \", round(ve[2]*100, 2), \"%\"))\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\n\n\n\nIf all you want is the scores and you don’t care about the scaling, you can just use augment and ggplot:\n\npca1 %&gt;%\n  augment(cars) %&gt;%\n  ggplot(aes(x=.fittedPC1, y= .fittedPC2)) +   # divide by lam[1] and lam[2] to get the scaled version\n  geom_point()\n\n\n\n\n\n21.1.1 Second example: penguins.\nThe palmer penguin data have 4 quantitative variables. We will scale them all to have mean 0 and standard deviation 1, since the units and magnitude of the numbers are not comparable. We will colour points by speices to make the patterns easier to see.\nautoplot has some quirks: the variable names must be quoted and colour must be spelled with a ‘u’. (Most ggplot functions allow for alternate spellings - color with and without a ‘u’, summarize with an ‘s’ instead of a ‘z’.) I don’t know of an easy way to use the ggrepel package with autoplot to avoid overprinting the text on the arrows or dots.\n\npenguins_no_na = na.omit(penguins)\npca2 &lt;- prcomp(penguins_no_na %&gt;% dplyr::select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm), scale=TRUE )\nautoplot(pca2, data = penguins_no_na, loadings=TRUE, loadings.label=TRUE,\n         colour='species', shape='island')\n\n\n\n\nGentoo penguins are mostly distinguished by having the highest body mass. Adélie and Chinstrap penguins have similar masses, but are distinguished by dimensions of their bills and flippers. You can use frame.type to shade the areas containing data from each species to highlight where the points are concentrated (by color).\n\nautoplot(pca2, data = penguins_no_na, loadings = TRUE, loadings.label = TRUE,\n         colour = 'species', shape = 'island', \n         frame.type = \"norm\", frame.level = 0.90)   # frame.type convex, norm, euclid, t; see ?ggbiplot\n\n\n\n\nWithout a PCA, you could attempt to see these patterns by making a complex array of scatterplots for each pair of variables.\n\npenguins_no_na %&gt;% \n  dplyr::select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm, species) %&gt;% \n  ggpairs(aes(color=species))\n\n\n\n\nYou should practice seeing how many of the pairwise differences in this pairs plot can be revealed in the single PCA.\nHere is a customized ggplot of the PCA results. I simplifed the work in the first example by not using the conventional scaling; instead I just picked scales for the arrows that looked good. Need to fix this.\n\nrotation2 &lt;- pca2 %&gt;% \n  tidy(matrix = \"rotation\") %&gt;%\n  pivot_wider(names_from = PC, names_prefix = \"PC\", values_from = value) \npca2 %&gt;%\n  augment(penguins_no_na) %&gt;%\n  ggplot() +\n  geom_point(aes(x = .fittedPC1, y = .fittedPC2, color = species, shape = island)) +\n  geom_segment(data = rotation2, mapping = aes(x = 0, xend = 3*PC1, y = 0, yend = 3*PC2), color = \"blue\",\n               arrow = arrow(angle = 20, type = \"closed\"))  +\n  geom_label_repel(data = rotation2,\n                  aes(x = 3*PC1, y = 3*PC2, label = column), \n                  color = \"darkblue\", fill = \"#FFFFFF80\",\n                  arrow = arrow(angle = 20, type = \"closed\")) +\n  labs(x = \"PC 1\", y = \"PC 2\")"
  },
  {
    "objectID": "lessons/121-PCA.html#further-reading",
    "href": "lessons/121-PCA.html#further-reading",
    "title": "21  Principal component analysis",
    "section": "\n21.2 Further reading",
    "text": "21.2 Further reading\n\nClaus Wilke’s PCA tutorial\n\nExample PCA on iris data\nhttps://juliasilge.com/blog/stack-overflow-pca/\nPCA using tidymodels\n\nPCA"
  },
  {
    "objectID": "lessons/122-mds.html#nmds-example",
    "href": "lessons/122-mds.html#nmds-example",
    "title": "22  Multidimensional scaling",
    "section": "\n22.1 NMDS example",
    "text": "22.1 NMDS example\nMorse code is a way of sending text messages using just two symbols: dot and dash, which was designed to be transmitted by a person clicking a key to make sounds and the receiver listening to the sounds and translating the message as it arrives.\nThe dataset below was created as part of an experiment to measure the rate at which patterns of sounds for one symbol were confused with sounds for a different symbol. The matrix is symmetric dissimilarity measure. Rows and columns vary across the 36 symbols tested (26 letters and 10 numeric digits). All diagonals are 0s. The off diagonal values are large if the sounds are not likely to be confuesed. Smaller dissimilarities correspond to symbols that are more likely to be confused. A small excerpt of the table is shown.\n\nmorse.dist &lt;- read.delim('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/morsecodes-dist.txt',\n                          row.names = 1, head = T)\nnames(morse.dist) &lt;- rownames(morse.dist)\nmorse.dist[1:5,1:5] |&gt; kable() |&gt; row_spec(0, monospace=TRUE) |&gt; \n  column_spec(1, monospace=TRUE) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\n.-\n-...\n-.-.\n-..\n.\n\n\n\n.-\n0\n167\n169\n159\n180\n\n\n-...\n167\n0\n96\n79\n163\n\n\n-.-.\n169\n96\n0\n141\n166\n\n\n-..\n159\n79\n141\n0\n172\n\n\n.\n180\n163\n166\n172\n0\n\n\n\n\n\nUse the function metaMDS from the package vegan to perform the NMDS ordination. The ordiplot function shows the objects from the dissimilarity matrix on a two-dimensional “ordination plot”. Points that are closer together are more likely to be confused (they are less dissimilar).\nFIX ME.\n\n# NMDS &lt;- metaMDS(morse.dist, trace=0)\n# NMDS\n# ordiplot(NMDS, cex = 1.5, type = 't')\n# stressplot(NMDS)\n\nThis next plot compares the distances in the original data to the distances as represented by the ordination. If the ordination represents the original data well, this will be close to a straight line. There is a point for every pairwise comparison. In this case there are 36 * 35 / 2 = 630 distances to compare. I have labeled some of the points with the largest distortion imposed by the ordination.\n\n# symbols &lt;- rownames(NMDS$points)\n# NMDS[c(\"dist\", \"dhat\", \"iidx\", \"jidx\")] |&gt; as_tibble() |&gt; \n#   mutate(comparison = paste0(symbols[iidx], \"/\", symbols[jidx]),\n#          comparison2 = case_when( abs(dist-dhat) &lt; 40 ~ \"\", TRUE ~ comparison)) |&gt;\n#   ggplot(aes(dist, dhat)) + \n#   geom_point(size=0.5, color=\"blue\") + \n#   geom_text_repel(aes(label = comparison2)) +\n#   theme_bw()\n\nYou can also access the points from the ordination and make the “ordiplot” using ggplot. The relative position of points on the plat is the only thing that matters – any rotation or translation of the plot contains the same information.\n\n# NMDS$points |&gt; as_tibble(rownames = \"Symbol\") |&gt;\n#   ggplot(aes(x = MDS1, y = MDS2 )) +\n#   geom_text(aes(label=Symbol)) + \n#   theme_bw()\n  # geom_point(size = 0.5) + \n  # geom_text_repel(aes(label = Symbol))\n\nYou might want to understand the NMDS analysis in terms of some properties of the Morse Code signals. This table has the length (1-5) and the ratio of short to long (dots and dashes) signals in each symbol. The envfit function then finds the direction each of these variables increases most rapidly across the ordination plane. The summary reports the direction and the correlation between these variables and the position on the ordination plot. The arrow for “length” follows the pattern in the ordination very well, while the arrow for the ratio of short to long only accounts for about half of the variation in the ordination plot.\n\n# morse.attr &lt;- read.delim('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/morsecodes-attr.txt',\n#                          row.names = 1, head = T)\n# ef &lt;- envfit(NMDS, morse.attr)\n# ef\n\nThe arrows show the direction of most rapid average increase of each variable.\n\n# ordiplot(NMDS, cex = 1.5, type = 't')\n# plot(ef)\n\nHere is a method to reproduce this plot using ggplot. First use str(ef) to examine the structure of the result from envfit. Then plot the points and arrows using ggplot.\n\n# arrows1 &lt;- ef$vectors$arrows |&gt; as_tibble(rownames = \"code\")\n# as_tibble(NMDS$points, rownames = \"code\") |&gt;\n#   ggplot(aes(x = MDS1, y = MDS2, label = code)) + \n#   geom_text() +\n#   geom_text(data = arrows1, aes(x = 25*NMDS1, y = 25*NMDS2)) +\n#   geom_segment(data = arrows1, aes(x = 20*NMDS1, y = 20*NMDS2, xend = 0, yend = 0)) +\n#     theme_bw()"
  },
  {
    "objectID": "lessons/122-mds.html#further-reading",
    "href": "lessons/122-mds.html#further-reading",
    "title": "22  Multidimensional scaling",
    "section": "\n22.2 Further reading",
    "text": "22.2 Further reading\n\n\nVegan package and the ggvegan package\nhttps://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/\nhttps://rpubs.com/collnellphd/GWU_nmds\n\nMorse code experiment description and analysis"
  },
  {
    "objectID": "lessons/123-kmean.html#make-clusters-from-points",
    "href": "lessons/123-kmean.html#make-clusters-from-points",
    "title": "23  K-means clustering",
    "section": "\n23.1 Make clusters from points",
    "text": "23.1 Make clusters from points\nLet’s start by making 3 clusters from the quantitative variables in the Palmer penguins data. We omit missing data and scale each variable to have mean 0 and standard deviation 1. We don’t want the different scales each variable is measured on to give more weight to one variable compared to the other. (We might choose to give one variable more weight, but we don’t want that to be simply a consequence of the units used to report the variables.)\n\npenguin_q &lt;- penguins %&gt;% select(species, \n                                 flipper_length_mm, bill_length_mm, bill_depth_mm, body_mass_g) %&gt;%\n  na.omit() \nkclust1 &lt;- kmeans(penguin_q %&gt;% select(-species) %&gt;% scale(),\n                 centers = 3)\n\nThe kclust object reports the means (centroid) of each cluster and the membership of each observation (1-3). We also get sum of squared deviations between the cluster mean and each observation. This is then compared to the total sums of squares, which is the sum of squared deviations from the mean if there was only one cluster. In our example, 72% of the total sums of squares is within clusters and the remainder (28%) is between clusters. This tells us that the three main clusters account for almost 75% of the squared deviations. This measure is the same one used in linear regression, which minimizes the total sum of squared deviations between the regression line and each observation.\n\nkclust1\n\nK-means clustering with 3 clusters of sizes 71, 148, 123\n\nCluster means:\n  flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n1        -0.3004658      0.8898759     0.7564847  -0.4487199\n2        -0.8175594     -0.9723116     0.5499273  -0.6907503\n3         1.1571696      0.6562677    -1.0983711   1.0901639\n\nClustering vector:\n  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [38] 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2\n [75] 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[186] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[223] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[260] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n[297] 2 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1]  81.56839 155.25908 143.15025\n (between_SS / total_SS =  72.1 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThese data can be presented in an easy to use table with tidy:\n\ntidy(kclust1) %&gt;% kable(digits = 2)\n\n\n\nflipper_length_mm\nbill_length_mm\nbill_depth_mm\nbody_mass_g\nsize\nwithinss\ncluster\n\n\n\n-0.30\n0.89\n0.76\n-0.45\n71\n81.57\n1\n\n\n-0.82\n-0.97\n0.55\n-0.69\n148\n155.26\n2\n\n\n1.16\n0.66\n-1.10\n1.09\n123\n143.15\n3\n\n\n\n\n\nCategorical variables (species) can be added to the resuls using augment as we have done with other models. We can then pick two variables and plot them along with the cluster identity and species. Here the colours show the cluster number and the shapes show the species. We can see that the clusters do not perfectly separate out observations by species, but there is a strong similarity. This means that the variables we selected are mostly, but not completely, sufficient to distingush the three species. Remember that we used 4 variables for the classification, not just the two shown here.\n\nkc1 &lt;- augment(kclust1, penguin_q)\nggplot(kc1, aes(x=bill_length_mm, y = bill_depth_mm, color=.cluster, shape=species)) + \n  geom_point() + theme_bw()\n\n\n\n\nYou can add the center of each cluster to the plot, but since the clustering was done on data scaled to have mean 0 and standard deviation 1, you must be careful to scale the data here as well.\n\nkc1 &lt;- augment(kclust1, penguin_q)\nggplot(kc1, aes(x = scale(bill_length_mm), \n                y = scale(bill_depth_mm))) + \n  geom_point(aes(color = .cluster, shape = species)) +\n  geom_point(data = tidy(kclust1), aes(color = cluster), \n             size = 10, shape = \"o\", show.legend = FALSE) + theme_bw()\n\n\n\n\nWe can make a table of the number of each species in each cluster using summarize. Adélie penguins make up most of cluster 1, with 8 observations appearing in cluster 3. Chinstrap penguins make up most of cluster 3, with 5 observations in cluster 1. All the Gentoo penguins are in cluster 2 and there are no other species of penguin in this cluster. This result should seem reasonable given all the previous exploration we have done on this data.\n\naugment(kclust1, data = penguin_q) %&gt;%\n  count(.cluster, species)\n\n# A tibble: 5 × 3\n  .cluster species       n\n  &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;\n1 1        Adelie        8\n2 1        Chinstrap    63\n3 2        Adelie      143\n4 2        Chinstrap     5\n5 3        Gentoo      123"
  },
  {
    "objectID": "lessons/123-kmean.html#example-2",
    "href": "lessons/123-kmean.html#example-2",
    "title": "23  K-means clustering",
    "section": "\n23.2 Example 2",
    "text": "23.2 Example 2\nLet’s take a look at a dataset with a lot more groups. The mpg dataset has several quantitative variables that might be useful for classifying cars (displ, cyl, cty, hwy). We can make a series of clusterings of these data and see how well they correspond to the class of the car (compact, subcompact, etc).\n\nkclust2 &lt;- kmeans(mpg %&gt;% select(displ, cyl, cty, hwy) %&gt;% scale, \n                  centers = 4)\ntidy(kclust2)\n\n# A tibble: 4 × 7\n    displ      cyl    cty    hwy  size withinss cluster\n    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;  \n1 -0.974  -1.14     0.756  0.736    67     21.5 1      \n2 -1.32   -1.17     2.37   2.11     14     14.9 2      \n3  1.29    1.31    -1.01  -0.976    70     47.5 3      \n4 -0.0752  0.00914 -0.159 -0.127    83     61.8 4      \n\n\nCompare these clusters to the class of the cars. There are many classes of each car in some clusters.\n\naugment(kclust2, mpg) %&gt;% count(.cluster, class)\n\n# A tibble: 19 × 3\n   .cluster class          n\n   &lt;fct&gt;    &lt;chr&gt;      &lt;int&gt;\n 1 1        compact       29\n 2 1        midsize       16\n 3 1        minivan        1\n 4 1        pickup         1\n 5 1        subcompact    14\n 6 1        suv            6\n 7 2        compact        5\n 8 2        subcompact     9\n 9 3        2seater        5\n10 3        midsize        2\n11 3        pickup        20\n12 3        subcompact     5\n13 3        suv           38\n14 4        compact       13\n15 4        midsize       23\n16 4        minivan       10\n17 4        pickup        12\n18 4        subcompact     7\n19 4        suv           18\n\n\nIf we use glance we get to see how the sums of squares are partitioned. I’ll add a fifth variable that measures the proportion of sums of squares within compared to the total.\n\nglance(kclust2) %&gt;% mutate(proportion_within = tot.withinss / totss)\n\n# A tibble: 1 × 5\n  totss tot.withinss betweenss  iter proportion_within\n  &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;\n1  932.         146.      786.     2             0.156\n\n\nHow can we decide how many clusters to make? Let’s compute the proportion of sums of squares accounted for by the clustering for 2, 3, 4, etc., clusters. We will use functions in broom to accomplish this including nest and map.\n\nkclusts2 &lt;- \n  tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(mpg %&gt;% select(displ, cyl, cty, hwy) %&gt;% scale,\n                            centers = .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, mpg)\n  )\n\nThat one object has a lot of information – the tidy and glance results on 9 clusterings plus the augmented data for each. Let’s extract them into 3 different tables to make the results easier to work with.\n\nclusters2    &lt;- kclusts2 %&gt;% unnest(cols = c(tidied))\nassignments2 &lt;- kclusts2 %&gt;% unnest(cols = c(augmented))\nclusterings2 &lt;- kclusts2 %&gt;% unnest(cols = c(glanced))\n\nWe can score each cluster according to the sums of squares as follows:\n\nclusterings2 %&gt;% mutate(proportion_within = tot.withinss / totss) %&gt;%\n  ggplot(aes(x = k, y = proportion_within)) + \n  geom_line() + geom_point() + theme_bw()\n\n\n\n\nSeven clusters is slightly worse than 6, so that’s probably too many. Most of the decrease happens with 4 or 5 clusters, so that may be enough.\nLet’s plot the clusters. I’ll show the number of cylinders using a symbol; there are only 4 to show, so that won’t be too many to look at.\n\np1 &lt;- ggplot(assignments2, aes(x = cty, y = displ)) +\n        geom_point(aes(color = .cluster, shape=factor(cyl)), alpha = 0.8) + \n        facet_wrap(~ k) + theme_bw()\np1\n\n\n\n\nWe can try to make a facetted plot, showing car classification and clusters. There are 7 classes and 9 clusters, so that would make 63 facets, which is a lot to look at. I’ll select just the classifications with 4, 5, and 6 clusters.\n\np2 &lt;- assignments2 %&gt;% filter(k &gt;= 4, k &lt;= 6) %&gt;%\n  ggplot(aes(x = cty, y = displ)) +\n        geom_point(aes(color = .cluster, shape = factor(cyl)), alpha = 0.8) + \n        facet_grid(k ~ class, scales = \"free\") + theme_bw()\np2\n\n\n\n\nWe can see that most classes of cars (except 2 seaters) get divided up into multiple clusters. So a clustering of these 4 quantiative variables does not separate out cars according to their class."
  },
  {
    "objectID": "lessons/123-kmean.html#further-reading",
    "href": "lessons/123-kmean.html#further-reading",
    "title": "23  K-means clustering",
    "section": "\n23.3 Further reading",
    "text": "23.3 Further reading\nThese two sources provide more detail on how k-means clustering works.\n\nNotes on K-means from the tidymodels package\nChapter on K-means from Roger Peng’s book"
  },
  {
    "objectID": "lessons/125-slides.html#what-makes-a-good-presentation",
    "href": "lessons/125-slides.html#what-makes-a-good-presentation",
    "title": "24  Making slides for presentations",
    "section": "\n24.1 What makes a good presentation?",
    "text": "24.1 What makes a good presentation?\nA good presentation tells a story that engages your audience, has a clear purpose advanced through a predictable arc from introduction through evidence to a conclusion. There are many ways to design a good presentation, but most people find that practice and reflection on what works for your own personal goals and audience are a key part of the trajectory."
  },
  {
    "objectID": "lessons/125-slides.html#why-we-make-slides",
    "href": "lessons/125-slides.html#why-we-make-slides",
    "title": "24  Making slides for presentations",
    "section": "\n24.2 Why we make slides",
    "text": "24.2 Why we make slides\nVisual aides, commonly called slides, can help you communicate with your audience. Computer software makes it easy to generate lots of slides full of text that the presenter reads. This is generally a bad approach as the audience’s attention is focused more on reading than listening and as a result they tend to be less engaged with your presentation. There are, of course, many different opinions on this topic!\nIn data visualization, we make slides to display the visualization! That’s the primary reason for the slide, so your slides should focus on the visualization. The visualization should be large enough to be seen by everyone watching. This is much easier when everyone watches a presentation by teleconferencing. In a classroom, when the slide is far away, you must be careful not to put too much on a slide and not to make any element of the slide too small."
  },
  {
    "objectID": "lessons/125-slides.html#how-to-make-slides-using-rstudio",
    "href": "lessons/125-slides.html#how-to-make-slides-using-rstudio",
    "title": "24  Making slides for presentations",
    "section": "\n24.3 How to make slides using Rstudio",
    "text": "24.3 How to make slides using Rstudio\nThere are several packages to help you make slides with R markdown. The method we will use is well integrated into Rstudio. To create a presentation, select the menu File &gt; New File &gt; Quarto Presentation.1 You will be asked to provide a title for your presentation and there are a few options, but you don’t need to change any of them. This will give you a sample file with a title slide. The R markdown tools you already know for text formatting, list making, and titles all work here. Of course, you can insert R code blocks and have the output included as well. All of this is described in the help available from the menu Help &gt; Markdown quick reference.1 You need to have a version of Rstudio from the July 2022 or newer for this to work. If your copy of Rstudio doesn’t have this menu item, this is a good time to update!\nThe top of your slide file should have five lines: a row with three minus signs, a title, an author, and a format line, followed by three minus signs by themselves on a line. Use format revealjs:\n---\ntitle: \"Practice slides\"\nauthor: \"Andrew Irwin\"\nformat: revealjs\n---\nThe most important new idea is how to start a new slide. It’s simple. Create a level one or two heading (one or two hash marks at the start of a line, followed a space, then a short title) or write three minus signs at the start of a blank line. A level one heading is the largest and it is centered vertically on the slide. A level 2 heading appears at the top of a slide. If you start a new slide with three minus signs, there is no title. That’s handy if you want to make as much room as possible for a graph or image.\nThe Quarto website has excellent documentation on the basics and more specialized formatting for making slides. In this lesson I’ll give you just enough information to get started."
  },
  {
    "objectID": "lessons/125-slides.html#formatting-tricks",
    "href": "lessons/125-slides.html#formatting-tricks",
    "title": "24  Making slides for presentations",
    "section": "\n24.4 Formatting tricks",
    "text": "24.4 Formatting tricks\n\n24.4.1 Hiding code\nUsually in a presenation you don’t want to show your computer code – its too complex to be easily digested by the audience. (This course, is naturally, a bit of an exception!) You can hide output by writing the start of your code block like this:\n{r echo=FALSE}\nThe main options for showing code, the results of code, warning messages, and errors are summarized here. The option fig.cap=\"Message\" is particularly useful for adding text. (The options are: echo, eval, include, warning, message, which can all be TRUE or FALSE.)\nIf you want to hide the code, but make it possible to see it after a click, you can “fold” the code so it’s not visible by adding two special comments inside your R code block:\n#| code-fold: true\n#| code-summary: \"Show the code\"\nTry it out to see what happens."
  },
  {
    "objectID": "lessons/125-slides.html#changing-the-size-and-position-of-a-plot",
    "href": "lessons/125-slides.html#changing-the-size-and-position-of-a-plot",
    "title": "24  Making slides for presentations",
    "section": "\n24.5 Changing the size and position of a plot",
    "text": "24.5 Changing the size and position of a plot\nThe following code chunk options let you control the size and position of a plot:\n\n\nout.width = \"70%\" (or other value, in quotation marks)\n\nfig.align = \"center\" (or left, right)\n\nFor more figure formatting options look here"
  },
  {
    "objectID": "lessons/125-slides.html#two-column-formatting",
    "href": "lessons/125-slides.html#two-column-formatting",
    "title": "24  Making slides for presentations",
    "section": "\n24.6 Two column formatting",
    "text": "24.6 Two column formatting\nIf you want to divide your slide into a left and right column, split your material between sections marked off with three colons (:::) as follows:\n:::: {.columns}\n\n::: {.column width=\"40%\"}\nLeft column\n:::\n\n::: {.column width=\"60%\"}\nRight column\n:::\n\n::::\nYou can adjust the amount of space used by each side by changing the percentages. You can have more than two columns if you like too—but your slide will get crowded and the columns get narrow."
  },
  {
    "objectID": "lessons/125-slides.html#what-if-my-material-wont-fit-on-a-single-slide",
    "href": "lessons/125-slides.html#what-if-my-material-wont-fit-on-a-single-slide",
    "title": "24  Making slides for presentations",
    "section": "\n24.7 What if my material won’t fit on a single slide?",
    "text": "24.7 What if my material won’t fit on a single slide?\nIf you try to put too much on a slide, there just isn’t room and it disappears off the bottom. Usually the right decision is to split your material across two slides.\nBut you have other options. After the title you can add an instruction to make everything on the slide smaller. For example write a title as\n## Slide title {.smaller}\nIf the material still doesn’t fit, you can make the slide scrollable using\n## Slide title {.scrollable}\nbut remember, when you show the slide you’ll have to scroll to let viewers see the material off the bottom. This might work best with slides you distribute so that the viewer can scroll by themselves. In a live presentation, scrolling like this could be quite disorienting!"
  },
  {
    "objectID": "lessons/125-slides.html#summary",
    "href": "lessons/125-slides.html#summary",
    "title": "24  Making slides for presentations",
    "section": "\n24.8 Summary",
    "text": "24.8 Summary\nThis was an introduction to making slides with R. I’ve picked a method which is relatively easy to use and is well integrated with Rstudio."
  },
  {
    "objectID": "lessons/125-slides.html#exercise",
    "href": "lessons/125-slides.html#exercise",
    "title": "24  Making slides for presentations",
    "section": "\n24.9 Exercise",
    "text": "24.9 Exercise\nMake a slide presentation with a few slides. Click “Render” (instead of knit) to see your slides. The slide presentation is just a regular html file, so you can open it in a web browser."
  },
  {
    "objectID": "lessons/125-slides.html#further-reading",
    "href": "lessons/125-slides.html#further-reading",
    "title": "24  Making slides for presentations",
    "section": "\n24.10 Further reading",
    "text": "24.10 Further reading\n\nNotes on using Quarto (very similar to R markdown) to make presentation slides: introduction and more information for making slides viewable in the web browser (which is what I recommend and show you in this course.)\nPatrick Winston’s video on how to speak effectively\n\nLots more information on using markdown and R markdown, presented in slide format\nA blog post about learning to make presentation slides\n\nThirty days of tips on using Quarto for presentations"
  },
  {
    "objectID": "lessons/127-test-data.html#an-introduction-to-testing",
    "href": "lessons/127-test-data.html#an-introduction-to-testing",
    "title": "25  Checking your work",
    "section": "\n25.1 An introduction to testing",
    "text": "25.1 An introduction to testing\nYou’ve made a data visualization. It looks great. All the calculations necessary are in a single Rmd file – reading the data, organizing the data, and creating the figure. You can revise the data and reproduce the plot any time you want.\nBut then a thought occurs to you – how do I know the visualization is correct? The computer saves us time by doing calculations for us, but the price is that you can’t keep track of everything it does.\nWhat if there is a problem with the data? Maybe the data you analyzed has a lot of observations – too many to check by hand. Or maybe the data are fine, but something went wrong when you read them into R. There are lots of ways for errors to creep in. Missing values when you thought there were none. Unexpected levels of factors (too many or too few). Detectable errors in the data such as impossible values.\nThe best idea to counteract all these problems is testing. The secret is to get the computer to perform the tests for you. In this lesson we will discuss two kinds of testing: checking your data and checking your calculations.\nA lot of testing is done for you before you even start R: most (we hope all) the packages you use are carefully tested to be sure they work as intended. Still – you might misunderstand what the packages are supposed to do. Or you might make a typo and use the wrong variable name somewhere. Or you might get the logic of your calculation wrong. So you should test your work in as many ways as you can.\nThe most important reason to test is that you will catch mistakes. Possibly the second most important reason for doing testing – and being explicit about it – is that it can help the people who use your analysis have more confidence in it. This includes you in the future!"
  },
  {
    "objectID": "lessons/127-test-data.html#checking-analysis-and-visualizations",
    "href": "lessons/127-test-data.html#checking-analysis-and-visualizations",
    "title": "25  Checking your work",
    "section": "\n25.2 Checking analysis and visualizations",
    "text": "25.2 Checking analysis and visualizations\nOnce you have a preliminary analysis, develop a testing plan. Methods that can help:\n\nCheck for errors in your data.\nPerform your analysis on a small subset of your data that you can understand without computer help. This is the “sample calculation” method of testing.\nPerform your analysis on simulated or fake “data” designed to test certain cases (independent variables, strong dependence, etc.) that will allow you to check your methods and interpretation.\nPerform your calculations or visualizations two different ways, to check your understanding. This is especially useful if you have a fast and a slow way of doing a calculation. Use the slow way as a check on the fast method."
  },
  {
    "objectID": "lessons/127-test-data.html#data-errors",
    "href": "lessons/127-test-data.html#data-errors",
    "title": "25  Checking your work",
    "section": "\n25.3 Data errors",
    "text": "25.3 Data errors\nWhat kinds of errors can appear in data?\n\nMisspellings, upper/lower case inconsistency, extra spaces\nDuplicated observations\nMiss-coded missing data (writing -999 or 0 instead of NA)\nInconsistently formatted dates and times\nImpossible values arising from typographical errors\nData in wrong columns\nAll the data can look correct, but the methods for collecting data may have changed, creating “silent” errors\n\nWhy do data sometimes have errors?\n\nTo answer this, you need to know about the process used to create the data. Were some data output by a particular machine or software package that has errors? Were the data typed in by a single person? Were many different people, who may have had different understandings of the data collection goals, involved? Were the data collected over a long period of time, when machines, people, and goals may have changed?\n\nHow can you find errors in data?\n\nMake summary tables and cross-check the summaries\nLook for missing data\nCheck to be sure the correct number of variables (columns) and rows (observations) are present\nPlot histograms and densities\nWrite tests to test data belong to a legitimate set of values\n\nWhat do you do with errors?\n\nKeep original data, so you can revert the change in case of a misunderstanding\nLog changes and describe error\nWrite tests to check for future errros\nCommunicate with the people who collected the data and the people who will receive the analysis\n\n\n25.3.1 Sample data to test\nThe following data were collected by a class of students evaluating their ability to identify a jelly bean flavour by blindfolded testing. Much of our sense of taste comes from smell, so there were two treatments. All participants were blindfolded. In the treatment, the participants blocked their nose to reduce their sense of smell and in the control participants could smell normally. Groups of students entered their data into a single Google docs spreadsheet which was exported to the csv file below.\nThis dataset has a lot of problems, but they are very typical for data entered by a group of people who are not directly involved in the systematic analysis of the data with a software package like R. (They are not attuned to the problems of data errors.) Take a look at the file and see how many problems you can find before continuing.\n\njelly &lt;- read_csv(here(\"lessons/static/jelly-bean-data.csv\"),\n                  show_col_types = FALSE) \n\nNew names:\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n\njelly |&gt; datatable()\n\n\n\n\n\n\nYou should always look at data. For a first look, View, skim (in the skimr package), and glimpse functions are useful and will help you identify some problems.\n\nglimpse(jelly)\n\nRows: 261\nColumns: 27\n$ Initials                            &lt;chr&gt; \"PKL\", \"PKL\", \"PKL\", \"LAG\", \"LAG\",…\n$ `Group (choose from drop down)`     &lt;chr&gt; \"control\", \"control\", \"control\", \"…\n$ `Trial #`                           &lt;dbl&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3…\n$ Flavour                             &lt;chr&gt; \"orange\", \"strawberry\", \"cherry\", …\n$ `Reaction time (in sec)`            &lt;chr&gt; \"7.2\", \"7.6\", \"10.1\", \"10.15\", \"12…\n$ `Accuracy (0=incorrect, 1=correct)` &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1…\n$ ...7                                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...8                                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...9                                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...10                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...11                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...12                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...13                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...14                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...15                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...16                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...17                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...18                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...19                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...20                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...21                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...22                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...23                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...24                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...25                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...26                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...27                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n# skimr::skim(jelly)  # try this yourself on some data\n\nThere are many columns that have no heading and are purely missing data. The janitor package has a function to remove rows and columns that are all missing data. I think some of these column names are a bit long and the punctuation makes them hard to type, so I will rename them to shorter names.\n\njelly &lt;- jelly |&gt;\n  clean_names() |&gt;\n  rename(group = group_choose_from_drop_down,\n         reaction_s = reaction_time_in_sec,\n         accuracy = accuracy_0_incorrect_1_correct) |&gt;\n  remove_empty(c(\"rows\", \"cols\"))\n\nCheck to see if there are any more missing data using the miss_var_summary function from the naniar package.\n\njelly |&gt; miss_var_summary() |&gt; \n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\ninitials\n56\n24.4541485\n\n\ngroup\n2\n0.8733624\n\n\nreaction_s\n2\n0.8733624\n\n\naccuracy\n2\n0.8733624\n\n\ntrial_number\n1\n0.4366812\n\n\nflavour\n1\n0.4366812\n\n\n\n\n\nAre those missing values are a problem? I might choose not to care about the person who collected the data (initials), but a missing flavour or accuracy is a big problem!\nThe dlookr package provides a similar table, plus it adds a count of the number of distinct values for each variable.\nFIX ME\n\n# diagnose(jelly) |&gt; kable() |&gt; kable_styling(full_width = FALSE)\n\nWe still don’t know what most of these variables mean, but already one thing should stand out. The variable accuracy (which originally also had the notation 0_incorrect_1_correct) presumably should only be 0 or 1, but has three different values. We may not care about the initials of the investigator, but we might be concerned about the fact that there are 56 missing values. The reaction time (measured in seconds) is text data, but should be a number."
  },
  {
    "objectID": "lessons/127-test-data.html#data-quality-assurance",
    "href": "lessons/127-test-data.html#data-quality-assurance",
    "title": "25  Checking your work",
    "section": "\n25.4 Data quality assurance",
    "text": "25.4 Data quality assurance\nYou can get report on your data using the pointblank package. (See the documentation linked in further reading for examples.) Let’s test a few things:\n\nare any values of the accuracy variable not 0 or 1?\nis reaction time and trial_number numeric? (We know the answer already, but I’ll show how to test this condition.)\nare there any negative reaction times?\nare all the rows distinct? (It’s unlikely, but not impossible to get the same result twice.)\n\nFirst we will write the tests.\n\nmy_test &lt;- jelly |&gt; \n  create_agent(actions = action_levels(warn_at = 1)) |&gt;\n  col_vals_in_set(accuracy, c(0,1)) |&gt;\n  col_is_numeric(vars(trial_number, reaction_s)) |&gt;\n  col_vals_gt(reaction_s, 0) |&gt;\n  rows_distinct() \n\nNow you can use the tests to get a report. (The report is not shown. Experiment with these functions on your own.)\n\nmy_test |&gt; interrogate() \n\nReaction time (in seconds) is text not a number. Let’s fix that. Probably there will be some non-numeric values in there, because otherwise the data would have been read as numeric. So let’s look for those rows first. If you try to convert text to a number and the text doesn’t look like a number, you’ll get an NA instead. So filter out the NAs that get created when you convert from text to number. R will give us a message to alert us to the fact that there are some NAs created.\n\njelly |&gt; filter(is.na(as.numeric(reaction_s))) |&gt;\n  kable() |&gt; kable_styling(full_width = FALSE)\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `is.na(as.numeric(reaction_s))`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n\n\ninitials\ngroup\ntrial_number\nflavour\nreaction_s\naccuracy\n\n\n\nKS\nexperimental\n3\nOrange\nLemon\n0\n\n\nBP\nNA\nNA\nNA\nNA\nNA\n\n\nNA\ncontrol\n3\nyellow and brown\nNA\n1\n\n\n\n\n\nOne of those is obviously an error. (Lemon is not a number!) In a careful analysis, you would contact the person who recorded the data (that’s what the initials are for) and find out what went wrong. Same thing with the NA, where the time didn’t get recorded. For now, we’ll just throw away these numbers.\n\njelly &lt;- jelly |&gt; \n  mutate(reaction_s = as.numeric(reaction_s)) |&gt;\n  filter(!is.na(reaction_s))\n\nThe pointblank package also has a helpful function to produce a structured report on your whole dataset. (Again the results are not shown here.)\n\njelly |&gt; scan_data()"
  },
  {
    "objectID": "lessons/127-test-data.html#data-cleaning",
    "href": "lessons/127-test-data.html#data-cleaning",
    "title": "25  Checking your work",
    "section": "\n25.5 Data cleaning",
    "text": "25.5 Data cleaning\nFreshly collected and tabulated data are often “dirty”: there are errors in the data such as typographical errors, impossible values, or simply more detail than is appropriate for your analysis. Preparing a dataset for analysis is called data cleaning and it can be a complex and lengthy task that is at least as complex as analyzing data. Although there are common tasks performed in data cleaning, every dataset presents its own set of challenges. That’s the reason we usually use “clean” data in courses.\nData cleaning tasks are very individual and can take considerable creativity. Here we’ll just try a few.\nLet’s look at the values of flavour. It seems like there are a lot of them.\n\njelly |&gt; count(flavour) |&gt; arrange(-n) |&gt; \n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\nflavour\nn\n\n\n\norange\n32\n\n\nlemon\n23\n\n\nOrange\n19\n\n\ncherry\n19\n\n\ngrape\n18\n\n\ncoconut\n14\n\n\nCherry\n10\n\n\nBanana\n9\n\n\nGrape\n9\n\n\nLemon\n9\n\n\nbanana\n7\n\n\nCoconut\n5\n\n\nlime\n4\n\n\npurple\n4\n\n\nYellow White\n3\n\n\napple\n3\n\n\nstrawberry\n3\n\n\nPurple\n2\n\n\nWhite\n2\n\n\nYellow Brown\n2\n\n\nblueberry\n2\n\n\nbubblegum\n2\n\n\nred\n2\n\n\nwhite\n2\n\n\nyellow\n2\n\n\nyellow and white\n2\n\n\nApple\n1\n\n\nBubblegum\n1\n\n\nCoffee\n1\n\n\nLime\n1\n\n\nPinapple\n1\n\n\nPlum\n1\n\n\nRed\n1\n\n\nWatermelon\n1\n\n\ncinnamon\n1\n\n\ncocnut\n1\n\n\nlemon/lime\n1\n\n\nlicorice\n1\n\n\nmarshmallow\n1\n\n\nraspberry\n1\n\n\nwatermelon\n1\n\n\nyellow + white\n1\n\n\nyellow and brown\n1\n\n\n\n\n\nWe definitely have some duplication. I’ll do one correction – changing all letters to lower case so differences in capitalization don’t duplicate flavours. (This simple change eliminates 13 different values of flavour.) If you look closely at the data, you will find spelling errors, different codings (and, +, or neither), and one missing flavour.\n\njelly |&gt; mutate(flavour = tolower(flavour)) |&gt;\n  count(flavour) |&gt; arrange(-n) |&gt; \n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\nflavour\nn\n\n\n\norange\n51\n\n\nlemon\n32\n\n\ncherry\n29\n\n\ngrape\n27\n\n\ncoconut\n19\n\n\nbanana\n16\n\n\npurple\n6\n\n\nlime\n5\n\n\napple\n4\n\n\nwhite\n4\n\n\nbubblegum\n3\n\n\nred\n3\n\n\nstrawberry\n3\n\n\nyellow white\n3\n\n\nblueberry\n2\n\n\nwatermelon\n2\n\n\nyellow\n2\n\n\nyellow and white\n2\n\n\nyellow brown\n2\n\n\ncinnamon\n1\n\n\ncocnut\n1\n\n\ncoffee\n1\n\n\nlemon/lime\n1\n\n\nlicorice\n1\n\n\nmarshmallow\n1\n\n\npinapple\n1\n\n\nplum\n1\n\n\nraspberry\n1\n\n\nyellow + white\n1\n\n\nyellow and brown\n1\n\n\n\n\n\nUse what you learned to tidy the data. Here are some changes I’ll make before continuing the analysis.\n\njelly &lt;- jelly |&gt; \n  mutate(flavour = tolower(flavour),\n         flavour = case_when(flavour == \"cocnut\" ~ \"coconut\", \n                             TRUE ~ flavour), \n         flavour = str_replace(flavour, \" and \", \" \"),\n         flavour = str_replace(flavour, \"[/+]\", \" \"),\n         flavour = str_squish(flavour))"
  },
  {
    "objectID": "lessons/127-test-data.html#analyze-a-subset-of-your-data",
    "href": "lessons/127-test-data.html#analyze-a-subset-of-your-data",
    "title": "25  Checking your work",
    "section": "\n25.6 Analyze a subset of your data",
    "text": "25.6 Analyze a subset of your data\nLet’s compute the average reaction time for correct and incorrect answers for each flavour and each treatment. We will count the number of observations for each grouping too.\n\njelly |&gt; group_by(flavour, group, accuracy) |&gt;\n  summarize(count = n(),\n            mean_reaction_time = mean(reaction_s),\n            .groups = \"drop\") |&gt; \n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\nflavour\ngroup\naccuracy\ncount\nmean_reaction_time\n\n\n\napple\nexperimental\n0\n4\n14.660000\n\n\nbanana\ncontrol\n1\n9\n12.616667\n\n\nbanana\nexperimental\n0\n4\n24.222500\n\n\nbanana\nexperimental\n1\n3\n17.166667\n\n\nblueberry\ncontrol\n0\n1\n5.980000\n\n\nblueberry\nexperimental\n0\n1\n20.730000\n\n\nbubblegum\ncontrol\n0\n1\n7.300000\n\n\nbubblegum\nexperimental\n0\n2\n17.620000\n\n\ncherry\ncontrol\n0\n4\n17.487500\n\n\ncherry\ncontrol\n1\n12\n6.785000\n\n\ncherry\nexperimental\n0\n11\n15.835455\n\n\ncherry\nexperimental\n1\n1\n12.320000\n\n\ncherry\nexperimental\nNA\n1\n11.000000\n\n\ncinnamon\ncontrol\n0\n1\n5.920000\n\n\ncoconut\ncontrol\n0\n3\n22.596667\n\n\ncoconut\ncontrol\n1\n6\n6.821667\n\n\ncoconut\nexperimental\n0\n9\n18.861111\n\n\ncoconut\nexperimental\n1\n2\n15.040000\n\n\ncoffee\nexperimental\n0\n1\n7.650000\n\n\ngrape\nControl\n1\n1\n13.250000\n\n\ngrape\ncontrol\n0\n6\n23.903333\n\n\ngrape\ncontrol\n1\n5\n5.926000\n\n\ngrape\nexperimental\n0\n12\n17.655833\n\n\ngrape\nexperimental\n1\n3\n16.473333\n\n\nlemon\ncontrol\n0\n10\n10.756000\n\n\nlemon\ncontrol\n1\n8\n7.138750\n\n\nlemon\nexperimental\n0\n10\n17.110000\n\n\nlemon\nexperimental\n1\n3\n15.613333\n\n\nlemon\nNA\n1\n1\n5.460000\n\n\nlemon lime\nexperimental\n1\n1\n10.800000\n\n\nlicorice\nexperimental\n0\n1\n8.460000\n\n\nlime\ncontrol\n0\n3\n10.640000\n\n\nlime\nexperimental\n0\n2\n12.760000\n\n\nmarshmallow\nexperimental\n0\n1\n12.050000\n\n\norange\ncontrol\n0\n9\n14.307778\n\n\norange\ncontrol\n1\n16\n6.546250\n\n\norange\nexperimental\n0\n16\n18.295000\n\n\norange\nexperimental\n1\n10\n10.086000\n\n\npinapple\nexperimental\n0\n1\n11.770000\n\n\nplum\ncontrol\n0\n1\n15.200000\n\n\npurple\ncontrol\n1\n2\n14.445000\n\n\npurple\nexperimental\n0\n3\n14.756667\n\n\npurple\nexperimental\n1\n1\n36.000000\n\n\nraspberry\nexperimental\n0\n1\n16.150000\n\n\nred\ncontrol\n1\n1\n12.120000\n\n\nred\nexperimental\n0\n2\n24.000000\n\n\nstrawberry\ncontrol\n0\n3\n23.716667\n\n\nwatermelon\ncontrol\n0\n2\n23.800000\n\n\nwhite\ncontrol\n1\n2\n3.300000\n\n\nwhite\nexperimental\n0\n1\n21.500000\n\n\nwhite\nexperimental\n1\n1\n7.480000\n\n\nyellow\ncontrol\n1\n2\n9.265000\n\n\nyellow brown\ncontrol\n1\n2\n15.800000\n\n\nyellow brown\nexperimental\n0\n1\n9.340000\n\n\nyellow white\ncontrol\n0\n1\n13.060000\n\n\nyellow white\ncontrol\n1\n1\n15.500000\n\n\nyellow white\nexperimental\n0\n3\n20.200000\n\n\nyellow white\nexperimental\n1\n1\n11.000000\n\n\n\n\n\nThis is a fairly simple calculation, but it serves to demonstrate the “manual check” method. Filter out just the incorrect, experimental results for apple flavours. Check that the number of rows and average match.\n\njelly |&gt; filter(flavour == \"apple\",\n                 accuracy == 0,\n                 group == \"experimental\") |&gt; \n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\ninitials\ngroup\ntrial_number\nflavour\nreaction_s\naccuracy\n\n\n\nFACP\nexperimental\n1\napple\n14.30\n0\n\n\nVP\nexperimental\n1\napple\n21.35\n0\n\n\nNA\nexperimental\n2\napple\n9.99\n0\n\n\nTw\nexperimental\n1\napple\n13.00\n0\n\n\n\n\n\nLooks good!"
  },
  {
    "objectID": "lessons/127-test-data.html#use-fake-data-where-you-already-know-the-answer",
    "href": "lessons/127-test-data.html#use-fake-data-where-you-already-know-the-answer",
    "title": "25  Checking your work",
    "section": "\n25.7 Use fake data, where you already know the answer",
    "text": "25.7 Use fake data, where you already know the answer\nI’ll create some simple data and then compute the averages using exactly the same code I wrote above. I’ll put in a NA in one row just to see what happens.\n\nmy_jelly &lt;- tribble(\n  ~group, ~ flavour, ~ reaction_s, ~ accuracy,\n  \"experimental\", \"lime\", 1, 1,\n  \"experimental\", \"lime\", 2, 1,\n  \"experimental\", \"lemon\", 3, 1,\n  \"control\", \"lemon\", 4, 1,\n)\nmy_jelly |&gt; kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\ngroup\nflavour\nreaction_s\naccuracy\n\n\n\nexperimental\nlime\n1\n1\n\n\nexperimental\nlime\n2\n1\n\n\nexperimental\nlemon\n3\n1\n\n\ncontrol\nlemon\n4\n1\n\n\n\n\n\nI know exactly what I’m expecting. Stop and decide for yourself before reading on.\n\nmy_jelly |&gt; group_by(flavour, group, accuracy) |&gt;\n  summarize(count = n(),\n            mean_reaction_time = mean(reaction_s),\n            .groups = \"drop\") |&gt; \n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\nflavour\ngroup\naccuracy\ncount\nmean_reaction_time\n\n\n\nlemon\ncontrol\n1\n1\n4.0\n\n\nlemon\nexperimental\n1\n1\n3.0\n\n\nlime\nexperimental\n1\n2\n1.5"
  },
  {
    "objectID": "lessons/127-test-data.html#do-you-calculations-two-different-ways",
    "href": "lessons/127-test-data.html#do-you-calculations-two-different-ways",
    "title": "25  Checking your work",
    "section": "\n25.8 Do you calculations two different ways",
    "text": "25.8 Do you calculations two different ways\nSuppose we weren’t really confident that we knew what mean does? Even simple functions like sd (for standard deviation) and median might not do what you think they should. How can we check it? Let’s write our own function based on sum, length, and division.\n\nmy_mean = function(x) sum(x) / length(x)\nmy_jelly |&gt; group_by(flavour, \n                      group, \n                      accuracy) |&gt;\n  summarize(count = length(flavour),\n            mean_reaction_time = my_mean(reaction_s),\n            .groups = \"drop\") |&gt;\n  kable() |&gt; kable_styling(full_width = FALSE)\n\n\n\nflavour\ngroup\naccuracy\ncount\nmean_reaction_time\n\n\n\nlemon\ncontrol\n1\n1\n4.0\n\n\nlemon\nexperimental\n1\n1\n3.0\n\n\nlime\nexperimental\n1\n2\n1.5\n\n\n\n\n\nExcellent! We got the same answers.\nThat’s an introduction to testing calculations. These are good checks to make as you calculate and test your analysis. The next step is to write computer code that checks your answers as well, by giving sample data to your calculation code along with the known answers. There is a package (of course!) for helping with that: testthat and tinytest (links below). We won’t be using these in the course, but you should take time to learn about them after the course."
  },
  {
    "objectID": "lessons/127-test-data.html#at-the-end-of-your-analysis",
    "href": "lessons/127-test-data.html#at-the-end-of-your-analysis",
    "title": "25  Checking your work",
    "section": "\n25.9 At the end of your analysis",
    "text": "25.9 At the end of your analysis\n\nProvide a relatively simple dataset together with analysis results that can be used to verify your code is working the same way in the future, or that someone who develops a new way of analysing the data can use for comparison\nDocument any testing processes you used for your data and for your computer code so that later users will know what problems you looked for\nDocument any problems you found in the data and what steps you took to fix the problems\nDescribe any weaknesses in your method which anticipate might cause problems for future users"
  },
  {
    "objectID": "lessons/127-test-data.html#lessons-for-this-course",
    "href": "lessons/127-test-data.html#lessons-for-this-course",
    "title": "25  Checking your work",
    "section": "\n25.10 Lessons for this course",
    "text": "25.10 Lessons for this course\nWe are not integrating testing into our work in this course. This lesson is here to alert you to this problem, ensure you know many people think carefully about this, and to show you the first steps to developing a good quality assurance plan. Be aware that testing takes time – possibly as much time as “the rest” of the work you do for data analysis."
  },
  {
    "objectID": "lessons/127-test-data.html#further-reading",
    "href": "lessons/127-test-data.html#further-reading",
    "title": "25  Checking your work",
    "section": "\n25.11 Further reading",
    "text": "25.11 Further reading\n\n\nData cleaning overview\n\npointblank documentation and examples\n\ndlookr documentation and examples\n\ndataMaid documentation for data cleaning\nFor testing R code, look at tinytest and testthat"
  },
  {
    "objectID": "lessons/155-dynamics-graphics.html#reader-interactions",
    "href": "lessons/155-dynamics-graphics.html#reader-interactions",
    "title": "26  Dynamic graphics",
    "section": "\n26.1 Reader interactions",
    "text": "26.1 Reader interactions\nVisualizations that are designed to respond to interactions (clicks, mouse movement) from the reader can show additional detail and provide opportunities for new ways of communication that are not possible with a static visualization. If you, as a creator of a visualization, require the reader to interact with your work to get the full meaning, you may find that your message is not communicated as effectively.\nWe will use the javascript plotly package, which is easily combined with ggplot graphics, to create some simple interactive visualizations."
  },
  {
    "objectID": "lessons/155-dynamics-graphics.html#example",
    "href": "lessons/155-dynamics-graphics.html#example",
    "title": "26  Dynamic graphics",
    "section": "\n26.2 Example",
    "text": "26.2 Example\nThe javascript package plot_ly can be used with R, both in Rstudio and in knitted HTML output. It automatically shows the coordinates of any point you aim your pointer at. It also provides the ability to pan and zoom the plot to focus in on part of the data. When you mouse is over the plot area, a menu appears at the top of the graph. Experiment with each control until you know what they all do.\nplot_ly is a bit slow with a large dataset, so I’ll use a subset of the diamonds data.\n\ndiamonds %&gt;% slice_sample(n = 1000) %&gt;%\n  plot_ly(x = ~ carat, \n          y = ~ price) %&gt;%\n  add_markers(color = ~ color)\n\n\n\n\n\nPlot_ly can do a lot more than this example shows, including animation. There is a lot of documentation you can find at the link under Further Reading."
  },
  {
    "objectID": "lessons/155-dynamics-graphics.html#animations",
    "href": "lessons/155-dynamics-graphics.html#animations",
    "title": "26  Dynamic graphics",
    "section": "\n26.3 Animations",
    "text": "26.3 Animations\nLet’s recreate a version of Rosling’s plot of life expectancy as a function of income over time.\nFirst we’ll make the plot for just one year. Then we will animate the plot changing year over time in the animation.\n\ngapminder %&gt;% \n  filter(year == 1957) %&gt;%\n  ggplot() +\n  geom_text(aes(label = min(year)), x = 3.8, y = 50, size = 40, color = \"lightgray\") +\n  geom_point(aes(x = gdpPercap,\n             y = lifeExp,\n             size = pop,\n             color = continent)) +\n  theme_bw() +\n  scale_x_log10()\n\n\n\n\nNow we will use functions from the gganimate package to change year from one frame to the next. Here we use transition_state to change the plot along values of year. The animation is produced with just one function: transition_time(year) which uses the quantitative value of year to control the display of data over time. The code to show the year in the geom_text is not at all obvious.\nThe results are not shown here, but this code should work in Rstudio.\n\nanimation &lt;- gapminder %&gt;% \n  ggplot() +\n  geom_text(aes(label = format(round(year))), \n            x = 3.8, y = 50, \n            size = 40, color = \"lightgray\") +\n  geom_point(aes(x = gdpPercap,\n             y = lifeExp,\n             size = pop,\n             color = continent)) +\n  theme_bw() +\n  scale_x_log10() +\n  transition_time(year) +\n  labs(title = \"Animation frame {frame} of {nframes}\")\nanimate(animation, nframes = 75)"
  },
  {
    "objectID": "lessons/155-dynamics-graphics.html#further-reading",
    "href": "lessons/155-dynamics-graphics.html#further-reading",
    "title": "26  Dynamic graphics",
    "section": "\n26.4 Further reading",
    "text": "26.4 Further reading\n\nDocumentation for plot_ly graphics for R\nDocumentation for gganimate, the grammar of animations"
  },
  {
    "objectID": "lessons/140-mapping.html#vector-map",
    "href": "lessons/140-mapping.html#vector-map",
    "title": "27  Making maps",
    "section": "\n27.1 Vector map",
    "text": "27.1 Vector map\nHere is a map of the 48 continental US states, with a quantiative variable used to shade each region. To change the variable used to colour the states, simply provide a new dataset with a numeric column and a text column called “state”. The map is drawn with ggplot, so the other features of ggplot including annotation, setting colour scales, labelling axes, etc., are all available to you and work the same way as for other visualizations we have created.\n\nstates_map &lt;- map_data(\"state\")\ncrimes &lt;- as_tibble(USArrests, rownames=\"state\") |&gt; mutate(state = tolower(state))\nggplot(crimes, aes(map_id = state)) +\n    geom_map(aes(fill = Murder), map = states_map) +\n    expand_limits(x = states_map$long, y = states_map$lat) +\n  coord_map(\"albers\", 40, 100)\n\n\n\n\nThe map_data function works with maps from the maps package, including two world maps (world and world2) and detailed maps of France, Italy, New Zealand, the USA and its states. The maps package has several other datasets including a list of canadian cities with population greater than about 1000. The world is a large and complex place and you will often need to obtain data and map boundaries for regions which are not readily available in this package. Some guidance appears at the end of the lesson, but this can be a challenging task.\nThe surface of the Earth is curved, so choices need to be made when plotting it on a flat surface. These choices are called projections. Here’s a map of France using an azimuthal equal area projection (see mapproj::mapproject() for more)\n\nmap_data &lt;- map_data('france') \nggplot(data = map_data, aes(group = group, map_id=region)) + \n       geom_map(map = map_data,\n                  aes(x = long, y = lat),\n                  fill = \"white\", colour = \"#7f7f7f\", alpha = 0.5, size=0.5)  +\n  coord_map(\"azequalarea\")\n\n\n\n\nIncidentally, a frequently used projection for the USA is the Bonne. Revise the USA map to use that projection by adding the following code + coord_map(\"bonne\", 45). (Albers: coord_map(\"albers\", 40, 100) and Lambert: coord_map(\"lambert\", 40, 100) are also used, although Lambert makes the USA look very wide in the North.)\nPolitical boundaries for the world are available as “world” (centered on the Atlantic Ocean) or “world2” (centered on the Pacific Ocean.)\n\nWorldData &lt;- map_data('world')  \nggplot(WorldData, aes(map_id=region)) + \n  geom_map(map = WorldData,\n           aes(x = long, y = lat),\n                  fill = \"lightgray\", colour = \"#7f7f7f\", alpha = 0.5, size=0.5) +\n  theme_bw()\n\nWarning in geom_map(map = WorldData, aes(x = long, y = lat), fill =\n\"lightgray\", : Ignoring unknown aesthetics: x and y\n\n\n\n\n\nYou can select a specific country if you want, for example, Canada:\n\nmy_map &lt;- map_data('world', region='Canada')  \nggplot(my_map, aes(map_id=region)) + \n  geom_map(map = my_map,\n           aes(x = long, y = lat),\n                  fill = \"white\", colour = \"#7f7f7f\", alpha = 0.5, size=0.5) +\n  theme_bw() +\n  coord_map(\"albers\", 60, 90)\n\nWarning in geom_map(map = my_map, aes(x = long, y = lat), fill = \"white\", :\nIgnoring unknown aesthetics: x and y\n\n\n\n\n\nThis map of Canada is made of 141 regions, separating islands, but does not contain provincial boundaries.\n\nggplot(my_map, aes(map_id=region)) + \n  geom_map(map = my_map,\n           aes(x = long, y = lat),\n           size=0.5) +\n  theme_bw() +  coord_map(\"albers\", 60, 90)\n\nWarning in geom_map(map = my_map, aes(x = long, y = lat), size = 0.5): Ignoring\nunknown aesthetics: x and y\n\n\n\n\n\nHere is a list of 252 regions available in the world map (abbreviated here).\n\nWorldData |&gt; pull(region) |&gt; unique() |&gt; head()\n\n[1] \"Aruba\"       \"Afghanistan\" \"Angola\"      \"Anguilla\"    \"Albania\"    \n[6] \"Finland\"    \n\n\nThe map of the globe and even of Canada does not look good at high latitudes, espcially if either pole is included. Here is a projection that is a bit more suitable for those regions. The geom_map function is not perfect; it creates stray lines when a region is clipped by the projection.\n\np1 &lt;- ggplot(WorldData, aes(group=group, map_id=region)) + \n  geom_map(map = WorldData, aes(long, lat),\n                  fill = \"gray80\", colour = \"#7f7f7f\", alpha = 0.5, size=0.5) +\n  labs(x=\"\", y=\"\") + theme_bw() +\n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(), \n        rect = element_blank())\n\nWarning in geom_map(map = WorldData, aes(long, lat), fill = \"gray80\", colour =\n\"#7f7f7f\", : Ignoring unknown aesthetics: x and y\n\np2a &lt;- p1 + coord_map(\"perspective\", 2.5, \n                      orientation=c(60, -100, 0))\n      # observer distance 2.5 Earth radii \np2b &lt;- p1 + coord_map(\"perspective\", 2.5, \n                      orientation=c(-60, 80, 0)) \n      # try also orthographic, with no observer distance\np2a + p2b"
  },
  {
    "objectID": "lessons/140-mapping.html#detailed-maps-of-canada",
    "href": "lessons/140-mapping.html#detailed-maps-of-canada",
    "title": "27  Making maps",
    "section": "\n27.2 Detailed maps of Canada",
    "text": "27.2 Detailed maps of Canada\nDetailed maps of Canada are not part of the maps package in R, so we need to do a bit of extra work. I will show you how to obtain map files called “shapefiles” from Statistics Canada and learn to use them with R. This is well worth learning as many maps are distributed in this format after being developed with GIS software. You don’t need to repeat these steps unless you want to make your own custom maps of Canada.\nHere are some packages we need to get the job done.\n\nlibrary(sf) # the base package manipulating shapes\n# library(rgdal) # geo data abstraction library # replaced with sf, stars, terra\n# See https://r-spatial.org/r/2022/04/12/evolution.html\nlibrary(geojsonio) # geo json input and output\n# library(spdplyr) # the `dplyr` counterpart for shapes # archived, don't use\nlibrary(rmapshaper) # the package that allows geo shape transformation\n\nI followed these instructions to get and transform the shape files for 2011 census divisions in Canada. There are many other options. The 2011 files are marked as archived content, but I had trouble with the 2016 files. Choose ARCGis .shp file format. Pick the cartographic boundary file. You should get a zip file called gpr_000b11a_e.\nThe shapefiles are very detailed and need to be simplified before being plotted with R. Here is some code to simplify these very detailed shapefiles down to simpler maps. This processing took about two hours on my computer, so when we are done we store the results in a file to enable rapid reuse.\ncanada_raw = readOGR(dsn = \"~/Downloads/gcd_000b11a_e\", layer = \"gcd_000b11a_e\", encoding = 'latin1') \ncanada_raw_json &lt;- geojson_json(canada_raw)  # takes a few minutes\ncanada_raw_sim &lt;- ms_simplify(canada_raw_json) # also takes a few minutes\ngeojson_write(canada_raw_sim, file = \"static/canada_cd_sim.geojson\") \n\n27.2.1 Draw the map\nNow that the preliminary work is done we can read the map data into R and get to mapmaking.\n\ncanada_cd &lt;- st_read(\"static/canada_cd_sim.geojson\", quiet = TRUE) \n\nThe official projection for maps of Canada is the Lambert conformal conic (“lcc”) with the following latitude and longitude parameters.\n\ncrs_string = \"+proj=lcc +lat_1=49 +lat_2=77 +lon_0=-91.52 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\n\nHere is a theme for the map to remove axes, ticks, borders and more.\n\ntheme_map &lt;- function(base_size=9, base_family=\"\") { \n    # require(grid)\n    theme_bw(base_size=base_size, base_family=base_family) %+replace%\n        theme(axis.line=element_blank(),\n              axis.text=element_blank(),\n              axis.ticks=element_blank(),\n              axis.title=element_blank(),\n              panel.background=element_blank(),\n              panel.border=element_blank(),\n              panel.grid=element_blank(),\n              panel.spacing=unit(0, \"lines\"),\n              plot.background=element_blank(),\n              legend.justification = c(0,0),\n              legend.position = c(0,0)\n        )\n}\n\nNow we draw the map with some colours from a palette.\n\nmap_colors &lt;- RColorBrewer::brewer.pal(9, \"Pastel1\") |&gt; rep(2)   # 18 colours, 9 repeated\n\nggplot() +\n    geom_sf(aes(fill = PRUID), color = \"gray60\", size = 0.1, data = canada_cd ) +\n    coord_sf(crs = crs_string) + # 6\n  scale_fill_manual(values = map_colors) +\n    guides(fill = FALSE) +\n    theme_map() +\n    theme(panel.grid.major = element_line(color = \"white\"))\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\nIf we have a list of latitudes and longitudes, we can add points to the map.\n\ncity_coords &lt;- tribble( ~city, ~lat, ~long,\n\"Vancouver\",    49.2827,    -123.1207,\n\"Calgary\",  51.0447,    -114.0719,\n\"Edmonton\", 53.5461,    -113.4938,\n\"Toronto\",  43.6532,    -79.3832,\n\"Ottawa\",   45.4215,    -75.6972,\n\"Montreal\", 45.5017,    -73.5673,\n\"Halifax\", 44.6488, -63.5752)\n\nConvert the latitude and longitudes to map coordinates.\n\nsf_cities = city_coords |&gt;\n    select(long, lat) |&gt; # 1\n    as.matrix() |&gt; # 2\n    st_multipoint(dim = 'XY') |&gt; \n    st_sfc() |&gt; \n    st_set_crs(4269)\n\nMake the map with projected points.\n\nggplot() +\n    geom_sf(aes(fill = PRUID), color = \"gray60\", size = 0.1, data = canada_cd) +\n    geom_sf(data = sf_cities, color = '#001e73', alpha = 0.5, size = 3) + # 17\n    coord_sf(crs = crs_string) +\n    scale_fill_manual(values = map_colors) +\n    guides(fill = FALSE) +\n    theme_map() +\n    theme(panel.grid.major = element_line(color = \"white\"),\n          legend.key = element_rect(color = \"gray40\", size = 0.1))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nIt’s easy to focus in on the Maritimes region of Canada – just filter the data to include only the province or census districts you want. Here are some of the names in the map data that you can use for filtering.\n\ncanada_cd |&gt; pull(PRNAME) |&gt; unique()\n\n [1] \"Manitoba\"                                           \n [2] \"British Columbia / Colombie-Britannique\"            \n [3] \"Alberta\"                                            \n [4] \"Saskatchewan\"                                       \n [5] \"Ontario\"                                            \n [6] \"Quebec / Québec\"                                    \n [7] \"Newfoundland and Labrador / Terre-Neuve-et-Labrador\"\n [8] \"Nova Scotia / Nouvelle-Écosse\"                      \n [9] \"New Brunswick / Nouveau-Brunswick\"                  \n[10] \"Prince Edward Island / Île-du-Prince-Édouard\"       \n[11] \"Northwest Territories / Territoires du Nord-Ouest\"  \n[12] \"Nunavut\"                                            \n[13] \"Yukon\"                                              \n\ncanada_cd |&gt; pull(CDNAME) |&gt; unique() |&gt; sample(10)\n\n [1] \"Division No. 18\"   \"La Vallée-de-l'Or\" \"Les Laurentides\"  \n [4] \"Region 1\"          \"Maskinongé\"        \"Charlotte\"        \n [7] \"Matane\"            \"Peace River\"       \"Matawinie\"        \n[10] \"Gatineau\"         \n\n\nI’ve changed the latitude and longitude parameters for the projection to values more suitable for this part of Canada.\n\ncrs_string2 = \"+proj=lcc +lat_1=40 +lat_2=50 +lon_0=-75 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\nggplot() +\n    geom_sf(aes(fill = PRUID), color = \"gray60\", size = 0.1, \n            data = canada_cd |&gt; filter(PRNAME %in% c(\"Nova Scotia / Nouvelle-Écosse\", \"New Brunswick / Nouveau-Brunswick\", \"Prince Edward Island / Île-du-Prince-Édouard\"))) +\n    # geom_sf(data = sf_cities, color = '#001e73', alpha = 0.5, size = 3) + # 17\n    coord_sf(crs = crs_string2) +\n    scale_fill_manual(values = map_colors) +\n    guides(fill = FALSE) +\n    theme_map() +\n    theme(panel.grid.major = element_line(color = \"white\"),\n          legend.key = element_rect(color = \"gray40\", size = 0.1))\n\n\n\n\nYou can also crop the data to be drawn or the projected map. This is a bit more complex than you might expect since you need to be sure you are specifying the area to be plotted and the actual projected coordinates in the same coordinate system. You can easily get errors (invalid points in a projection), empty maps, or croppings that don’t look right. See the link at the start of this paragraph for several approaches.\n\ncrs_string2 = \"+proj=lcc +lat_1=40 +lat_2=50 +lon_0=-75 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\nzoom_to &lt;- c(-64.3683, 45.8979) # Sackville, New Brunswick; could try c(-63.5752, 44.6488)  # Halifax\nzoom_level &lt;- 6\nC &lt;- 40075016.686   # ~ circumference of Earth in meters\nx_span &lt;- C / 2^zoom_level\ny_span &lt;- C / 2^(zoom_level)\nzoom_to_xy &lt;- st_transform(st_sfc(st_point(zoom_to), crs = 4326),\n                           crs = crs_string2)\ndisp_window &lt;- st_sfc(\n    st_point(st_coordinates(zoom_to_xy - c(x_span / 2, y_span / 2))),\n    st_point(st_coordinates(zoom_to_xy + c(x_span / 2, y_span / 2))),\n    crs = crs_string2\n)\nggplot() +\n    geom_sf(aes(fill = PRUID), color = \"gray60\", size = 0.1, \n            data = canada_cd) +\n  # geom_sf(data = sf_cities, color = '#001e73', alpha = 0.5, size = 3) + # 17\n  geom_sf(data = zoom_to_xy, color = 'red') +\n  coord_sf(xlim = st_coordinates(disp_window)[,'X'],\n             ylim = st_coordinates(disp_window)[,'Y'],\n             crs = crs_string2, datum = crs_string2) +\n    scale_fill_manual(values = map_colors) +\n    guides(fill = FALSE) +\n    theme_map() +\n    theme(panel.grid.major = element_line(color = \"white\"),\n          legend.key = element_rect(color = \"gray40\", size = 0.1))\n\n\n\n\n\n27.2.2 Just the map, please\nHere is a file of province boundaries that can be used without showing the census district regions. I’ve added a latitude-longitude grid to the map.\n\ncanada_prov &lt;- geojson_read(\"https://gist.githubusercontent.com/mikelotis/2156d7c170d10d2c77cb79424fe2137d/raw/7a13748ed7ea5ba64876c77c53b6cb64dd5c3ab0/canada-province.geojson\", what=\"sp\") |&gt; st_as_sf()\n\nggplot() +\n    geom_sf(aes(fill = name), \n            color = \"gray60\", size = 0.1, \n            data = canada_prov ) +\n    coord_sf(crs = crs_string) + \n  scale_fill_manual(values = map_colors) +\n    guides(fill = FALSE) +\n    theme_map() +\n    theme(panel.grid.major = element_line(color = \"lightgray\"),\n          panel.grid.minor = element_line(color = \"lightgray\")) +\n  scale_x_continuous(breaks = seq(-160, 0, 10)) +\n  scale_y_continuous(breaks = seq(40, 85, 5))"
  },
  {
    "objectID": "lessons/140-mapping.html#summary",
    "href": "lessons/140-mapping.html#summary",
    "title": "27  Making maps",
    "section": "\n27.3 Summary",
    "text": "27.3 Summary\nMap making is complex for at least two reasons: obtaining the data to describe complex political boundaries and using suitable projections for your data. This lesson introduced you to some simple solutions to both problems and gave a starting point for learning more about the complexity of making customized maps."
  },
  {
    "objectID": "lessons/140-mapping.html#further-reading",
    "href": "lessons/140-mapping.html#further-reading",
    "title": "27  Making maps",
    "section": "\n27.4 Further reading",
    "text": "27.4 Further reading\n\nWilke\nHealy\nMapping example in our MDS lesson.\nA collection of maps from a 30 day challenge\n\nMaps using ggmap + openstreetmaps\n\nThe PROJ tool for projecting map coordiates\nR journal article on mapping with mapmisc\n\nFinding and fixing problems with simple feature geometry and a book, especially section 8.4\nThere are a huge number of map projections. Here is an introduction to some of the kinds of projections.\n\n\n27.4.1 Maps of Canada\n\nThe blog I used as source material for the detailed maps of Canada\nProvince and census division shape files from Statistics Canada\n\nElection data for boundaries and chloropleths and populated weighted chloropleths\nA map showing wind turbines in Maritimes"
  },
  {
    "objectID": "lessons/141-mapping-2.html#tiled-maps-with-leaflet",
    "href": "lessons/141-mapping-2.html#tiled-maps-with-leaflet",
    "title": "28  Mapping II",
    "section": "\n28.1 Tiled maps with leaflet",
    "text": "28.1 Tiled maps with leaflet\nIn the previous lesson we made maps to show geographical data using map outlines from R packages. In this lesson we will look at making maps that build on detailed basemaps from online mapping systems. Google maps is a well known service; OpenStreetMap is a community-built free alternative. We will use the leaflet package.\nLet’s make a map and place a marker at the location of the Mathematics and Statistics Department. You can pan and zoom this map from within Rstudio and in the knitted output.\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-63.5932, lat=44.63697, popup=\"Math & Stats, Dalhousie U\")\nm  # Print the map\n\n\n\n\n\nLet’s use the city database we used in the MDS lesson to label the 61 cities we selected in that lesson.\n\ncities &lt;- read_csv(\"static/selected_cities.csv\", show_col_types = FALSE)\nm2 &lt;- leaflet(data = cities) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~ lng, ~lat, label = ~city)\nm2\n\n\n\n\n\nThere are several different basemaps, or tiles, that you can use. A fun one to look at – because it changes frequently is a radar map for the USA from Iowa State.\n\nm3 &lt;- leaflet(data = cities) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~ lng, ~lat, label = ~city) %&gt;%\n  addWMSTiles(\"http://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi\",\n    layers = \"nexrad-n0r-900913\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    attribution = \"Weather data © 2012 IEM Nexrad\") %&gt;%\n  setView(-93.65, 42.0285, zoom = 4)\nm3\n\n\n\n\n\nIt’s common to see “chloropleth” maps of the USA in which geographic regions are coloured to show a quantitative or categorical variable, such as population density or election results. These maps are less common in Canada due to two main factors: the geographic concentration of the population in cities means that area maps are even less informative than for the USA and the latitude range of Canada makes for some severe challenges when plotting whole-country territory and province chloropleth maps on commonly used projections. Nevertheless, it is possible. Here’s a demonstration with some random numbers for “data”.\n\nlibrary(geojsonio)\npal &lt;- colorNumeric(\"viridis\", NULL)  # make a viridis palette\ncanada &lt;- geojson_read(\"https://gist.githubusercontent.com/mikelotis/2156d7c170d10d2c77cb79424fe2137d/raw/7a13748ed7ea5ba64876c77c53b6cb64dd5c3ab0/canada-province.geojson\", what=\"sp\")\nmy_values = runif(13, 0, 10) # random numbers between 0 and 10\ncanada %&gt;% \n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(fillColor = ~ pal(my_values)) %&gt;%\n  addLegend(pal = pal, values = my_values, opacity = 1.0)"
  },
  {
    "objectID": "lessons/141-mapping-2.html#tiled-map-with-ggmap",
    "href": "lessons/141-mapping-2.html#tiled-map-with-ggmap",
    "title": "28  Mapping II",
    "section": "\n28.2 Tiled map with ggmap",
    "text": "28.2 Tiled map with ggmap\nIn this section we will use tile maps from the openstreetmap database rendered by Stamen Maps. We will use the ggmap package to download and draw the map.\nThere are two steps. First download the tiles needed for your map by specifying\n\nthe bounding box in degrees latitude and longitude,\nthe zoom level of the map, and\nthe type of the tiles (terrain, terrain-background, etc. See the help for get_stamenmap for more.)\n\nAlways start with a small zoom level – if you make it too big you will download a lot of unnecessary data, which takes time and imposes a burden on the service.\nFIX ME\n\n# mymap_terrain &lt;- get_stamenmap(bbox = c(left = -130, bottom = 41, right = -50, top = 60),\n#                                zoom=4, maptype = \"terrain\")\n# ggmap(mymap_terrain)  \n\nThis is a regular ggplot, so you can add lines and points to a map easily, using latitude and longitude as coordinates. Placing text on a map in a readable and attractive way can be quite a challenge.\n\nmy_points &lt;- tibble(lat = c(43+57/60, 49+53/60),\n                    lon = c(-59-55/60, -97-9/60),\n                    label = c(\"Sable Is.\", \"Winnipeg\")\n)\n# ggmap(mymap_terrain)  +\n#   geom_point(data =my_points, color=\"brown\") + \n#   geom_text(data = my_points, aes(label=label))\n\nYou can also zoom in on urban areas.\n\n# mymap_halifax &lt;- get_stamenmap(bbox = c(left = -63.6-0.1, bottom = 44.60, right = -63.6+0.1, top = 44.75), \n#                                zoom=12, maptype = \"toner-lite\")\n# ggmap(mymap_halifax)"
  },
  {
    "objectID": "lessons/141-mapping-2.html#further-reading",
    "href": "lessons/141-mapping-2.html#further-reading",
    "title": "28  Mapping II",
    "section": "\n28.3 Further reading",
    "text": "28.3 Further reading\n\nHealy’s chapter 7 on mapping\n\nWilke’s chapter 15 on geospatial data\n\nAn article describing how ggmap works\nA ggmap tutorial, showing how to use Google map services including geocoding (turning an address into a location and the reverse) and routing.\nAnother well-developed approach to mapping: mapview"
  },
  {
    "objectID": "lessons/142-mapping-3.html#heatmaps",
    "href": "lessons/142-mapping-3.html#heatmaps",
    "title": "29  Alternatives to maps",
    "section": "\n29.1 Heatmaps",
    "text": "29.1 Heatmaps\nSuppose we want to show a combination of spatial and temporal data. The geographic data is important, but the location on a map is not the most important feature. As an example, consider this data on total cases of COVID-19 in 5 regions of Canada reported on 2021-01-21. In the code below I select the number of cases identified broken down into age classes and geographic regions. The number of cases is presented as a shade of blue using geom_tile.\n\ncovid &lt;- cansim::get_cansim(\"13-10-0775-01\")\n\nAccessing CANSIM NDM product 13-10-0775 from Statistics Canada\n\n\nParsing data\n\n# covid2 &lt;- get_cansim(\"13-10-0774-01\")  # related data with extra information\ncovid_ss &lt;- covid |&gt; filter(GEO != \"Canada\", Gender == \"Total, all genders\", `Age group` != \"Total, all age groups\") |&gt;\n  filter(Statistics == \"Community exposures\") \ncovid_ss |&gt;\n  ggplot(aes(y = GEO, x = `Age group`, fill = VALUE)) + \n  geom_tile() +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nThere are two common ways to improve a figure like this. First is to ensure the ordering of categorical variables is easily interpreted. Here we would remove hte “Not stated” age group and order the geographic regions from west to east to follow Canadian convention. We might swap the x and y axes to align left-right flow with the west-east geography of Canada.\n\ncovid_ss2 &lt;- covid_ss |&gt; filter(`Age group` != \"Not stated, age group\")\nregions &lt;- c(\"BC + Yukon\", \"Prairies + NWT\",\n             \"ON + NU\", \"QC\", \"Atlantic\")\ncovid_ss3 &lt;- covid_ss2 |&gt;\n  mutate(GEO = fct_recode(GEO, \"BC + Yukon\" = \"British Columbia and Yukon Region\",\n              \"Prairies + NWT\" = \"Prairies and Northwest Territories Region\",\n              \"ON + NU\" = \"Ontario and Nunavut Region\",\n             \"QC\"  =  \"Quebec Region\", \n             \"Atlantic\" = \"Atlantic Region\"))\ncovid_ss3 |&gt;\n  ggplot(aes(x = fct_relevel(GEO, regions),\n             y = `Age group`, fill = VALUE)) + \n  geom_tile() +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nA second approach is to group the rows and columns so that similar patterns are side by side. The ggheatmap function in the ggheatmap package makes this easy.\n\ncovid_ss4 &lt;- covid_ss3 |&gt; select(GEO, `Age group`, VALUE) |&gt;\n  pivot_wider(names_from = GEO, values_from = VALUE) \ndata.frame(covid_ss4 |&gt; select(-`Age group`), \n              row.names = covid_ss4 |&gt; pull(`Age group`)) |&gt;\n  ggheatmap()\n\n\n\n\nFrom this analysis you can tell that the number of cases is structured by age (see the tree on left side of the plot) and geographically. The number of people in each region is very different, so you might want to account for that in some way. One approach would be to compute the cases per capita in each region, but I don’t have those data handy. An alternative is to allocate the number of cases to age classes as proportions, by dividing the number of cases in each region and age group by the total in each region. We can use scale to accomplish that scaling. Now the color represents the proportion of COVID-19 cases in each age class, scaled within a region.\n\ndata.frame(covid_ss4 |&gt; select(-`Age group`), \n              row.names = covid_ss4 |&gt; pull(`Age group`)) %&gt;%\n  scale(center = FALSE, scale = colSums(.)) |&gt;\n  as.data.frame() |&gt;\n  ggheatmap()\n\n\n\n\nMatching the previous visualization, we see that the proportion of cases is higher in people age 60 and older compared to younger groups. We now also see that the highest proportion of cases (about 20% or more) was in inviduals aged 80 and over in Quebec and in the 20-29 age group in BC and Yukon. The distribution of cases across ages was most similar between Ontario + Nunavut and BC + Yukon."
  },
  {
    "objectID": "lessons/142-mapping-3.html#specialized-maps-of-the-usa",
    "href": "lessons/142-mapping-3.html#specialized-maps-of-the-usa",
    "title": "29  Alternatives to maps",
    "section": "\n29.2 Specialized maps of the USA",
    "text": "29.2 Specialized maps of the USA\nCustomization of any visualization is always possible, but takes time to develop. Maps of the USA are very common, so a huge number of variations have been developed. Here is a kind of heatmap: each state is the same size and the states are arranged in an approximate geographic representation of the USA. (Example adapted from Healy Section 7.3.)\n\nlibrary(statebins)\nload(\"static/election.rda\")\nelection |&gt; ggplot(aes(state = state, fill = pct_trump)) +\n  geom_statebins() +\n  theme_statebins() +\n  labs(fill=\"Percent Trump\")"
  },
  {
    "objectID": "lessons/142-mapping-3.html#summary",
    "href": "lessons/142-mapping-3.html#summary",
    "title": "29  Alternatives to maps",
    "section": "\n29.3 Summary",
    "text": "29.3 Summary\nMaps are commonly used in data visualization to display data (quantitative or categorical) over a geographical area, when the geography will provide insight into patterns in the data. Maps can be misleading because the area of the region (province, state, country) may have little to do with the message you want to communicate, but the area on a data visualization usually has greater impact than the color hue or brightness which is used to encode the primary data.\nFor these reasons, you should consider alternatives to maps even when your data are clearly geographic. All the visualizations we have used in this course that permit a categorical variable can be useful. In this lesson we added one more, the heatmap, which is like a map, except all the regions are the same area and the “geographic” trend is arranged along one axis."
  },
  {
    "objectID": "lessons/142-mapping-3.html#further-reading",
    "href": "lessons/142-mapping-3.html#further-reading",
    "title": "29  Alternatives to maps",
    "section": "\n29.4 Further reading",
    "text": "29.4 Further reading\n\nHealy (Healy 2018) Chapter 7 Maps\n\n\n\n\n\n\n\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press. https://socviz.co/."
  },
  {
    "objectID": "lessons/130-bonus-topics.html#working-with-text",
    "href": "lessons/130-bonus-topics.html#working-with-text",
    "title": "30  Working with text, factors, dates and times",
    "section": "\n30.1 Working with text",
    "text": "30.1 Working with text\nWhen making visualizations, you will often want to manipulate text strings before displaying them. Sometimes this is to simplify text that is being displayed on a graph. Or perhaps there is a typographical error or formatting problem with text. The stringr package contains many useful functions for manipulating text strings. I will explain a few simple examples that I use frequently.\n\nA very common task is to remove leading, trailing, and duplicated spaces in a string. The str_squish function removes whitespace (spaces, tabs, and new lines) in these positions.\n\n\nmy_string &lt;- \"   A cat  is a small \n                    and furry animal.  \"\nmy_string2 &lt;- str_squish(my_string)\nmy_string2\n\n[1] \"A cat is a small and furry animal.\"\n\n\n\nIt can be helpful to convert all text to a uniform pattern of capitalization: all capital letters, all lower case, and other patterns. Standardization is important for aesthetic reasons and to avoid splitting categories because of unimportant spelling differences.\n\n\nstr_to_lower(my_string2)\n\n[1] \"a cat is a small and furry animal.\"\n\nstr_to_upper(my_string2)\n\n[1] \"A CAT IS A SMALL AND FURRY ANIMAL.\"\n\nstr_to_sentence(my_string2)\n\n[1] \"A cat is a small and furry animal.\"\n\nstr_to_title(my_string2)\n\n[1] \"A Cat Is A Small And Furry Animal.\"\n\n\n\nIf there are particular letters or symbols you want to remove, the str_remove function can accomplish this. You can match literal strings or on patterns. Patterns allow for the use of character classes, selecting specific sequences, and more complex symbolic descriptions of strings. I will give a couple of simple examples of patterns. str_remove matches only once; str_remove_all matches the pattern as many times as possible.\n\n\nstr_remove(my_string2, \"cat\")\n\n[1] \"A  is a small and furry animal.\"\n\nstr_remove_all(my_string2, \"[aeiou]\")\n\n[1] \"A ct s  smll nd frry nml.\"\n\nstr_remove_all(my_string2, \"[ ,\\\\.!]\")\n\n[1] \"Acatisasmallandfurryanimal\"\n\n\n(The . is a special pattern character representing any character; to match a literal . you need to write \\\\.)\n\nIf a variable contains numbers, but R is interpreting the data as text, you can use the function as.numeric to convert the text to numbers. Any string that can’t be interpreted as a number will be converted to NA.\n\n\ntext_and_numbers &lt;- tibble( text = c(\"Andrew\", \"33\", \"12.45\", \n                                     \"-1.00\", \"Inf\"))\ntext_and_numbers %&gt;% mutate(numbers = as.numeric(text), \n                            integers = as.integer(text)) %&gt;% kable()\n\n\n\ntext\nnumbers\nintegers\n\n\n\nAndrew\nNA\nNA\n\n\n33\n33.00\n33\n\n\n12.45\n12.45\n12\n\n\n-1.00\n-1.00\n-1\n\n\nInf\nInf\nNA\n\n\n\n\n\n\nIf a pattern appears in a string, you might want to extract that information. str_extract allows you to write a pattern that matches part of the string and extract that from the source material.\n\n\nsets &lt;- c(\"A1\", \"A2\", \"B1\", \"B4\", \"C5\")\nstr_extract(sets, \"[0-9]\")\n\n[1] \"1\" \"2\" \"1\" \"4\" \"5\"\n\nstr_extract(sets, \"[A-Z]\")\n\n[1] \"A\" \"A\" \"B\" \"B\" \"C\"\n\n\nThinking about patterns is a lot of work and prone to error, so the pair of functions glue and unglue were created to perform common tasks of combining text and data and then to separate them again.\n\nlibrary(glue)\nlibrary(unglue)\na &lt;- 1\nb &lt;- 6\nc &lt;- 15.63\nmy_string3 &lt;- glue(\"The numbers a, b, and c are {a}, {b}, and {c}, respectively. Their sum is {a+b+c}.\")\nmy_string3\n\nThe numbers a, b, and c are 1, 6, and 15.63, respectively. Their sum is 22.63.\n\nunglue(my_string3, \"The numbers a, b, and c are {a}, {b}, and {c}, respectively. Their sum is {d}.\")\n\n$`1`\n  a b     c     d\n1 1 6 15.63 22.63\n\nmy_strings1 &lt;- tibble(greeting = c(\"My name is Andrew.\", \n                                   \"My name is Li.\", \n                                   \"My name is Emily.\"))\nunglue_unnest(my_strings1, \n              greeting, \n              \"My name is {name}.\", \n              remove=FALSE) %&gt;% kable()\n\n\n\ngreeting\nname\n\n\n\nMy name is Andrew.\nAndrew\n\n\nMy name is Li.\nLi\n\n\nMy name is Emily.\nEmily"
  },
  {
    "objectID": "lessons/130-bonus-topics.html#working-with-factors",
    "href": "lessons/130-bonus-topics.html#working-with-factors",
    "title": "30  Working with text, factors, dates and times",
    "section": "\n30.2 Working with factors",
    "text": "30.2 Working with factors\nFactors are categorical variables in which a set of text labels are the possible values of a variable. These are sometimes interpreted as integers and sometimes interpreted as text. In data visualization, our primary concern is mapping factors on to sequence of colours, shapes, or locations on an axis. In R, if a factor is not given an explicit order by the analyst, but must have an order (on a scale), this is usually alphabetical. This ordering is rarely the best one for visualizations!\nThe forcats package has a series of functions for reordering factors. These can be used to explicitly reorder a factor by value (level) or a quantitative value can be used to reorder a factor.\nHere are a few examples using the mpg data set. First a visualization without any explicit reordering of factors. Notice the factors on the vertical axis are arranged alphabetically with the first one at the bottom of the axis (the order follows the usual increase along the y-axis since the factors are interpreted as the numbers 1, 2, 3, … on the visualization.)\n\nmpg %&gt;% ggplot(aes(x = cty,\n                   y = trans)) +\n  geom_boxplot()\n\n\n\n\nNext we reorder the transmission categorical variable according to the minimum value of highway fuel economy. The three arguments to fct_reorder are the categorical variable to be reordered, the quantitative variable to use for the reordering, and a function that converts a vector of numbers to a single value for sorting (such as mean, median, min, max, length). The smallest value is plotted on the left of the horizontal axis or the bottom of the vertical axis. The option .desc=TRUE (descending = TRUE) is an easy way to reverse the order of factors and is especially useful for the vertical axis.\n\nmpg %&gt;% ggplot(aes(x = cty,\n                   y = fct_reorder(trans, hwy, median, .desc=TRUE))) +\n  geom_boxplot() \n\n\n\n\nYou should to practice working with strings and factors to develop flexible methods of customizing your display of categorical variables.\nNext we extract the number of gears from the transmission and reorder transmission on this basis.\nFIX ME\n\nmpg %&gt;% unglue_unnest(trans, \"{trans_desc}({trans_code})\", \n                      remove=FALSE)  %&gt;%\n  mutate(gears = str_extract(trans_code, \"[0-9]\") %&gt;% as.numeric()) %&gt;%\n  ggplot(aes(x = cty,\n             # y = fct_reorder(trans, gears)\n             )) +\n  geom_boxplot() \n\n\n\n\nWhen there are too many cateogories to display on a graph, it can be helpful to pick out the ones with the most observations and to group the remaining observations together in an “other” category. Here’s how you can accomplish that.\n\nmpg %&gt;% \n  ggplot(aes(x = cty,\n             y = fct_lump(trans, 4))) +\n  geom_boxplot() \n\n\n\n\nYou can also use this function to keep the rare categories and lump the common ones, or group all categories appearing more or less often that some proportion of all observations. See the help for this function for details."
  },
  {
    "objectID": "lessons/130-bonus-topics.html#working-with-dates-and-times",
    "href": "lessons/130-bonus-topics.html#working-with-dates-and-times",
    "title": "30  Working with text, factors, dates and times",
    "section": "\n30.3 Working with dates and times",
    "text": "30.3 Working with dates and times\nDates and times are complex data to work with. Dates are represented in many formats. Times are reported in time zones, which change depending on the time of year and the location of the measurement. Dates are further complicated by leap years and local rules operating in specific countries. Special formatting is required for labelling dates and times on plots.\nThe package lubridate contains many functions to help you work with dates and times. For data visualization purposes I mostly use functions to parse dates and times (converting text to a date-time object), perform arithmetic such as subtracting dates to find the time difference, extract components of a date, and format axis labels.\nTo see some nicely formatted dates and times, use the today and now functions. When reporting times, you need to pick a time zone. This can be surprisingly complicated, especially since the time zone used in a particular location changes (daylight savings time) and according to changes in local regulations and legislation. For example, as I write these notes the “Atlantic/Halifax” time zone corresponds to AST (Atlantic Standard Time), but when you read the notes, the same time zone will be ADT (Daylight time.) We don’t use the three letter codes for time zone (except as an abbreviation when displaying the time), because there are multiple meanings for some three letter codes. Many people report time in UTC (referenced to longitude 0, Greenwich UK, but without the complexity of daylight savings time) to make times a bit easier. Of course, the date in UTC may not be the date where you are right now (it could be ‘yesterday’ or ‘tomorrow’), so be on the lookout for that!\n\ntoday()\n\n[1] \"2024-01-01\"\n\nnow() # for me this is: now(tz = \"America/Halifax\") \n\n[1] \"2024-01-01 16:20:28 AST\"\n\nnow(tz = \"UTC\")\n\n[1] \"2024-01-01 20:20:28 UTC\"\n\n\n\n30.3.1 Reading dates\nThere are a family of functions ymd, dmy, mdy, and ymd_hms among others that are used to turn text (such as in a table you read from a file) into a date. I strongly encourage the use of ISO 8601 date formatting. Illegal dates are converted to NA_Date and displayed as NA.\n\ndt1 &lt;- tibble(text_date = c(\"1999-01-31\", \"2000-02-28\", \"2010-06-28\",\n                            \"2024-03-14\", \"2021-02-29\"),\n             date = ymd(text_date))\n\nWarning: 1 failed to parse.\n\ndt1 %&gt;% kable()\n\n\n\ntext_date\ndate\n\n\n\n1999-01-31\n1999-01-31\n\n\n2000-02-28\n2000-02-28\n\n\n2010-06-28\n2010-06-28\n\n\n2024-03-14\n2024-03-14\n\n\n2021-02-29\nNA\n\n\n\n\n\nHere is an example with times. You can specify a time zone if you want, but sometimes you can get away with ignoring the problem. Here the timezone information tells the computer how to interpret the text representation of the time.\n\ndt2 &lt;- tibble(text_date = c(\"1999-01-31 09:14\", \"2000-02-28 12:15\",\n                            \"2010-06-28 23:45\", \n                            \"2024-03-14 07:00 AM\", \"2021-03-01 6:16 PM\"),\n             date_time = ymd_hm(text_date, tz=\"America/Halifax\"))\ndt2 %&gt;% kable()\n\n\n\ntext_date\ndate_time\n\n\n\n1999-01-31 09:14\n1999-01-31 09:14:00\n\n\n2000-02-28 12:15\n2000-02-28 12:15:00\n\n\n2010-06-28 23:45\n2010-06-28 23:45:00\n\n\n2024-03-14 07:00 AM\n2024-03-14 07:00:00\n\n\n2021-03-01 6:16 PM\n2021-03-01 18:16:00\n\n\n\n\n\nThese functions are remarkably powerful, for example they work on formats like this:\n\ntibble(date = c(\"Jan 5, 1999\", \"Saturday May 16, 70\", \"8-8-88\",\n               \"December 31/99\", \"Jan 1, 01\"),\n      decoded = mdy(date)) %&gt;% kable()\n\n\n\ndate\ndecoded\n\n\n\nJan 5, 1999\n1999-01-05\n\n\nSaturday May 16, 70\n1970-05-16\n\n\n8-8-88\n1988-08-08\n\n\nDecember 31/99\n1999-12-31\n\n\nJan 1, 01\n2001-01-01\n\n\n\n\n\nAs many people working in the late 20th century discovered, you should be very careful with two digit years. Best not to use them.\nIf you want to know how much time has passed since the earliest observation in a dataset, you can do arithmetic. Note the data types of each column (chr = character, date, time, dbl = double = numeric, drtn = duration).\n\ndt1 %&gt;% arrange(date) %&gt;%\n  mutate(elapsed = date - min(date, na.rm=TRUE),\n         t_days = as.numeric(elapsed))\n\n# A tibble: 5 × 4\n  text_date  date       elapsed   t_days\n  &lt;chr&gt;      &lt;date&gt;     &lt;drtn&gt;     &lt;dbl&gt;\n1 1999-01-31 1999-01-31    0 days      0\n2 2000-02-28 2000-02-28  393 days    393\n3 2010-06-28 2010-06-28 4166 days   4166\n4 2024-03-14 2024-03-14 9174 days   9174\n5 2021-02-29 NA           NA days     NA\n\n\nLet’s add some random data to the second table and make a scatter graph. Special codes are used to format dates and times, but these are fairly well standardized (see the help for strptime).\n\ndt2 %&gt;% mutate(r = rnorm(n(), 20, 3)) %&gt;%\n  ggplot(aes(x = date_time, y = r)) + \n  geom_point() +\n  scale_x_datetime(date_labels = \"%Y\\n%b-%d\")\n\n\n\n\nThere are lots more options for formatting date and time axes. See the help pages for more (in particular the examples, as always)."
  },
  {
    "objectID": "lessons/130-bonus-topics.html#working-with-missing-data",
    "href": "lessons/130-bonus-topics.html#working-with-missing-data",
    "title": "30  Working with text, factors, dates and times",
    "section": "\n30.4 Working with missing data",
    "text": "30.4 Working with missing data\nData are often missing. Missing data are encoded as NA in R, but occasionally you need to know a bit more than this. There are a few ways you can get tripped up with missing data.\n\n30.4.1 Reading from a file\nIf you read data from a csv or spreadsheet, an empty cell (and sometimes “NA”) will be interpreted as missing data. If some other value is used to represent NA, then you can use the option na = in read_csv or read_excel. (read_excel only converts blank values into NA unless you specify that the text ‘NA’ is a missing value and should be turned into NA.)\n\n30.4.2 Computations with NA\nAny arithmetic computation with an NA will result in an NA result.\n\n1 + NA\n\n[1] NA\n\nInf + NA\n\n[1] NA\n\nNA/0\n\n[1] NA\n\nlog(NA)\n\n[1] NA\n\n\nWhen we use functions that turn a vector into a single number (mean, min, median, etc.), sometimes we want to ignore missing values. This is because we have at least two different ways of thinking about missing values: missing values should be ignored versus missing values are important signals of data. The option na.rm= is useful here.\n\ndt3 &lt;- tibble(x = c(1, 5, 9, 14.5, NA, 21, NA))\ndt3 %&gt;% summarize(mean_with_NA = mean(x),\n                  mean_no_NA = mean(x, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_with_NA mean_no_NA\n         &lt;dbl&gt;      &lt;dbl&gt;\n1           NA       10.1\n\n\nIf you want to know the number of observations, non-missing or missing data, use n or the idiom sum(!is.na(...)) and sum(is.na(...)) The exclamation mark (!, sometimes called bang) means logical not so !is.na means not missing.\n\ndt3 %&gt;% summarize(n_with_NA = n(),\n                  n_no_NA = sum(!is.na(x)),\n                  n_is_NA = sum(is.na(x)))\n\n# A tibble: 1 × 3\n  n_with_NA n_no_NA n_is_NA\n      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1         7       5       2\n\n\nThere are special functions n_missing, n_complete and more in the skimr package but I sometimes forget these and use the sum(...) calculations above.\n\ndt3 %&gt;% summarize(n_with_NA = n(),\n                  n_no_NA = skimr::n_complete(x),\n                  n_is_NA = skimr::n_missing(x))\n\n# A tibble: 1 × 3\n  n_with_NA n_no_NA n_is_NA\n      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1         7       5       2\n\n\nIf you have missing data in one or more columns and want to remove all observations from a table that have missing data, you can use na.omit.\n\nna.omit(dt3)\n\n# A tibble: 5 × 1\n      x\n  &lt;dbl&gt;\n1   1  \n2   5  \n3   9  \n4  14.5\n5  21"
  },
  {
    "objectID": "lessons/130-bonus-topics.html#further-reading",
    "href": "lessons/130-bonus-topics.html#further-reading",
    "title": "30  Working with text, factors, dates and times",
    "section": "\n30.5 Further reading",
    "text": "30.5 Further reading\n\nA blog post about missing values and data types\n\nR 4 Data Science chapters on Strings, Factors, Dates and Times."
  },
  {
    "objectID": "lessons/201-colour.html#disciplinary-cultural-and-human-elements",
    "href": "lessons/201-colour.html#disciplinary-cultural-and-human-elements",
    "title": "31  Using colour well",
    "section": "\n31.1 Disciplinary, cultural, and human elements",
    "text": "31.1 Disciplinary, cultural, and human elements\nThere are existing conventions for the use of colour in many disciplines. For example, topographic maps have a consistent use of green-brown scales to show altitude. Temperature is often indicated on a blue-red scale for cold to warm. In more abstract or less commonly encountered settings, there may be no obvious scheme to use to indicate larger/smaller or positive/negative and then cultural patterns take over. In particular some people will want to use red for increases/positive values while others may want to use red to mean decreases/negative values. A good legend should always be included to make your meaning explicit. People have different preferences for colour, from liking bold, saturated colours to preferring muted, pastel colours. Usually extreme choices should be avoided to make your colour choices appealing to as wide an audience as possible. This is the motivation for using established colour scales that are already popular. Finally, you should be aware that a fraction of the population has some degree of colour vision deficiency. There are tools to screen for these design concerns, but an easy to remember guideline is to not use red and green on the same visualization."
  },
  {
    "objectID": "lessons/201-colour.html#how-will-the-colour-be-displayed",
    "href": "lessons/201-colour.html#how-will-the-colour-be-displayed",
    "title": "31  Using colour well",
    "section": "\n31.2 How will the colour be displayed?",
    "text": "31.2 How will the colour be displayed?\nYour visualizations will most likely be displayed in one of three ways: on a computer monitor up to one meter from one person’s eyes, on a projector many metres away from the eyes of a group of people, or on paper. Within each of these media, there is considerable variation – computer montors differ in how they display colour, projectors may be located in a bright or dark room, and paper can be matte or glossy – but the differences across these media are huge.\nWhen you look at a visualation on your own computer monitor, you are able to perceive fine colour differences and small features such as relatively minor differences in line thickness and symbol size. Many of these differences are “washed out” when images are displayed on a projector, either because of the projector itself or because of the environment in the room such as other light sources or the distance between the image and the observer. Light colours will look nearly white and various dark colours will be difficult to distinguish when projected. The differences that arise when colour is printed are even more dramatic. This is partly due to the fact that the mechanism of showing the colour is so different: on a computer display, light shines through a filter or is emitted in a particular colour combination, but on paper, dyes absorb incident light and must be layered and blended together. Yellow-white combinations that look great on a computer display may not work on a sign in front of a business (see figure).\n\n\n\n\nA store front with a yellow-on-white sign. I bet it looked great on the computer screen."
  },
  {
    "objectID": "lessons/201-colour.html#naming-colours",
    "href": "lessons/201-colour.html#naming-colours",
    "title": "31  Using colour well",
    "section": "\n31.3 Naming colours",
    "text": "31.3 Naming colours\nIn order to use a colour, you must identify it in some way. There are many ways to name colours: by appeal to the rainbow spectrum, widely-used corporate names for crayons and printing (Pantone), software systems such as unix X11, and of course a huge variety in common usage. The rainbow contains colours in an order arranged by wavelength, but this numbering misses a huge number of colours.\nOn computers, colours are often identified using a particular colour model:\n\nred-green-blue (RGB), which refers to the three colours in the pixels of your montior and the wavelength of greatest sensitivity of the colour sensors in your eye;\ncyan-magenta-yellow-black (CMYK) which refers to the four colours used in basic (“4 colour”) printing;\nhue-saturation-brightness (HSB), which separates dimensions of perception of colour;\nhue-chroma-lightness (HCL), such as the Munsell color system;\nand many others!\n\nA key difference between these schemes is that RGB and CMYK are about mixing different amounts of colour together, while HSB and similar schemes have separate values for very different perceptual features: hue, saturation (or chroma, colourfulness), and brightness (or lightness).\nWhile names are required to identify a colour, none of these names are particularly helpful when selecting a colour, as that is a perceptual problem: you need to know what the colour looks like, and what it looks like in combination with other colours you selected. This the the palette problem: what set of colours will you use. The easiest solution to use a palette someone else has created already. A list of many palettes available in R is provided by the paletteer package. These palettes are collected in a GitHub repository.\nHere are some online interactive tools for selecting colours:\n\n\nColor Brewer 2 shows a colour palette on a map and lets you select discrete and quantitative scales with options for the hues and number of colours in the palette.\n\nHCL wizard is good tool to famiiarize yourself with the HCL colour encoding\n\nCoolors emphsizes perceptual and artistic elements you might use to select colours for a palette"
  },
  {
    "objectID": "lessons/201-colour.html#categorical-variables",
    "href": "lessons/201-colour.html#categorical-variables",
    "title": "31  Using colour well",
    "section": "\n31.4 Categorical variables",
    "text": "31.4 Categorical variables\nCategorical variables with 5 or fewer levels you want to highlight are an excellent use of colour. With more than 5 categories, it becomes very difficult to quickly or reliably distinguish colours and map them from the data to the colour legend.\nLet’s use the Palmer penguins data to show how three colours can be selected: by palette or as custom choices.\nThe viridis palette is designed to have approximately uniform brightness (making it suitable for printing in gray scale) and colour-blind friendly. There are four different colour scales you can select (option: magma = A, inferno = B, plasma = C, viridis = D, cividis = E), plus you select the beginning and ending location (begin, end: numbers between 0 and 1). I find the end of the viridis scale to be too yellow, so I usually adjust end.\n\npenguins |&gt; ggplot(aes(flipper_length_mm, bill_length_mm, color = species)) +\n  geom_point(size=3) + theme_bw() +\n  scale_color_viridis_d(begin = 0.1, end = 0.9, option = \"D\")\n\n\n\n\nNow I’ll show how to use other scales you can find from paletteer. Here we use the ggthemes package and its palette hc_default.\n\n# paletteer_d(\"ggthemes::hc_default\", n = 3)  # to see the colours\npenguins |&gt; ggplot(aes(flipper_length_mm, bill_length_mm, \n                        color = species)) +\n  geom_point(size=3) + theme_bw() +\n  scale_color_paletteer_d(\"ggthemes::hc_default\")\n\n\n\n\nTo make a custom scale use scale_color_manual. You need to give the right number of colours (or some extras which will be ignored), or a function which takes and integer and returns that many colours.\n\npenguins |&gt; ggplot(aes(flipper_length_mm, bill_length_mm, \n                        color = species)) +\n  geom_point(size=3) + theme_bw() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"orange\"))"
  },
  {
    "objectID": "lessons/201-colour.html#quantitative-variables",
    "href": "lessons/201-colour.html#quantitative-variables",
    "title": "31  Using colour well",
    "section": "\n31.5 Quantitative variables",
    "text": "31.5 Quantitative variables\nQuantitative data can be effectively displayed using colour on a monochromatic scale to show larger-smaller values, but be aware that your ability to convey quantiative values is very limited. Even the apparently simple task of showing the ordering of quantitative values is difficult because you perceive colour and shading in a local context, as demonstrated by the checker shadow illusion.\nQuantitative data that are separated into positive and negative, or above or below a mean or reference value, are effectively displayed by diverging colour scales (red-blue, green-brown, etc.), sometimes with a neutral colour (white or black) in the middle. One reason this works so well is that it converts a quantitative scale into a categorical two-valued scale, with some residual variation shown with brightness, so be sure that there is a good reason to divide the scale into two classes.\nQuantitative data are almost always best shown as position on an axis, so when would you want to use colour brightness to show a quantitative variable? A common reason is that you are putting points on a map (or heatmap) and the location in the visualization is being used to describe the location on the map. A related reason is that you are trying to display a third quantitative varaible. Here we will use the penguin data and show body mass as colour.\nQuantitative values can be mapped onto colour in a “continuous” way or in a “binned” way. Of course there are only a finite number of colours so this is a matter of degree. Do you want colours to change smoothly (continuously) or do you want a small number of discrete colours (binned). Examples of both are shown below.\n\npenguins |&gt; ggplot(aes(flipper_length_mm, bill_length_mm, \n                        color = body_mass_g)) +\n  geom_point(size=3) + theme_bw() +\n  scale_color_viridis_c(begin = 0.2)\n\n\n\npenguins |&gt; ggplot(aes(flipper_length_mm, bill_length_mm, \n                        color = body_mass_g)) +\n  geom_point(size=3) + theme_bw() +\n  scale_color_paletteer_binned(\"ggthemes::Blue\")\n\n\n\n\nIf it is important to draw a difference between two classes, for example positive and negative values, or numbers larger and smaller than a mean, you can use a diverging colour scale. For variety I will use functions from the RColorBrewer package that has a great set of palettes. With a diverging colour scale you generally want to make a symmetric scale so that 0 is placed at the centre of your color scale.\n\npenguins |&gt; mutate(body_mass_centered = \n                      body_mass_g - mean(body_mass_g, na.rm=TRUE)) |&gt;\n  ggplot(aes(flipper_length_mm, bill_length_mm, \n             color = body_mass_centered)) +\n  geom_point(size=3) + theme_bw() +\n  # scale_color_distiller(type=\"div\", palette=\"RdBu\", limits = c(-2000,2000)) \n  scale_color_fermenter(type=\"div\", \n                        palette=\"RdBu\", \n                        limits = c(-2000,2000)) \n\n\n\n\nThe ‘fermenter’ function makes a binned quantitative scale, while the distiller makes a continuous quantitative scale. Experiment with both to see the difference."
  },
  {
    "objectID": "lessons/201-colour.html#highlighting",
    "href": "lessons/201-colour.html#highlighting",
    "title": "31  Using colour well",
    "section": "\n31.6 Highlighting",
    "text": "31.6 Highlighting\nSometimes you will just want to highlight a small number of points. Perhaps they are outliers or interesting for a particular reason. One of the most flexible ways to do this is to create a new variable that identifies an observation as being highlighted or not, then use this to control plotting.\n\npenguins |&gt; mutate(highlight = \n                      body_mass_g &gt; quantile(body_mass_g, 0.9, \n                                             na.rm=TRUE)) |&gt;\n  ggplot(aes(flipper_length_mm, bill_length_mm, \n             color = highlight)) +\n  geom_point(size=3, show.legend=FALSE,\n             alpha = 0.75) +\n  theme_bw() +\n  scale_color_manual(values = c(\"black\", \"red\"))"
  },
  {
    "objectID": "lessons/201-colour.html#consistent-use-of-colour-across-multiple-figures",
    "href": "lessons/201-colour.html#consistent-use-of-colour-across-multiple-figures",
    "title": "31  Using colour well",
    "section": "\n31.7 Consistent use of colour across multiple figures",
    "text": "31.7 Consistent use of colour across multiple figures\nWilke has some excellent ideas about using colour effectively and consistently across figures and in multi-panel figures. Key points are\n\nif the same information (species, highlights) occurr in multiple figures, use the same colours to mean the same thing in each figure\nif you have figures that show the data in contrasting ways, for example with species in one figure and no species in a different figure, use different colour pallettes in the two figures to avoid confusion."
  },
  {
    "objectID": "lessons/201-colour.html#annotation-to-avoid-legends",
    "href": "lessons/201-colour.html#annotation-to-avoid-legends",
    "title": "31  Using colour well",
    "section": "\n31.8 Annotation to avoid legends",
    "text": "31.8 Annotation to avoid legends\nParticularly in a oral presentation you want your figure to be decoded rapidly. A legend off to one side slows down your reader and requires focus to move back and forth between the data and the legend. You can use coloured text labels on the plot to reduce the need for legends.\n\npenguins |&gt; ggplot(aes(flipper_length_mm, bill_length_mm, \n                        color = species)) +\n  geom_point(size=3, show.legend = FALSE) + theme_bw() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"orange\")) +\n  annotate(\"text\", x = 175, y = 44, label = \"Adélie\", color = \"red\", fontface = \"bold\") +\n  annotate(\"text\", x = 195, y = 55, label = \"Chinstrap\", color = \"blue\", fontface = \"bold\") +\n  annotate(\"text\", x = 220, y = 40, label = \"Gentoo\", color = \"orange\", fontface = \"bold\")"
  },
  {
    "objectID": "lessons/201-colour.html#colour-vision-deficiency",
    "href": "lessons/201-colour.html#colour-vision-deficiency",
    "title": "31  Using colour well",
    "section": "\n31.9 Colour vision deficiency",
    "text": "31.9 Colour vision deficiency\nYou should be aware that not everyone peceives colour in the same way. In particular, avoid combining red and green on the same plot. Here are a few links to help you learn more:\n\nColour Blind Awareness\nNIH statistics and other data\nWikipedia\nAn R package to simulate colour vision deficiency."
  },
  {
    "objectID": "lessons/201-colour.html#summary",
    "href": "lessons/201-colour.html#summary",
    "title": "31  Using colour well",
    "section": "\n31.10 Summary",
    "text": "31.10 Summary\nColor is most useful\n\nto highlight information already distinguished in some other way,\nto distinguish a small number of categories (2-5), and\nas a semi-quantitative scale, particularly if only one or two hues are used (e.g., red and blue) in varying degrees of brightness or saturation.\n\nAvoid using more than 5 colours. It is hard to make quantitative comparisons between two colours. There are lots of tools to help you select colour palettes that work together well. The viridis and RColorBrewer palettes are often good choices."
  },
  {
    "objectID": "lessons/201-colour.html#suggested-reading",
    "href": "lessons/201-colour.html#suggested-reading",
    "title": "31  Using colour well",
    "section": "\n31.11 Suggested reading",
    "text": "31.11 Suggested reading\n\nColour in Healy Chapter 8\n\nColour in Wilke Chapter 4\n\n\nWhich color scale to use? a 4-part blog posting\nPaleteer R package\nCatalog of palettes\n\nColor Brewer 2 interactive website to select and illustrate use of color scales on maps.\nHCL wizard\nThe colorspace package enables conversions between many different ways of describing colours"
  },
  {
    "objectID": "lessons/133-themes.html#example-plots-for-customization",
    "href": "lessons/133-themes.html#example-plots-for-customization",
    "title": "32  Themes",
    "section": "\n32.1 Example plots for customization",
    "text": "32.1 Example plots for customization\nWe’ll use a couple of plots repeatedly: one with facets, and one with many other elements, but no facets.\n\np1 &lt;- penguins |&gt; ggplot(aes(x = body_mass_g, y = flipper_length_mm, \n                              color = species, shape = sex)) + \n  geom_point()\np1\n\nWarning: Removed 11 rows containing missing values (`geom_point()`).\n\n\n\n\np2 &lt;- mpg |&gt; \n  unglue_unnest(trans, \"{trans_type}({trans_code})\") |&gt;\n           ggplot(aes(x = displ, y = hwy, color = trans_type)) +\n                    geom_point() +\n                    facet_wrap(~ factor(cyl))\np2"
  },
  {
    "objectID": "lessons/133-themes.html#built-in-themes",
    "href": "lessons/133-themes.html#built-in-themes",
    "title": "32  Themes",
    "section": "\n32.2 Built-in themes",
    "text": "32.2 Built-in themes\nThere are many themes available that change the whole look of your plot. I’ve used theme_bw many times already, so here are a couple of other examples. For each you can use the base_size option (default value 11) to scale text elements and base_line_size to scale line elements of the figure.\n\np1 + theme_linedraw(base_size = 15, base_line_size = 0.2)\n\nWarning: Removed 11 rows containing missing values (`geom_point()`).\n\n\n\n\np2 + theme_dark(base_size = 20)\n\n\n\n\nThere are also add-in packages with more themes: ggthemes has theme_tufte and theme_economist\n\np1 + theme_economist() + scale_fill_economist()\n\nWarning: Removed 11 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnother collection of themes is available in ggthemer. The main goal of this lesson is to give you the vocabulary and skills to customize the theme of your plot, so I won’t spend any more time on theme packages. On to the details!"
  },
  {
    "objectID": "lessons/133-themes.html#customizing-text-on-a-figure",
    "href": "lessons/133-themes.html#customizing-text-on-a-figure",
    "title": "32  Themes",
    "section": "\n32.3 Customizing text on a figure",
    "text": "32.3 Customizing text on a figure\nThe most common change I want to make to a plot’s theme (after axis and guide labels) is to increase the size of the text. This can be done using theme(text= ...) or for specific elements, theme(axis.title=...) (see the help page for more elements of the theme that can be set with a text element.) On the right side of the equals sign, you need the function element_text which allows you to control 10 different characteristics of your text. Here is an example of a few of the features.\nThemes are controlled hierarchically, so adjusting text affects all text on the plot. Adjusting axis.title affects both axis.title.x and axis.title.y.\n\np2  + theme(text = element_text(size = 15),\n            axis.text.x = element_text(angle = 45),\n            axis.title.y = element_text(color = \"blue\"),\n            strip.text = element_text(face = \"bold\"),\n            legend.text = element_text(hjust = 1),\n            legend.title = element_text(family = \"serif\"),\n            axis.title.x = element_text(color = \"#FF00FF\", size = 18, face = \"italic\", family = \"mono\"))\n\n\n\n\nCompare this with the original version of the plot and play “spot the differences”."
  },
  {
    "objectID": "lessons/133-themes.html#customizing-borders-ticks-axis-labels",
    "href": "lessons/133-themes.html#customizing-borders-ticks-axis-labels",
    "title": "32  Themes",
    "section": "\n32.4 Customizing borders, ticks, axis labels",
    "text": "32.4 Customizing borders, ticks, axis labels\nThere are many elements on a plot drawn with lines. These too can be customized all at once using theme(line = ...) or element by element using, for example theme(axis.line.x=...) or theme(axis.ticks.y=...). The grid lines can be adjusted using panel.grid.major and panel.grid.minor. These elements are named and controlled hierarchically as well. The right hand size of the expressions above must have an element_line function which allows you to control color, size, linetype and other features like arrows.\n\np2 + theme(line = element_line(size = 1),\n           axis.ticks.y = element_line(color = \"pink\"),\n           axis.line.x = element_line(linetype = 3, size = 0.25),\n           panel.grid.minor.y = element_blank(),\n           panel.grid.major = element_line(linetype = 3, color = \"black\", size = 0.25))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead."
  },
  {
    "objectID": "lessons/133-themes.html#enhanced-text",
    "href": "lessons/133-themes.html#enhanced-text",
    "title": "32  Themes",
    "section": "\n32.5 Enhanced text",
    "text": "32.5 Enhanced text\nThe ggtext package allows you to use markdown and HTML code in any text element on your plot. You need to use element_markdown (or element_textbox_simple) instead of element_text for each element you want to use this enhanced formatting method with.\nUnlike other theme elements, you can’t easily use ggtext formatting hierarchically: you can’t simply set all text to use elemment_markdown or even both axis labels. You must control each element individually.\n\np1 + labs(title = \"_Penguin data from **Palmer station**, Antarctica_&lt;sup&gt;1&lt;/sup&gt;\",\n          x = \"&lt;span style = 'color:red;'&gt;body mass (g)&lt;/span&gt;\",\n          y = \"flipper length (mm)\",\n          caption = \"&lt;sup&gt;1&lt;/sup&gt;Collected by K. Gorman\"\n          ) + \n  theme(plot.title = element_markdown(lineheight = 1.2),\n        plot.caption = element_markdown(),\n        axis.title.x = element_textbox_simple(halign = 0.5)\n        )\n\nWarning: Removed 11 rows containing missing values (`geom_point()`).\n\n\n\n\n\nYou can also add formatted boxes for the titles of facets and at any location on a plot.\n\np2 + theme(\n    strip.background = element_blank(),\n    strip.text = element_textbox(\n      size = 12,\n      color = \"white\", fill = \"#5D729D\", box.color = \"#4A618C\",\n      halign = 0.5, linetype = 1, r = unit(5, \"pt\"), width = unit(1, \"npc\"),\n      padding = margin(2, 0, 1, 0), margin = margin(3, 3, 3, 3)\n    )\n)\n\n\n\ntext_annotations &lt;- tibble(\n  label = c(\n    \"Some text **in bold.**\",\n    \"Linebreaks&lt;br&gt;Linebreaks&lt;br&gt;Linebreaks\",\n    \"*x*&lt;sup&gt;2&lt;/sup&gt; + 5*x* + *C*&lt;sub&gt;*i*&lt;/sub&gt;\",\n    \"Some &lt;span style='color:blue'&gt;blue text **in bold.**&lt;/span&gt;\"\n  ),\n  x = c(4000, 5200, 5700, 5000),\n  y = c(220, 200, 175, 150),\n  hjust = c(0.5, 0, 0, 1),\n  vjust = c(0.5, 1, 0, 0.5),\n  angle = c(0, 0, 45, -45),\n  # sex = c(\"male\", \"male\", \"male\", \"male\")\n)\np1 + geom_richtext(mapping = aes(x = x, y = y, label = label,\n                                 angle = angle, hjust = hjust, vjust = vjust),\n                   data = text_annotations,\n                   inherit.aes = FALSE,\n                   fill = NA,\n                   color = \"black\")\n\nWarning: Removed 11 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "lessons/133-themes.html#further-reading",
    "href": "lessons/133-themes.html#further-reading",
    "title": "32  Themes",
    "section": "\n32.6 Further reading",
    "text": "32.6 Further reading\n\n\nggplot tidyverse documentation - like the theme help page, but shows the results of the examples\nDocumentation for ggtext"
  },
  {
    "objectID": "lessons/130-graphics-output.html#formatting-visualizations-for-a-purpose",
    "href": "lessons/130-graphics-output.html#formatting-visualizations-for-a-purpose",
    "title": "33  Graphics formatting",
    "section": "\n33.1 Formatting visualizations for a purpose",
    "text": "33.1 Formatting visualizations for a purpose\nA final design check for your visualization can be done by asking two questions:\n\nHow big will this graphic be and how far away will the viewer be from it?\nHow long will the viewer have to look at the graphic?\n\nThese two questions focus your attention on whether elements of your visualization are too small and whether your visualization is too complex to be understood in the time available. Many visualizations are both too complex and contain detail that is too small. To exercise your thinking about these questions, design a visualization and then ask yourself how it could be made simpler or more complex, and how it should change if its apparent size changed.\nLet’s start with a frequently used visualization of penguin data.\n\np1 &lt;- penguins %&gt;% ggplot(aes(body_mass_g, flipper_length_mm, \n                        color = species, shape = sex)) + \n        facet_wrap( ~ island) + \n        geom_point() +\n        scale_shape(na.translate = FALSE) +\n        labs(x = \"Body mass (g)\", y = \"Flipper length (mm)\")\np1\n\n\n\n\nThis is a fairly complex figure, but it’s fine if you will have a minute to look at it on a monitor or printed page within a metre. For quick or distant viewing, the text should be made bigger, the symbols for sex should be dropped (or made more prominent if they are critical information), and the facets should be combined (unless they are critical.) Essentially what I’ve done with the figure above is to make a “multi-purpose” figure that shows a lot of data, but I’ve avoided the “should I show feature X?” question. This is one reason editing figures for other uses is so valuable: it focusses your attention on what is essential for your story.\nLet’s simplify the visualization to emphasize the species differences, discard the sex and island differences, and make it suitable for display in an oral presentation or poster.\n\np2 &lt;- penguins %&gt;% ggplot(aes(body_mass_g, flipper_length_mm, \n                        color = species)) + \n        geom_point(size = 3, alpha = 0.5) + \n        theme_clean() + \n        theme(axis.title = element_text(size = 18),\n              axis.text = element_text(size = 14),\n              legend.position = \"none\") +\n        geom_text(aes(label = species),\n                  size = 8,\n               data = tibble(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n                             body_mass_g = c(4000, 3200, 4500),\n                             flipper_length_mm = c(173, 207, 227))) +\n        labs(x = \"Body mass (g)\", y = \"Flipper length (mm)\")\np2"
  },
  {
    "objectID": "lessons/130-graphics-output.html#graphics-formats",
    "href": "lessons/130-graphics-output.html#graphics-formats",
    "title": "33  Graphics formatting",
    "section": "\n33.2 Graphics formats",
    "text": "33.2 Graphics formats\nComputer graphics can be divided into two types: raster images and vector drawings. A photograph is a raster image; there is a separate color value for each of millions of pixels. A scatter plot is often best represented as a vector drawing which is just a list of instructions: make circles of a certain colour at these locations, draw lines for axes of a particular thickness, etc. For “simple” visualizations, the vector graphics format is smaller to store, faster to transmit, and easy to display at high fidelity on devices with different numbers of dots per unit length.\nR can produce graphics files in many formats, of which the most common vector formats are PDF (which can include raster images as well) and SVG and the most common raster formats are PNG and JPEG. There are a huge number of other formats, some of which R can produce directly and others can be achieved by conversion.\nWhen you create a PDF or SVG file, you specify the size in units such as mm, cm, or inches. The function ggsave will write out a graphic to a file name you specify.\n\nggsave(\"static/penguin-plot.pdf\", p2, width = 15, height = 10 , units = \"cm\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\nggsave(\"static/penguin-plot.svg\", p2, width = 15, height = 10 , units = \"cm\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\nFigures are often produced with an aspect ratio (height / width) in the range of 0.6-0.7. Square images almost always look “too tall”. For multi-panel figures such as the first facetted plot above, this ratio is usually applied to each panel. Deviations from this ratio arise from aesthetich preference, constraints imposed by external factors, and the size of axis labels, titles, and legends.\nIf you examine the versions of this plot in the PDF file just created, in the HTML version of this book, and the interactive version produced by Rstudio, you will see that they are all slightly different. This includes minor features like the placement of the dots in the horizontal gridlines relative to other features and major features like the size of the on-graph “legend”. In particular, text near a border can easily fall partially outside the boarder and get clipped. (In the first version of my PDF of the “C” in Chinstrap was clipped by the y-axis even though it looked fine in the Rstudio preview.) The only way I know to resolve these problems is by testing and iterative refinement of the graph.\nThe actual sizes of these plots are of course completely fictional. The instructions stored on a computer have no physical size! What purpose to the size measurements have? They allow for the relative sizing of different elements: lines, symbols, and text. If you change the size of the images created above, all that will change is the relative sizes of these elements. This is why the “size” of text elements is not closely tied to a “point size” for a font; ggplot does not know how the figure will be rendered not when stored in a file a certainly not when ultimately placed on a computer monitor, phone, projected on a wall, printed on a page or a poster. The size of a figure in these functions serves as a convenient reminder of how you are planning to use the image and will help you think about your design. Sometimes the simplest way to refine a plot is to change the “size” it is created at slightly to make a bit more or less white space between the graphical elements.\nHere is how to make the raster format images; it’s fairly easy to guess the functions now!\n\nggsave(\"static/penguin-plot.png\", p2, width = 15, height = 10, units = \"cm\", dpi = 300)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\nggsave(\"static/penguin-plot.jpeg\", p2, width = 15, height = 10, units = \"cm\", dpi = 144)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\nFor raster images, the the “dots per inch” (dpi) provides some information on how the figure is intended to be displayed. Montiors, phones, and other devices generally range from 100-250 dpi and physical printing usually ranges from 300-600 dpi. There are, of course, exceptions.\nThe JPEG file format was designed to store photographic images compactly. Image compression algorithms are used to reduce the size of the file from the literal content (x resolution x y resolution x colour bits per pixel, commonly a total of 30-40 million bytes even for a camera phone) to a size of 5-10% of that total. Photographs rarely include precisely straight lines, crisp text, and lots of small symbols. The JPEG compression tends to create lots of “artifacts” or “jaggies” on an image since the data compression algorithms smooth out sharp, high contrast boundaries that are common in data visualizations and rare in photographs. Don’t use JPEG for ggplot figures.\nThe PNG format, by contrast, is excellent at high contrast lines and shapes and has none of these problems. It is also good at photographic images. This is the format you should use for ggplot visualizations if, for some reason, you need to produce a raster image. One reason to produce a raster image is that you will know exactly what it will look like when it is displayed or printed. You don’t know what size it will be, but you can know the relative placement of all elements on the figure.\nA third application for rendering visualiztions is the one we have been using in this course since the very first lesson: in an interactive or rendered R markdown document. There are several options you can provide on the {r} line that opens a code block: fig.height and fig.width which control the size in inches, fig.align (left, right, or center) to control placement left-to-right, and out.height and out.width for scaling to a percentage. For more, see these notes by Nicholas Tierney. You can also get a figure written to a file in any of the formats discussed above using the option dev. Here I’ll demonstrate a few of these options. Some of these options only have an effect when your document is knitted.\n\np1"
  },
  {
    "objectID": "lessons/130-graphics-output.html#vector-or-raster",
    "href": "lessons/130-graphics-output.html#vector-or-raster",
    "title": "33  Graphics formatting",
    "section": "\n33.3 Vector or raster?",
    "text": "33.3 Vector or raster?\nThe quality of a vector image (PDF, svg) should always be superior to a raster image (png). Vector images can be redrawn at different scales easily, making it possible for your reader to zoom in on your graphics. If your plot has millions of points or a huge number of lines, then a vector format which requires the computer to draw each feature every time the image is shown will be slow. So you should normally use a raster format (svg for the web, pdf for printing) and only use raster formats (png) for very complicated figures, for example if you notice that your graphics are displaying slowly.\n\np1\n\nWarning: Removed 11 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "lessons/130-graphics-output.html#size-and-resolution",
    "href": "lessons/130-graphics-output.html#size-and-resolution",
    "title": "33  Graphics formatting",
    "section": "\n33.4 Size and resolution",
    "text": "33.4 Size and resolution\nThe differences between raster and vector formats give rise to different ideas of a graphics size. Raster images are defined by the number of pixels in each dimension. Vector graphics are not made of pixels, so they are defined by a target size for presentation in inches or centimeters. When they are actually displayed, raster images may be simplified to lower resolution or dots may be interpolated to match the device you are using. Similarly, a vector image on a monitor, printed page, or projected on a screen will have very different sizes, even if the computer doesn’t know it. These resizing operations happen all the time, sometimes even interactively (as you zoom on a phone for example) and the person creating the image has no control over how it will be displayed!"
  },
  {
    "objectID": "lessons/130-graphics-output.html#recommendations",
    "href": "lessons/130-graphics-output.html#recommendations",
    "title": "33  Graphics formatting",
    "section": "\n33.5 Recommendations",
    "text": "33.5 Recommendations\nIn your code chunks use\n\n\nfig.width = 6 (6 inches)\n\nfig.asp = 0.65 (aspect ratio of height is 65% of the width)\n\nout.width = '70%' (so that there is some blank margin on both sides of your image)\nfig.align = 'center'\n\ndev = 'svg' or dev = 'svglite' (the output format)\n\nYou can set the defaults in your first code chunk using the following code:\n\nknitr::opts_chunk$set(\n  fig.width = 6, fig.asp = 0.65, fig.align=\"center\", out.width = '70%'\n)\n\nThen you only need to specify these chunk options if you need to change them for some reason."
  },
  {
    "objectID": "lessons/130-graphics-output.html#a-solution-for-scaling-graphics",
    "href": "lessons/130-graphics-output.html#a-solution-for-scaling-graphics",
    "title": "33  Graphics formatting",
    "section": "\n33.6 A solution for scaling graphics",
    "text": "33.6 A solution for scaling graphics\nWhen you show a figure on the screen, on a projector, printed page, or poster, you will want to redesign some elements. There is a relatively new solution to this resizing challenge. (So new it seems to not work sometimes.) Once you have made a graphic look just the way you want, you can adjust the scale of the elements of the figure without changing the “physical” size of the image. Here I make the graphic have a much larger “physical” size, but I also scale up all the elements of the graphic by a factor of three (using scaling = 3). This allows you to make a high resolution object you can include in a printed poster without redesigning a figure too much – just set the scaling factor and adjust the width and height by the same factor.\n\npngfile &lt;- fs::path(knitr::fig_path(),  \"penguins-ragg.png\")\nggsave(pngfile, p2, device = agg_png, \n        width = 45, height = 30, units = \"cm\", res = 300, scaling = 3)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\nknitr::include_graphics(pngfile)\n\n\n\n\n\n\n# agg_png(\"static/penguins-ragg.png\", width = 45, height = 30, units = \"cm\", res = 300, scaling = 3)\n# p2\n# invisble(dev.off())\n\nThe ragg library and graphics output device also provides reliable rendering text in many (perhaps most) languages and a way to scale the size of images across different output formats. See the link below for more information."
  },
  {
    "objectID": "lessons/130-graphics-output.html#further-reading",
    "href": "lessons/130-graphics-output.html#further-reading",
    "title": "33  Graphics formatting",
    "section": "\n33.7 Further reading",
    "text": "33.7 Further reading\n\nNotes on figure scaling in R Markdown from R for Data Science.\nFor a demonstration of text in other languages, please see here. For a discussion of scaling plots see here."
  },
  {
    "objectID": "lessons/134-reconstruction.html#goal",
    "href": "lessons/134-reconstruction.html#goal",
    "title": "\n34  Reconstructing a visualization\n",
    "section": "\n34.1 Goal",
    "text": "34.1 Goal\nIn this lesson we will find a visualization and try to reconstruct it using R.\nA reasonable challenge is to redraw a version of the textbook cover for STAT 1060 (De Veaux et al. 2018) using the gapminder data.\n\n\nFigure 34.1: Cover of the textbook Stats: Data and Models"
  },
  {
    "objectID": "lessons/134-reconstruction.html#exercise",
    "href": "lessons/134-reconstruction.html#exercise",
    "title": "\n34  Reconstructing a visualization\n",
    "section": "\n34.2 Exercise",
    "text": "34.2 Exercise\nReconstruct the WeatherSpark visualization for a location in Canada using Environment Canada weather data.\n\n\n\n\nDe Veaux, Richard D., Paul F. Velleman, David E. Bock, Augustin M. Vukov, and Augustine C. M. Wong. 2018. Stats: Data and Models. Third Canadian Edition. Pearson Education Canada."
  },
  {
    "objectID": "lessons/499-review.html",
    "href": "lessons/499-review.html",
    "title": "R Review",
    "section": "",
    "text": "This page summarizes the most important R functions, datasets, and packages described in the course. You can sort the following table by object (function or dataset), lesson, or package. You can also search the table by function name or description. There is a simple example of how to use most functions. Take a look at the help pages for more examples.\nThis table will be updated and expanded throughout the term.\nI strongly encourage you to make your own notes about functions you learn to use.\n\n\n\n\n\n\n\nNote that the tidyverse package includes the following packages:\nbroom, conflicted, cli, dbplyr, dplyr, dtplyr, forcats, ggplot2, googledrive, googlesheets4, haven, hms, httr, jsonlite, lubridate, magrittr, modelr, pillar, purrr, ragg, readr, readxl, reprex, rlang, rstudioapi, rvest, stringr, tibble, tidyr, xml2"
  },
  {
    "objectID": "lessons/901-TMI-R.html#recent-changes",
    "href": "lessons/901-TMI-R.html#recent-changes",
    "title": "More details about R",
    "section": "Recent changes",
    "text": "Recent changes\nSoftware is notorious for frequent and unanticipated changes. Websites appear, disappear, and are redesigned frequently. Features appear and disappear even in mature software like email clients, word processors and spreadsheets. R is no different. Here are a few changes that have occurred in the last few years that are particularly relevant to our course.\nThe “pipe” feature has become part of R (as of version 4.1), but it looks different than the old pipe and works a little bit differently. I don’t think this difference will affect the examples in this course very much. I have changed to the new pipe (|&gt;) most places in the examples, but the old pipe (%&gt;%) probably still appears in some places.\nGitHub authentication has changed; passwords are no longer allowed from R (or anywhere except the website). I have revised and simplified the instructions for using GitHub. This is a technical subject that can be difficult for beginners to master—give yourself some time to fully understand how git and github work.\nData sources and external links disappear and change frequently. I think I have checked them all. If you see problems, let me know!"
  },
  {
    "objectID": "lessons/901-TMI-R.html#overview",
    "href": "lessons/901-TMI-R.html#overview",
    "title": "More details about R",
    "section": "Overview",
    "text": "Overview\nR is a big software package. It’s developed over decades of use. It has millions of regular users across many disciplines in teaching, academic research and professional applications. It’s a computer programming language, and a tool for accessing thousands of data analysis tools, and a great interactive environment for exploring, analyzing, and visualizing data. You won’t learn it all.\nThe approach in this course is to teach you how to use R in a particular style, with an emphasis on the ggplot and tidyverse packages. The R markdown documents we write balance interactive use while documentating the sequence of steps used in any analysis. I have omitted a lot of details that might be called “fundamentals” and instead focussed on getting work done quickly. In this lesson I fill in some of the gaps. My reasoning is that most students want to learn a tool before you spend too much time on details, so this course emphasizes what you can do with R. On the other hand, I hope you will use R for a long time on many projects, and once you reach the point you know you will do that, you should start developing a solid mental model of how R works so that you can build robust understandings of how to accomplish tasks with R and get beyond copy, paste, search, and experiment. Without these fundamentals, its easy to make up incorrect ideas about why and how R works and then get really confused and frustrated.\nA simple introduction to the basics of R appears in Healy Appendix. A different approach is taken in the R for Data Science book, which introduces the basics of many topics, such as tibbles, strings, factors and dates, functions and vectors together with their applications one chapter at a time."
  },
  {
    "objectID": "lessons/901-TMI-R.html#what-kinds-of-data-structures-does-r-use",
    "href": "lessons/901-TMI-R.html#what-kinds-of-data-structures-does-r-use",
    "title": "More details about R",
    "section": "What kinds of data structures does R use?",
    "text": "What kinds of data structures does R use?\nR has many basic types: logical, numeric (double), character, factor. Any combination of these types can be combined into lists. Elements of lists can be named so that they work like dictionaries. Vectors, matrices, and higher dimensional arrays are all composed of the same type of data, usually numeric, factor, or logical.\nLists are a very powerful type because they can be given attributes that identify them as special structures with particular interpetations. The most common special list is a data frame, data table or tibble. (The tibble is a refined kind of data frame.) You probably think of a tibble as a matrix where each column can be of a different type. How is this accomplished? It’s a list of vectors. Each vector corresponds to a column or variable in the data frame with its own name and data type.\nHere are some R commands you can use to decode the structure of any object you have in your workspace. I’ll demonstrate on the diamonds tibble. As always, experiment with these commands by trying them on other objects such as cars (a data frame).\n\ntypeof(diamonds)  # it's a list\n\n[1] \"list\"\n\nclass(diamonds) # which works as a data.frame and also as a tbl (tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nstr(diamonds)\n\ntibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n $ cut    : Ord.factor w/ 5 levels \"Fair\"&lt;\"Good\"&lt;..: 5 4 2 4 2 3 3 3 1 3 ...\n $ color  : Ord.factor w/ 7 levels \"D\"&lt;\"E\"&lt;\"F\"&lt;\"G\"&lt;..: 2 2 2 6 7 7 6 5 2 5 ...\n $ clarity: Ord.factor w/ 8 levels \"I1\"&lt;\"SI2\"&lt;\"SI1\"&lt;..: 2 3 5 4 2 6 7 3 4 5 ...\n $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\n\nclass(diamonds$carat)\n\n[1] \"numeric\"\n\nclass(diamonds$cut)  # its a factor, and the factor is ordered (factors can be unordered)\n\n[1] \"ordered\" \"factor\" \n\nclass(as.matrix(diamonds %&gt;% select(depth, table, price)))  # if you pick columns of the same type, you can convert them to a matrix\n\n[1] \"matrix\" \"array\" \n\nclass(as.matrix(diamonds %&gt;% select(cut, color, clarity)))  \n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "lessons/901-TMI-R.html#data-frame-tibbles-and-data-tables",
    "href": "lessons/901-TMI-R.html#data-frame-tibbles-and-data-tables",
    "title": "More details about R",
    "section": "data frame, tibbles, and data tables",
    "text": "data frame, tibbles, and data tables\nWhen R was young, there was one way to organize data into the tables we are using throughout this course: the data frame. This data structure is made from a list of vectors; each column is an entry in the list and all the data in each list (column) must be of the same type. Over time, people wanted to add new features to data tables, but existing code made this difficult. So two new kinds of data frames were created.\nA tibble is a data frame, but it has some extra features. Most notably the way it is displayed in R has been improved. Only a few rows and columns are shown (unless you ask for more) with the rest of the information provided as a summary. Tibbles are widely used by the tidyverse packages.\nA data table is a kind of data frame where all the code has been written with the goal of increasing calculation speed. Some of the methods for manipulating data tables are quite different from what we have learned in this course; some of the examples from R4DS chapter on data transformations are demonstrated here."
  },
  {
    "objectID": "lessons/901-TMI-R.html#what-is-the-difference-between-strings-and-factors",
    "href": "lessons/901-TMI-R.html#what-is-the-difference-between-strings-and-factors",
    "title": "More details about R",
    "section": "What is the difference between strings and factors?",
    "text": "What is the difference between strings and factors?\nA string is just a sequence of text, enclosed by single or double quotation marks. You can display strings, and search for text, and do other text-type things with them, but you can’t (of course) do calculations with them.\nYou can convert a vector of strings into a vector of factors. This assigns an integer to each different element in the vector. Why would you do this? The numbers can be assigned to give a particular order to the factors (strings) that is different from alphabetical. The factors can be used in statistical models as though they are numbers, as though they were the indexes you use on a variable in math class (e.g., \\(x_1, x_2, \\dots\\)).\nHere are some simple examples.\n\nv &lt;- c(\"Apple\", \"Bananna\", \"Cat\", \"Apple\", \"Orange\")\ntypeof(v)\n\n[1] \"character\"\n\nf &lt;- factor(v)\ntypeof(f)\n\n[1] \"integer\"\n\nas.numeric(f)\n\n[1] 1 2 3 1 4\n\nf\n\n[1] Apple   Bananna Cat     Apple   Orange \nLevels: Apple Bananna Cat Orange\n\n\nThe forcats package has lots of great functions for working with factors which can help you control how your plots are drawn. That’s the main use we will have for them in this course."
  },
  {
    "objectID": "lessons/901-TMI-R.html#whats-the-pipe",
    "href": "lessons/901-TMI-R.html#whats-the-pipe",
    "title": "More details about R",
    "section": "What’s the pipe?",
    "text": "What’s the pipe?\nThe pipe %&gt;% is a way to write function composition. In our data analysis we build up calculations from lots of different functions such as select, group_by, arrange, filter, summarize, distinct and many more described in R for Data Science. Each function starts with a tibble, does something to it, and produces a revised tibble. It’s very natural to develop a complex series of calculations like a recipe in a cookbook. (Sometimes these recipes are called pipelines!) Before pipes were invented there were basically two options; write the functions down in reverse order, or create lots of temporary variables to store the intermediate results and then discard them. Pipelines are easier to write and easier to read. Here’s an example of the three styles, based on a simple analysis of the diamonds data.\nPipeline:\n\ndiamonds %&gt;% filter(cut==\"Ideal\") %&gt;% group_by(color) %&gt;% summarize(mean_price = mean(price))\n\n# A tibble: 7 × 2\n  color mean_price\n  &lt;ord&gt;      &lt;dbl&gt;\n1 D          2629.\n2 E          2598.\n3 F          3375.\n4 G          3721.\n5 H          3889.\n6 I          4452.\n7 J          4918.\n\n\nTemporary variables:\n\nd1 &lt;- filter(diamonds, cut==\"Ideal\")\nd2 &lt;- group_by(d1, color) \nsummarize(d2, mean_price = mean(price))\n\n# A tibble: 7 × 2\n  color mean_price\n  &lt;ord&gt;      &lt;dbl&gt;\n1 D          2629.\n2 E          2598.\n3 F          3375.\n4 G          3721.\n5 H          3889.\n6 I          4452.\n7 J          4918.\n\n\nComposition:\n\nsummarize(\n  group_by(\n    filter(diamonds, cut == \"Ideal\"), color\n  ), mean_price = mean(price)\n)\n\n# A tibble: 7 × 2\n  color mean_price\n  &lt;ord&gt;      &lt;dbl&gt;\n1 D          2629.\n2 E          2598.\n3 F          3375.\n4 G          3721.\n5 H          3889.\n6 I          4452.\n7 J          4918.\n\n\nWhich do you think is easier to understand? What if the pipline was longer? Or shorter?\nNote: as of late 2020, a new kind of pipe is becoming part of R. It looks like this |&gt; and works a bit like the existing pipe we use but has some differences. It’s not in this course at all yet since the official release of R doesn’t have it. This is another example of how R (and all computer systems) change over time. These sorts of big changes are very rare with R because so many people use it for so many purposes."
  },
  {
    "objectID": "lessons/901-TMI-R.html#why-does-ggplot-use",
    "href": "lessons/901-TMI-R.html#why-does-ggplot-use",
    "title": "More details about R",
    "section": "Why does ggplot use +?",
    "text": "Why does ggplot use +?\nWhen we make plots with ggplot we do something very similar – we start with a tibble or data frame, but then we make a plot, and then modify it step by step, setting geometries, adding more data, creating facets, setting scales, altering the theme. This seems a lot like a pipeline. Why do we use + to connect different ggplot2 commands?\nSimple. The developer didn’t know about the pipe operator in R when ggplot was developed. Adding a feature to a plot seemed like a “plus” operation, so that was chosen. The developer says if he was starting over today, ggplot would use pipes, but its not practical to change at this point. Too many people use it with the +. Some people think of functions as verbs and ggplot commands as nouns, and justify the difference that way. Software development is a complicated human activity that takes place over time and develops quirks and lore."
  },
  {
    "objectID": "lessons/901-TMI-R.html#what-are-packages-whats-the-difference-between-installing-one-and-using-one-why-is-r-organized-this-way",
    "href": "lessons/901-TMI-R.html#what-are-packages-whats-the-difference-between-installing-one-and-using-one-why-is-r-organized-this-way",
    "title": "More details about R",
    "section": "What are packages? What’s the difference between installing one and using one? Why is r organized this way?",
    "text": "What are packages? What’s the difference between installing one and using one? Why is r organized this way?\nThere are thousands of R packages. So many that it seems you are always needing to install a new one. Worse, sometimes you want a function, but you can’t remember which package it’s installed in – so then you can’t use it. (There is a solution to this: use the double ?? followed by the fuction name in the console to search for a function across all packages available on your computer.) Why this complication?\nThere are several reasons.\n\nNew packages are developed frequently, by different people. And some packages get abandoned too. Packages create modularity that makes it easier to test and fix problems with packages across different groups of developers.\nThe library command tells R you want to use a particular package in your current session. Not loading all packages can save time, computer resources, and avoid name collisions (see below).\nMost people only use a small fraction of all the packages available – you simply don’t need to install them all.\nTwo packages can use the same name for a different function or different dataset. If there were no packages, everyone using R would have to coordinate the naming of everything across all the packages. An impossible task. Sometimes when you load a package with library you’ll get a message that an existing function has been redefined (or masked) by this new package. You can always add the package name and two colons, dplyr::filter for example, to refer to a function in a particular package."
  },
  {
    "objectID": "lessons/901-TMI-R.html#why-are-there-so-many-kinds-of-equals-signs",
    "href": "lessons/901-TMI-R.html#why-are-there-so-many-kinds-of-equals-signs",
    "title": "More details about R",
    "section": "Why are there so many kinds of “equals signs”?",
    "text": "Why are there so many kinds of “equals signs”?\nThere are many different concepts behind the humble equals sign used in mathematics. In R we use different symbols: =, ==, &lt;- and even some special functions like all.equal and near. Even with all these differences = can mean different things in different contexts, and there is a special version of &lt;- written in the other way: -&gt;. The pipe adds its own twist, you can write %&lt;&gt;% as a combined assignment operator and pipe.\nSo what does each mean?\n\n\na &lt;- b means assign the name a to the object b. (You can use = for this, but I encourage you not to.)\n\na == b is a comparison between a and b with the result TRUE if they are the same and otherwise FALSE. (The notion of “same” in computing is surprisingly complicated, but for basic types, in particular strings, this is fairly straightforward.)\n\na = b is used when naming elements of a list or arguments to a function, for example ggplot(mapping = aes(...)) or list(a= \"apple\", b=\"bananna\").\n\nnear(1.25, 1.253, tol = 0.01) in the dplyr package is used to compare numbers to see if they are close together.\n\nThe waldo package has functions for testing equality of data frames and showing differences."
  },
  {
    "objectID": "lessons/901-TMI-R.html#numbers",
    "href": "lessons/901-TMI-R.html#numbers",
    "title": "More details about R",
    "section": "Numbers",
    "text": "Numbers\nR is\nNA nan inf -inf Testing for Na. Replacing. If then functions."
  },
  {
    "objectID": "lessons/901-TMI-R.html#whats-a-function",
    "href": "lessons/901-TMI-R.html#whats-a-function",
    "title": "More details about R",
    "section": "What’s a function?",
    "text": "What’s a function?"
  },
  {
    "objectID": "lessons/901-TMI-R.html#whats-a-formula",
    "href": "lessons/901-TMI-R.html#whats-a-formula",
    "title": "More details about R",
    "section": "What’s a formula?",
    "text": "What’s a formula?\nLm facet also t.test gam"
  },
  {
    "objectID": "lessons/901-TMI-R.html#programming-styles",
    "href": "lessons/901-TMI-R.html#programming-styles",
    "title": "More details about R",
    "section": "Programming styles",
    "text": "Programming styles\nGrammar? Gg gt Descriptive vs imperative SQL as another example"
  },
  {
    "objectID": "lessons/901-TMI-R.html#comparing-sql-to-dplyr",
    "href": "lessons/901-TMI-R.html#comparing-sql-to-dplyr",
    "title": "More details about R",
    "section": "Comparing SQL to dplyr",
    "text": "Comparing SQL to dplyr"
  },
  {
    "objectID": "lessons/901-TMI-R.html#using-databases-with-r",
    "href": "lessons/901-TMI-R.html#using-databases-with-r",
    "title": "More details about R",
    "section": "Using databases with R",
    "text": "Using databases with R"
  },
  {
    "objectID": "lessons/901-TMI-R.html#computing-notes",
    "href": "lessons/901-TMI-R.html#computing-notes",
    "title": "More details about R",
    "section": "Computing notes",
    "text": "Computing notes\nThere are thousands of different programming languages. Why? Partly different ideas have arisen over time. Sometimes, languages have been created for particular problems. But most often, languages are created to enable a new kind of interaction between programmer and computer. Over time, as computing power increases and the kinds of problems solved by computer change, new opportunities and new needs arise and new languages are developed.\nWhat are some of the constraints and trade-offs when using a computer? The three most important are * the amount of storage required to solve a problem, * the number of computations to solve a problem, and * the amount of human time required to design and implement a solution.\nMany programming languages prioritize the first two. The trade-off between the first two is a classic idea in computer science. You can see how it arises from a simple example. Suppose you know you need the approximate result of some complex mathematical computation. You can either perform the calcuation when you need it (which takes time), or you can compute a table of possible computations in advance (which takes storage) and lookup the result when you need it. This is the idea behind statistical tables in the back of statistics textbooks – the calculations are hard, but a good enough table fits on a page or two.\nThe designers of the R language and packages often prioritize minimizing the amount of time required for a human to design and code the solution to a problem. To do this well, the designers needed to give you flexible and powerful tools (functions). The flexibility of the functions means that they are not always optimized to use the least storage or time.\nPowerful tools require significant study to learn how to use them effectively. The examples in this course are selected to convince you that that investment is worth your time. There are also many specialized packages of functions, each created to make a certain type of problem easier or faster to solve. This is now a feature of all programming domains, which have specialized tools for different operating systems, the web, or particular problem domains like databases, machine learningm or statistical data analysis. Each of these requires effort to learn, but a helpful insight comes from the early design of graphical user interfaces – if tools made by different programmers have enough in common and adhere to conventions, then the burden on the programmer and user is greatly reduced.\nThere are many other optimizations and trade-offs in computing. For example, numerical computations sometimes are done in a different order compared to the way you would do them in math class as a result of numerical approximations made by computers.\nEven within the R system there are many different styles of programming and problem solving. In this course I emphasize one particular style, now known popularly as the tidyverse. This approach organizes data and results in tables (called tibbles) as much as possible and encourages you to build larger solutions from composing powerful functions together (like th dplyr package). As a result in this course a hidden message throughout most lessons is how to organize your data and results and how to use a small set of powerful functions to solve a large set of general problems.\nMany other computing systems would work well for this course. Among programmers, a very popular choice is python, which shares a lot in common with R with many add-on packages providing similar functions. As a simplification, python use tends to be favoured by people developing software to solve a particular problem, while R tends to be favoured by people who want to interactively explore their data analysis options and need to develop custom analyses for each problem they encounter. Python and R have both existed since before the year 2000, but the styles of data analysis and problem solving possible with each has grown and converged together considerably in recent years."
  },
  {
    "objectID": "lessons/999-metadata.html#about-the-author-and-course-instructor",
    "href": "lessons/999-metadata.html#about-the-author-and-course-instructor",
    "title": "Metadata",
    "section": "About the author and course instructor",
    "text": "About the author and course instructor\nThese course notes were written by Andrew Irwin who is in the Department of Mathematics & Statistics at Dalhousie University in Halifax, Nova Scotia, Canada. Andrew has been at Dalhousie since 2018. Before that he was in the Department of Mathematics & Computer Science at Mount Allison University and before that in the Biology Department at the College of Staten Island at the City University of New York.\nAndrew’s research applies mathematical and statistical models to problems in biological oceanography, particularly questions about phytoplankton biogeography, productivity, and their role in biogeochemical cycles. The prospect of large-scale systemic change over the next century plays a key motivating role in the desire to understand this important system more fully. Andrew co-directs the Marine Microbial Macroecology Lab at Dalhousie and more information about his research can be found there."
  },
  {
    "objectID": "lessons/999-metadata.html#thanks-to-the-community",
    "href": "lessons/999-metadata.html#thanks-to-the-community",
    "title": "Metadata",
    "section": "Thanks to the community",
    "text": "Thanks to the community\nThis course builds on the knowledge and generosity of many people.\nThe software described in this course is all open source: free to use, modify, and extend by all. I started to learn to use these tools as a graduate student with the commercial product S-PLUS. Participants in the open software development model have created an incredible set of resources for students to learn and use.\nI have referenced three textbooks extensively in these notes. All are commercially available books, but the the authors and publishers have made these resources available online in a free to use format, which is wonderful for students and makes it easy for me to refer to specific sections, ideas, and skills demonstrated in these books.\nMany people are actively engaged in making new resources for learning, supporting the creation of courses, and collecting data for interesting data visualization projects. Many of these are linked throughout these notes and used with much appreciation.\nPosit (formerly Rstudio) is a public benefit corporation based in the USA that is committed to developing tools for the statistics and data science community. The contribution of the Rstudio software and many R packages has been a huge boost to the community of people using R."
  },
  {
    "objectID": "lessons/999-metadata.html#r-version-and-packages-used",
    "href": "lessons/999-metadata.html#r-version-and-packages-used",
    "title": "Metadata",
    "section": "R version and packages used",
    "text": "R version and packages used\nThese documents were prepared using R version 4.3.1 (2023-06-16) on aarch64-apple-darwin20.\n\n\n\nHere is a list of all packages used in any lesson:\nbroom, cansim, coefplot, dbplyr, dplyr, DT, equatiomatic, fields, forcats, gapminder, geojsonio, GGally, gganimate, ggfortify, ggheatmap, ggmap, ggplot2, ggrepel, ggtext, ggthemes, glue, gratia, hablar, here, janitor, kableExtra, knitr, lattice, leaflet, lubridate, mapproj, maps, MASS, mgcv, naniar, nlme, paletteer, palmerpenguins, patchwork, permute, plotly, purrr, quantreg, ragg, RColorBrewer, readr, readxl, report, rmapshaper, RSQLite, scales, sf, skimr, spam, SparseM, statebins, stringr, styler, svglite, tibble, tidyr, tidyverse, unglue, vegan, viridisLite"
  },
  {
    "objectID": "lessons/999-metadata.html#license",
    "href": "lessons/999-metadata.html#license",
    "title": "Metadata",
    "section": "License",
    "text": "License\nThese course notes and supporting materials are all copyright by Andrew Irwin. Material linked or quoted from other sources are, naturally, owned by the original creators.\nOriginal materials are licensed under a creative commons license that encourages non-commerical reuse and adaptation. For other uses, please contact the author.\n\nData Visualization course notes by Andrew Irwin is licensed under Attribution-NonCommercial 4.0 International"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "De Veaux, Richard D., Paul F. Velleman, David E. Bock, Augustin M.\nVukov, and Augustine C. M. Wong. 2018. Stats: Data and Models.\nThird Canadian Edition. Pearson Education Canada.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical\nIntroduction. Princeton University Press. https://socviz.co/.\n\n\nO’Donoghue, Seán I., Benedetta Frida Baldi, Susan J. Clark, Aaron E.\nDarling, James M. Hogan, Sandeep Kaur, Lena Maier-Hein, et al. 2018.\n“Visualization of Biomedical Data.” Annual Review of\nBiomedical Data Science 1 (1): 275–304. https://doi.org/10.1146/annurev-biodatasci-080917-013424.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27. https://doi.org/10.1126/science.1213847.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. “Ten Simple Rules for Reproducible Computational\nResearch.” Edited by Philip E. Bourne. PLoS Computational\nBiology 9 (10): e1003285. https://doi.org/10.1371/journal.pcbi.1003285.\n\n\nStoelzle, Michael, and Lina Stein. 2021. “Rainbow Color Map\nDistorts and Misleads Research in Hydrology  Guidance for\nBetter Visualizations and Science Communication.” Hydrology\nand Earth System Sciences 25 (8): 4549–65. https://doi.org/10.5194/hess-25-4549-2021.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\nhttps://doi.org/10.1198/jcgs.2009.07098.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed. \"O’Reilly Media\". https://r4ds.had.co.nz/.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer\non Making Informative and Compelling Figures. \"O’Reilly Media\". https://clauswilke.com/dataviz/."
  }
]